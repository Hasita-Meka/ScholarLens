{"file_contents":{"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"networkx>=3.6\",\n    \"nltk>=3.9.2\",\n    \"numpy>=2.3.5\",\n    \"openai>=2.8.1\",\n    \"pandas>=2.3.3\",\n    \"pdfplumber>=0.11.8\",\n    \"plotly>=6.5.0\",\n    \"psycopg2-binary>=2.9.11\",\n    \"python-dotenv>=1.2.1\",\n    \"requests>=2.32.5\",\n    \"scikit-learn>=1.7.2\",\n    \"sqlalchemy>=2.0.44\",\n    \"streamlit>=1.51.0\",\n    \"trafilatura>=2.0.0\",\n]\n","path":null,"size_bytes":480,"size_tokens":null},"LOCAL_SETUP.md":{"content":"# ScholarLens - Local Setup Guide\n\nThis guide explains how to run ScholarLens on your local machine.\n\n## Prerequisites\n\n- Python 3.10 or higher\n- pip (Python package manager)\n\n## Installation Steps\n\n### 1. Create a Virtual Environment (recommended)\n\n```bash\n# Windows\npython -m venv venv\nvenv\\Scripts\\activate\n\n# macOS/Linux\npython3 -m venv venv\nsource venv/bin/activate\n```\n\n### 2. Install Dependencies\n\n```bash\npip install streamlit sqlalchemy psycopg2-binary openai python-dotenv networkx plotly pandas numpy scikit-learn pdfplumber nltk requests trafilatura\n```\n\n### 3. Configure Environment Variables\n\nCreate a `.env` file in the project root directory:\n\n```\n# For SQLite (easiest - no database setup needed):\nDATABASE_URL=sqlite:///scholarlens.db\n\n# For PostgreSQL (if you have it installed):\n# DATABASE_URL=postgresql://username:password@localhost:5432/scholarlens\n\n# AI Provider - Choose ONE of the following:\n\n# Option 1: Google AI Studio (FREE - Recommended!)\n# Get your free API key at: https://aistudio.google.com/app/apikey\nGOOGLE_API_KEY=your-google-api-key-here\n\n# Option 2: OpenAI (Paid)\n# Get your API key at: https://platform.openai.com/api-keys\n# OPENAI_API_KEY=your-openai-api-key-here\n```\n\n### 4. Run the Application\n\n```bash\nstreamlit run app.py --server.port 5000\n```\n\nThe app will open in your browser at http://localhost:5000\n\n## AI Provider Options\n\n### Google AI Studio (FREE - Recommended)\n\n1. Go to https://aistudio.google.com/app/apikey\n2. Sign in with your Google account\n3. Click \"Create API key\"\n4. Copy the key and add it to your `.env` file as `GOOGLE_API_KEY`\n\n**Benefits:**\n- Completely FREE (1 million tokens/minute)\n- Uses Gemini 2.0 Flash model\n- No credit card required\n- Fast and reliable\n\n### OpenAI (Paid)\n\n1. Go to https://platform.openai.com/api-keys\n2. Create an account and add billing\n3. Generate an API key\n4. Add it to your `.env` file as `OPENAI_API_KEY`\n\n**Note:** If both keys are set, Google AI Studio will be used by default.\n\n## Database Options\n\n### SQLite (Easiest)\n- No installation needed\n- Just set `DATABASE_URL=sqlite:///scholarlens.db` in your `.env` file\n- Data is stored in a local file called `scholarlens.db`\n\n### PostgreSQL (Production-grade)\n1. Install PostgreSQL on your machine\n2. Create a database: `createdb scholarlens`\n3. Set the DATABASE_URL in your `.env` file\n\n## Troubleshooting\n\n### \"DATABASE_URL not set\" Error\n- Make sure you have a `.env` file with the DATABASE_URL variable\n- Make sure python-dotenv is installed: `pip install python-dotenv`\n\n### \"AI API not configured\" Error\n- Add either `GOOGLE_API_KEY` or `OPENAI_API_KEY` to your `.env` file\n- Google AI Studio is free: https://aistudio.google.com/app/apikey\n\n### SQLAlchemy Errors\n- Make sure all packages are installed correctly\n- Try reinstalling: `pip install --upgrade sqlalchemy psycopg2-binary`\n\n## Features That Work Without AI API\n\n- PDF Upload and text extraction\n- Search across papers\n- arXiv and PubMed paper fetching\n- Knowledge graph visualization\n- Analytics dashboard\n- Topic clustering\n- Reading list and notes\n- Export to Markdown, LaTeX, BibTeX\n\n## Features That Require AI API (Google or OpenAI)\n\n- AI-powered Q&A\n- Multi-audience summaries (Expert, Student, Policymaker)\n- Flashcard generation\n- Quiz generation\n- Policy brief generation\n- Cross-domain analogies\n- Key insights extraction\n","path":null,"size_bytes":3352,"size_tokens":null},"utils/graph_builder.py":{"content":"\"\"\"\nKnowledge Graph builder and visualization for ScholarLens\nCreates interactive network visualizations using NetworkX and Plotly\n\"\"\"\n\nimport networkx as nx\nimport plotly.graph_objects as go\nfrom typing import List, Dict, Tuple, Optional\nimport json\n\n\ndef build_knowledge_graph(papers: List[Dict], methods: List[Dict], \n                          datasets: List[Dict], authors: List[Dict]) -> nx.Graph:\n    \"\"\"\n    Build a knowledge graph from extracted entities\n    \"\"\"\n    G = nx.Graph()\n    \n    for paper in papers:\n        G.add_node(\n            f\"paper_{paper['id']}\",\n            type='paper',\n            label=paper.get('title', 'Unknown')[:50],\n            full_title=paper.get('title', 'Unknown'),\n            year=paper.get('year'),\n            size=20\n        )\n    \n    for method in methods:\n        G.add_node(\n            f\"method_{method['id']}\",\n            type='method',\n            label=method.get('name', 'Unknown'),\n            category=method.get('category', 'unknown'),\n            usage_count=method.get('usage_count', 0),\n            size=15 + min(method.get('usage_count', 0) * 2, 30)\n        )\n    \n    for dataset in datasets:\n        G.add_node(\n            f\"dataset_{dataset['id']}\",\n            type='dataset',\n            label=dataset.get('name', 'Unknown'),\n            domain=dataset.get('domain', 'unknown'),\n            usage_count=dataset.get('usage_count', 0),\n            size=15 + min(dataset.get('usage_count', 0) * 2, 30)\n        )\n    \n    for author in authors:\n        G.add_node(\n            f\"author_{author['id']}\",\n            type='author',\n            label=author.get('name', 'Unknown'),\n            paper_count=len(author.get('papers', [])),\n            size=12 + min(len(author.get('papers', [])) * 3, 25)\n        )\n    \n    for paper in papers:\n        paper_node = f\"paper_{paper['id']}\"\n        \n        for method in paper.get('methods', []):\n            method_node = f\"method_{method['id']}\"\n            if G.has_node(method_node):\n                G.add_edge(paper_node, method_node, type='uses_method', weight=1)\n        \n        for dataset in paper.get('datasets', []):\n            dataset_node = f\"dataset_{dataset['id']}\"\n            if G.has_node(dataset_node):\n                G.add_edge(paper_node, dataset_node, type='uses_dataset', weight=1)\n        \n        for author in paper.get('authors', []):\n            author_node = f\"author_{author['id']}\"\n            if G.has_node(author_node):\n                G.add_edge(paper_node, author_node, type='authored_by', weight=1)\n    \n    return G\n\n\ndef build_method_dag(methods: List[Dict], prerequisites: Dict[str, List[str]]) -> nx.DiGraph:\n    \"\"\"\n    Build a directed acyclic graph of method prerequisites\n    \"\"\"\n    G = nx.DiGraph()\n    \n    for method in methods:\n        G.add_node(\n            method['name'],\n            category=method.get('category', 'unknown'),\n            usage_count=method.get('usage_count', 0)\n        )\n    \n    for method_name, prereqs in prerequisites.items():\n        if G.has_node(method_name):\n            for prereq in prereqs:\n                if G.has_node(prereq):\n                    G.add_edge(prereq, method_name)\n                else:\n                    G.add_node(prereq, category='foundation', usage_count=0)\n                    G.add_edge(prereq, method_name)\n    \n    return G\n\n\ndef create_plotly_graph(G: nx.Graph, title: str = \"Knowledge Graph\", show_labels: bool = False) -> go.Figure:\n    \"\"\"\n    Create an interactive Plotly visualization of a NetworkX graph\n    Only shows connected nodes (removes orphans) for cleaner visualization\n    \"\"\"\n    connected_nodes = [node for node in G.nodes() if G.degree(node) > 0]\n    \n    if not connected_nodes:\n        fig = go.Figure()\n        fig.add_annotation(\n            text=\"No connected nodes to display.<br>Upload papers to build relationships.\",\n            xref=\"paper\", yref=\"paper\",\n            x=0.5, y=0.5, showarrow=False,\n            font=dict(size=16, color='gray')\n        )\n        fig.update_layout(\n            title=dict(text=title, x=0.5),\n            height=400,\n            plot_bgcolor='rgba(0,0,0,0)',\n            paper_bgcolor='rgba(0,0,0,0)',\n            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n        )\n        return fig\n    \n    H = G.subgraph(connected_nodes).copy()\n    \n    num_nodes = H.number_of_nodes()\n    k_value = max(0.5, 3 / (num_nodes ** 0.5)) if num_nodes > 1 else 1\n    iterations = min(100, max(50, num_nodes * 2))\n    \n    pos = nx.spring_layout(H, k=k_value, iterations=iterations, seed=42)\n    \n    node_colors = {\n        'paper': '#4CAF50',\n        'method': '#2196F3',\n        'dataset': '#FF9800',\n        'author': '#9C27B0',\n        'institution': '#E91E63'\n    }\n    \n    edge_x = []\n    edge_y = []\n    for edge in H.edges():\n        x0, y0 = pos[edge[0]]\n        x1, y1 = pos[edge[1]]\n        edge_x.extend([x0, x1, None])\n        edge_y.extend([y0, y1, None])\n    \n    edge_trace = go.Scatter(\n        x=edge_x,\n        y=edge_y,\n        mode='lines',\n        line=dict(width=1, color='rgba(150,150,150,0.5)'),\n        hoverinfo='none',\n        showlegend=False\n    )\n    \n    node_traces = {}\n    for node_type in node_colors.keys():\n        node_traces[node_type] = {\n            'x': [], 'y': [], 'text': [], 'hover_text': [], 'size': [], 'customdata': []\n        }\n    \n    for node in H.nodes():\n        x, y = pos[node]\n        node_data = H.nodes[node]\n        node_type = node_data.get('type', 'paper')\n        degree = H.degree(node)\n        \n        if node_type in node_traces:\n            node_traces[node_type]['x'].append(x)\n            node_traces[node_type]['y'].append(y)\n            label = node_data.get('label', node)\n            node_traces[node_type]['text'].append(label if show_labels else '')\n            node_traces[node_type]['hover_text'].append(f\"{label}<br>Type: {node_type}<br>Connections: {degree}\")\n            base_size = node_data.get('size', 15)\n            size = base_size + (degree * 2)\n            node_traces[node_type]['size'].append(min(size, 40))\n            node_traces[node_type]['customdata'].append(node)\n    \n    fig = go.Figure()\n    \n    fig.add_trace(edge_trace)\n    \n    for node_type, data in node_traces.items():\n        if data['x']:\n            fig.add_trace(go.Scatter(\n                x=data['x'],\n                y=data['y'],\n                mode='markers+text' if show_labels else 'markers',\n                marker=dict(\n                    size=data['size'],\n                    color=node_colors.get(node_type, '#888'),\n                    line=dict(width=2, color='white'),\n                    opacity=0.9\n                ),\n                text=data['text'],\n                textposition='top center',\n                textfont=dict(size=9, color='white'),\n                customdata=data['customdata'],\n                name=node_type.capitalize(),\n                hovertext=data['hover_text'],\n                hoverinfo='text'\n            ))\n    \n    fig.update_layout(\n        title=dict(text=title, x=0.5, font=dict(size=16)),\n        showlegend=True,\n        legend=dict(\n            orientation='h',\n            yanchor='bottom',\n            y=1.02,\n            xanchor='center',\n            x=0.5,\n            bgcolor='rgba(0,0,0,0.3)',\n            font=dict(size=11)\n        ),\n        hovermode='closest',\n        dragmode='pan',\n        margin=dict(b=20, l=5, r=5, t=60),\n        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False, fixedrange=False),\n        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False, fixedrange=False, scaleanchor='x', scaleratio=1),\n        plot_bgcolor='rgba(0,0,0,0)',\n        paper_bgcolor='rgba(0,0,0,0)',\n        height=600\n    )\n    \n    fig.update_layout(\n        modebar=dict(\n            orientation='v',\n            bgcolor='rgba(0,0,0,0.5)',\n            activecolor='#4CAF50'\n        ),\n        modebar_add=['pan2d', 'zoom2d', 'zoomIn2d', 'zoomOut2d', 'resetScale2d']\n    )\n    \n    return fig\n\n\ndef create_method_dag_visualization(G: nx.DiGraph, title: str = \"Concept Dependency Map\") -> go.Figure:\n    \"\"\"\n    Create a hierarchical visualization for method prerequisites\n    \"\"\"\n    try:\n        pos = nx.nx_agraph.graphviz_layout(G, prog='dot')\n    except:\n        for layer, nodes in enumerate(nx.topological_generations(G)):\n            for i, node in enumerate(nodes):\n                pos = getattr(G, '_pos', {})\n                pos[node] = (i * 100, layer * 100)\n                G._pos = pos\n        pos = getattr(G, '_pos', nx.spring_layout(G))\n    \n    edge_x = []\n    edge_y = []\n    \n    for edge in G.edges():\n        x0, y0 = pos[edge[0]]\n        x1, y1 = pos[edge[1]]\n        edge_x.extend([x0, x1, None])\n        edge_y.extend([y0, y1, None])\n    \n    edge_trace = go.Scatter(\n        x=edge_x, y=edge_y,\n        line=dict(width=2, color='#888'),\n        hoverinfo='none',\n        mode='lines'\n    )\n    \n    node_x = []\n    node_y = []\n    node_text = []\n    node_size = []\n    \n    for node in G.nodes():\n        x, y = pos[node]\n        node_x.append(x)\n        node_y.append(y)\n        node_text.append(node)\n        node_size.append(20 + G.nodes[node].get('usage_count', 0) * 2)\n    \n    node_trace = go.Scatter(\n        x=node_x, y=node_y,\n        mode='markers+text',\n        hoverinfo='text',\n        text=node_text,\n        textposition='top center',\n        marker=dict(\n            size=node_size,\n            color='#2196F3',\n            line=dict(width=2, color='white')\n        )\n    )\n    \n    fig = go.Figure(data=[edge_trace, node_trace])\n    \n    fig.update_layout(\n        title=dict(text=title, x=0.5),\n        showlegend=False,\n        hovermode='closest',\n        margin=dict(b=20, l=5, r=5, t=40),\n        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n        plot_bgcolor='rgba(0,0,0,0)',\n        paper_bgcolor='rgba(0,0,0,0)',\n        height=500\n    )\n    \n    return fig\n\n\ndef build_coauthorship_network(authors: List[Dict], papers: List[Dict]) -> nx.Graph:\n    \"\"\"\n    Build a co-authorship network\n    \"\"\"\n    G = nx.Graph()\n    \n    for author in authors:\n        G.add_node(\n            author['id'],\n            name=author.get('name', 'Unknown'),\n            paper_count=len(author.get('papers', []))\n        )\n    \n    for paper in papers:\n        paper_authors = paper.get('authors', [])\n        for i, author1 in enumerate(paper_authors):\n            for author2 in paper_authors[i+1:]:\n                if G.has_node(author1['id']) and G.has_node(author2['id']):\n                    if G.has_edge(author1['id'], author2['id']):\n                        G[author1['id']][author2['id']]['weight'] += 1\n                    else:\n                        G.add_edge(author1['id'], author2['id'], weight=1)\n    \n    return G\n\n\ndef calculate_graph_metrics(G: nx.Graph) -> Dict:\n    \"\"\"\n    Calculate various graph metrics\n    \"\"\"\n    metrics = {\n        'num_nodes': G.number_of_nodes(),\n        'num_edges': G.number_of_edges(),\n        'density': nx.density(G) if G.number_of_nodes() > 1 else 0,\n    }\n    \n    if G.number_of_nodes() > 0:\n        if nx.is_connected(G):\n            metrics['avg_path_length'] = nx.average_shortest_path_length(G)\n        else:\n            metrics['avg_path_length'] = 0\n        \n        metrics['clustering_coefficient'] = nx.average_clustering(G)\n        \n        degree_centrality = nx.degree_centrality(G)\n        metrics['top_central_nodes'] = sorted(\n            degree_centrality.items(), \n            key=lambda x: x[1], \n            reverse=True\n        )[:10]\n    \n    return metrics\n\n\ndef find_collaboration_opportunities(G: nx.Graph, min_common_neighbors: int = 2) -> List[Dict]:\n    \"\"\"\n    Find potential collaboration opportunities based on network structure\n    \"\"\"\n    opportunities = []\n    \n    nodes = list(G.nodes())\n    for i, node1 in enumerate(nodes):\n        for node2 in nodes[i+1:]:\n            if not G.has_edge(node1, node2):\n                common = len(list(nx.common_neighbors(G, node1, node2)))\n                if common >= min_common_neighbors:\n                    opportunities.append({\n                        'node1': node1,\n                        'node2': node2,\n                        'common_neighbors': common,\n                        'node1_name': G.nodes[node1].get('name', node1),\n                        'node2_name': G.nodes[node2].get('name', node2)\n                    })\n    \n    opportunities.sort(key=lambda x: x['common_neighbors'], reverse=True)\n    return opportunities[:20]\n","path":null,"size_bytes":12720,"size_tokens":null},"utils/semantic_search.py":{"content":"\"\"\"\nSemantic search utilities for ScholarLens\nProvides vector-based search using TF-IDF and cosine similarity\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Dict, Tuple, Optional\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport json\n\n\nclass SemanticSearch:\n    \"\"\"\n    Semantic search using TF-IDF vectorization\n    Lightweight alternative to heavy embedding models\n    \"\"\"\n    \n    def __init__(self):\n        self.documents = []\n        self.document_vectors = None\n        self.metadata = []\n        self.is_fitted = False\n        self.vectorizer = None\n        self._create_vectorizer()\n    \n    def _create_vectorizer(self):\n        \"\"\"Create vectorizer with appropriate settings for document count\"\"\"\n        doc_count = len(self.documents)\n        if doc_count <= 1:\n            max_df = 1.0\n        elif doc_count <= 5:\n            max_df = 1.0\n        else:\n            max_df = 0.95\n        \n        self.vectorizer = TfidfVectorizer(\n            max_features=5000,\n            stop_words='english',\n            ngram_range=(1, 2),\n            min_df=1,\n            max_df=max_df\n        )\n    \n    def add_documents(self, documents: List[str], metadata: List[Dict] = None):\n        \"\"\"\n        Add documents to the search index\n        \"\"\"\n        self.documents.extend(documents)\n        if metadata:\n            self.metadata.extend(metadata)\n        else:\n            self.metadata.extend([{}] * len(documents))\n        \n        if self.documents:\n            try:\n                self._create_vectorizer()\n                self.document_vectors = self.vectorizer.fit_transform(self.documents)\n                self.is_fitted = True\n            except ValueError as e:\n                if \"no terms remain\" in str(e).lower() or \"max_df\" in str(e).lower():\n                    self.vectorizer = TfidfVectorizer(\n                        max_features=5000,\n                        stop_words=None,\n                        ngram_range=(1, 1),\n                        min_df=1,\n                        max_df=1.0\n                    )\n                    try:\n                        self.document_vectors = self.vectorizer.fit_transform(self.documents)\n                        self.is_fitted = True\n                    except Exception:\n                        self.is_fitted = False\n                else:\n                    self.is_fitted = False\n    \n    def search(self, query: str, top_k: int = 5) -> List[Dict]:\n        \"\"\"\n        Search for documents similar to query\n        \"\"\"\n        if not self.is_fitted or not self.documents:\n            return []\n        \n        query_vector = self.vectorizer.transform([query])\n        \n        similarities = cosine_similarity(query_vector, self.document_vectors)[0]\n        \n        top_indices = np.argsort(similarities)[::-1][:top_k]\n        \n        results = []\n        for idx in top_indices:\n            if similarities[idx] > 0.01:\n                result = {\n                    'content': self.documents[idx],\n                    'score': float(similarities[idx]),\n                    'index': int(idx)\n                }\n                if idx < len(self.metadata):\n                    result.update(self.metadata[idx])\n                results.append(result)\n        \n        return results\n    \n    def clear(self):\n        \"\"\"\n        Clear all documents from the index\n        \"\"\"\n        self.documents = []\n        self.document_vectors = None\n        self.metadata = []\n        self.is_fitted = False\n    \n    def get_document_count(self) -> int:\n        \"\"\"\n        Get number of indexed documents\n        \"\"\"\n        return len(self.documents)\n\n\nclass PaperSearchIndex:\n    \"\"\"\n    Search index specifically for research papers\n    Maintains separate indices for different content types\n    \"\"\"\n    \n    def __init__(self):\n        self.chunk_search = SemanticSearch()\n        self.title_search = SemanticSearch()\n        self.abstract_search = SemanticSearch()\n        self.method_search = SemanticSearch()\n    \n    def index_paper(self, paper_id: int, title: str, abstract: str, \n                    chunks: List[Dict], methods: List[str] = None):\n        \"\"\"\n        Index a paper's content for search\n        \"\"\"\n        self.title_search.add_documents(\n            [title],\n            [{'paper_id': paper_id, 'type': 'title'}]\n        )\n        \n        if abstract:\n            self.abstract_search.add_documents(\n                [abstract],\n                [{'paper_id': paper_id, 'type': 'abstract'}]\n            )\n        \n        for chunk in chunks:\n            self.chunk_search.add_documents(\n                [chunk['content']],\n                [{\n                    'paper_id': paper_id,\n                    'chunk_index': chunk.get('index', 0),\n                    'section': chunk.get('section', 'body'),\n                    'type': 'chunk'\n                }]\n            )\n        \n        if methods:\n            for method in methods:\n                self.method_search.add_documents(\n                    [method],\n                    [{'paper_id': paper_id, 'type': 'method'}]\n                )\n    \n    def search_all(self, query: str, top_k: int = 10) -> List[Dict]:\n        \"\"\"\n        Search across all indices and combine results\n        \"\"\"\n        results = []\n        \n        title_results = self.title_search.search(query, top_k=3)\n        for r in title_results:\n            r['source'] = 'title'\n            r['score'] *= 1.5\n            results.append(r)\n        \n        abstract_results = self.abstract_search.search(query, top_k=3)\n        for r in abstract_results:\n            r['source'] = 'abstract'\n            r['score'] *= 1.2\n            results.append(r)\n        \n        chunk_results = self.chunk_search.search(query, top_k=top_k)\n        for r in chunk_results:\n            r['source'] = 'content'\n            results.append(r)\n        \n        results.sort(key=lambda x: x['score'], reverse=True)\n        \n        return results[:top_k]\n    \n    def search_chunks(self, query: str, top_k: int = 5) -> List[Dict]:\n        \"\"\"\n        Search only in paper content chunks (for RAG)\n        \"\"\"\n        return self.chunk_search.search(query, top_k)\n    \n    def search_by_method(self, method_name: str, top_k: int = 10) -> List[Dict]:\n        \"\"\"\n        Find papers using a specific method\n        \"\"\"\n        return self.method_search.search(method_name, top_k)\n\n\ndef compute_document_similarity(doc1: str, doc2: str) -> float:\n    \"\"\"\n    Compute similarity between two documents\n    \"\"\"\n    vectorizer = TfidfVectorizer(stop_words='english')\n    try:\n        vectors = vectorizer.fit_transform([doc1, doc2])\n        similarity = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n        return float(similarity)\n    except:\n        return 0.0\n\n\ndef find_similar_papers(query_text: str, papers: List[Dict], top_k: int = 5) -> List[Dict]:\n    \"\"\"\n    Find papers similar to query text\n    \"\"\"\n    if not papers:\n        return []\n    \n    texts = [p.get('abstract', '') or p.get('content', '')[:1000] for p in papers]\n    texts.append(query_text)\n    \n    vectorizer = TfidfVectorizer(max_features=3000, stop_words='english')\n    try:\n        vectors = vectorizer.fit_transform(texts)\n        \n        query_vector = vectors[-1]\n        paper_vectors = vectors[:-1]\n        \n        similarities = cosine_similarity(query_vector, paper_vectors)[0]\n        \n        top_indices = np.argsort(similarities)[::-1][:top_k]\n        \n        results = []\n        for idx in top_indices:\n            if similarities[idx] > 0.01:\n                paper = papers[idx].copy()\n                paper['similarity_score'] = float(similarities[idx])\n                results.append(paper)\n        \n        return results\n    except:\n        return papers[:top_k]\n","path":null,"size_bytes":7848,"size_tokens":null},"utils/openai_helper.py":{"content":"\"\"\"\nAI integration for ScholarLens\nSupports Google AI Studio (Gemini) and OpenAI APIs\n\"\"\"\n\nimport os\nimport re\nimport json\nimport time\nfrom typing import List, Dict, Optional\nfrom openai import OpenAI\n\nGOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n\nclient = None\nprovider = None\nmodel_name = None\n\nif GOOGLE_API_KEY:\n    try:\n        client = OpenAI(\n            api_key=GOOGLE_API_KEY,\n            base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n        )\n        provider = \"google\"\n        model_name = \"gemini-2.0-flash\"\n        print(\"Using Google AI Studio (Gemini) API\")\n    except Exception as e:\n        print(f\"Failed to initialize Google AI client: {e}\")\n        client = None\nelif OPENAI_API_KEY:\n    try:\n        client = OpenAI(api_key=OPENAI_API_KEY)\n        provider = \"openai\"\n        model_name = \"gpt-4o-mini\"\n        print(\"Using OpenAI API\")\n    except Exception as e:\n        print(f\"Failed to initialize OpenAI client: {e}\")\n        client = None\n\n\ndef is_available() -> bool:\n    \"\"\"Check if AI API is available\"\"\"\n    return client is not None\n\n\ndef get_provider_info() -> Dict:\n    \"\"\"Get information about the current AI provider\"\"\"\n    if not is_available():\n        return {\"available\": False, \"provider\": None, \"model\": None}\n    return {\n        \"available\": True,\n        \"provider\": provider,\n        \"model\": model_name\n    }\n\n\ndef _safe_api_call(func, *args, max_retries=2, **kwargs):\n    \"\"\"Wrapper for safe API calls with retry logic\"\"\"\n    for attempt in range(max_retries + 1):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            if attempt < max_retries:\n                time.sleep(1 * (attempt + 1))\n                continue\n            raise e\n\n\ndef _extract_json(text: str) -> str:\n    \"\"\"\n    Extract JSON from a response that may contain extra text.\n    Uses json.JSONDecoder to find the first valid JSON object/array.\n    \"\"\"\n    text = text.strip()\n    \n    if text.startswith(\"```json\"):\n        text = text[7:]\n    elif text.startswith(\"```\"):\n        text = text[3:]\n    if text.endswith(\"```\"):\n        text = text[:-3]\n    text = text.strip()\n    \n    for i, char in enumerate(text):\n        if char in '[{':\n            try:\n                decoder = json.JSONDecoder()\n                result, _ = decoder.raw_decode(text[i:])\n                return json.dumps(result)\n            except json.JSONDecodeError:\n                continue\n    \n    return text\n\n\ndef _make_completion(messages: List[Dict], max_tokens: int = 1000, json_mode: bool = False) -> str:\n    \"\"\"Make a chat completion request with provider-specific handling\"\"\"\n    if not is_available():\n        raise Exception(\"AI API not configured\")\n    \n    kwargs = {\n        \"model\": model_name,\n        \"messages\": messages,\n        \"max_tokens\": max_tokens\n    }\n    \n    if json_mode and provider == \"openai\":\n        kwargs[\"response_format\"] = {\"type\": \"json_object\"}\n    \n    response = client.chat.completions.create(**kwargs)\n    return response.choices[0].message.content\n\n\ndef generate_summary(text: str, audience: str = \"expert\") -> str:\n    \"\"\"\n    Generate a summary tailored to a specific audience\n    audience: 'expert', 'student', or 'policymaker'\n    \"\"\"\n    if not is_available():\n        return \"AI API not configured. Please add GOOGLE_API_KEY or OPENAI_API_KEY to use AI features.\"\n    \n    audience_prompts = {\n        \"expert\": \"\"\"You are a research expert. Provide a technical summary of this research paper that:\n- Highlights key methodological contributions\n- Identifies novel techniques and their technical details\n- Discusses experimental results with specific metrics\n- Notes limitations and future research directions\nKeep the summary concise but technically rigorous.\"\"\",\n        \n        \"student\": \"\"\"You are an educational assistant helping students understand research. Provide a summary that:\n- Explains complex concepts using simple language and analogies\n- Breaks down the main research question and why it matters\n- Describes the approach step by step\n- Highlights key findings in accessible terms\n- Suggests related topics to learn more about\nUse clear, engaging language suitable for undergraduate students.\"\"\",\n        \n        \"policymaker\": \"\"\"You are a policy advisor. Provide a summary focused on:\n- Real-world applications and societal impact\n- Potential benefits and risks\n- Ethical considerations and concerns\n- Policy implications and recommendations\n- Timeline for practical deployment\nKeep technical jargon minimal and focus on actionable insights.\"\"\"\n    }\n    \n    system_prompt = audience_prompts.get(audience, audience_prompts[\"expert\"])\n    \n    try:\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": f\"Please summarize this research paper:\\n\\n{text[:15000]}\"}\n        ]\n        return _make_completion(messages, max_tokens=1500)\n    except Exception as e:\n        return f\"Error generating summary: {str(e)}\"\n\n\ndef answer_question(question: str, context: List[Dict], paper_title: str = \"\") -> Dict:\n    \"\"\"\n    Answer a research question using RAG with source citations\n    context: List of relevant text chunks with metadata\n    \"\"\"\n    if not is_available():\n        return {\n            \"answer\": \"AI API not configured. Please add GOOGLE_API_KEY or OPENAI_API_KEY to use AI features.\",\n            \"sources\": []\n        }\n    \n    context_text = \"\\n\\n\".join([\n        f\"[Source {i+1}] (Paper: {c.get('paper_title', 'Unknown')}, Section: {c.get('section', 'Unknown')}):\\n{c['content']}\"\n        for i, c in enumerate(context[:5])\n    ])\n    \n    system_prompt = \"\"\"You are a research assistant helping answer questions about scientific papers.\n\nRules:\n1. Answer based ONLY on the provided context\n2. Cite your sources using [Source N] format\n3. If the context doesn't contain enough information, say so\n4. Be precise and accurate\n5. Provide specific quotes or data when relevant\"\"\"\n\n    try:\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": f\"\"\"Context from research papers:\n\n{context_text}\n\nQuestion: {question}\n\nPlease provide a detailed answer with citations.\"\"\"}\n        ]\n        \n        answer = _make_completion(messages, max_tokens=1000)\n        \n        sources = []\n        for i, c in enumerate(context[:5]):\n            if f\"[Source {i+1}]\" in answer or i < 2:\n                sources.append({\n                    \"index\": i + 1,\n                    \"paper_title\": c.get('paper_title', 'Unknown'),\n                    \"section\": c.get('section', 'Unknown'),\n                    \"content\": c['content'][:300] + \"...\"\n                })\n        \n        return {\n            \"answer\": answer,\n            \"sources\": sources\n        }\n    except Exception as e:\n        return {\n            \"answer\": f\"Error generating answer: {str(e)}\",\n            \"sources\": []\n        }\n\n\ndef generate_flashcards(text: str, num_cards: int = 5) -> List[Dict]:\n    \"\"\"\n    Generate flashcards from paper content\n    \"\"\"\n    if not is_available():\n        return []\n    \n    try:\n        system_content = f\"\"\"Generate exactly {num_cards} educational flashcards from this research paper.\n\nFormat your response as a JSON array with this exact structure:\n[\n    {{\"question\": \"...\", \"answer\": \"...\", \"difficulty\": \"easy\"}},\n    {{\"question\": \"...\", \"answer\": \"...\", \"difficulty\": \"medium\"}},\n    {{\"question\": \"...\", \"answer\": \"...\", \"difficulty\": \"hard\"}}\n]\n\nFocus on:\n- Key concepts and definitions\n- Important methods and their purposes\n- Main findings and their significance\n- Technical terms and their meanings\n\nIMPORTANT: Return ONLY the JSON array, no other text.\"\"\"\n\n        messages = [\n            {\"role\": \"system\", \"content\": system_content},\n            {\"role\": \"user\", \"content\": f\"Generate flashcards from:\\n\\n{text[:10000]}\"}\n        ]\n        \n        response_text = _make_completion(messages, max_tokens=1500, json_mode=True)\n        json_text = _extract_json(response_text)\n        \n        result = json.loads(json_text)\n        if isinstance(result, dict) and 'flashcards' in result:\n            return result['flashcards']\n        elif isinstance(result, list):\n            return result\n        else:\n            return []\n    except Exception as e:\n        print(f\"Error generating flashcards: {e}\")\n        return []\n\n\ndef generate_quiz(text: str, num_questions: int = 5) -> List[Dict]:\n    \"\"\"\n    Generate quiz questions from paper content\n    \"\"\"\n    if not is_available():\n        return []\n    \n    try:\n        system_content = f\"\"\"Generate exactly {num_questions} multiple-choice quiz questions from this research paper.\n\nFormat your response as JSON with this exact structure:\n{{\n    \"questions\": [\n        {{\n            \"question\": \"What is the main contribution of this paper?\",\n            \"options\": [\"A) Option 1\", \"B) Option 2\", \"C) Option 3\", \"D) Option 4\"],\n            \"correct_answer\": \"A\",\n            \"explanation\": \"Brief explanation of why this is correct\"\n        }}\n    ]\n}}\n\nQuestions should test understanding of:\n- Core concepts and methods\n- Research findings\n- Technical details\n- Practical applications\n\nIMPORTANT: Return ONLY the JSON object, no other text.\"\"\"\n\n        messages = [\n            {\"role\": \"system\", \"content\": system_content},\n            {\"role\": \"user\", \"content\": f\"Generate quiz questions from:\\n\\n{text[:10000]}\"}\n        ]\n        \n        response_text = _make_completion(messages, max_tokens=2000, json_mode=True)\n        json_text = _extract_json(response_text)\n        \n        result = json.loads(json_text)\n        return result.get('questions', [])\n    except Exception as e:\n        print(f\"Error generating quiz: {e}\")\n        return []\n\n\ndef generate_policy_brief(text: str, paper_title: str = \"\") -> str:\n    \"\"\"\n    Generate a policy brief from paper content\n    \"\"\"\n    if not is_available():\n        return \"AI API not configured. Please add GOOGLE_API_KEY or OPENAI_API_KEY to use AI features.\"\n    \n    try:\n        system_content = \"\"\"You are a policy analyst creating a brief for policymakers.\n\nStructure the policy brief as follows:\n\n## Executive Summary\nBrief overview of the research and its significance\n\n## Key Findings\nBullet points of main discoveries\n\n## Applications\nReal-world use cases and potential implementations\n\n## Risks and Concerns\nPotential negative impacts, ethical issues, and limitations\n\n## Recommendations\nActionable policy suggestions\n\n## Timeline\nExpected development and deployment timeline\n\nKeep language accessible to non-technical readers.\"\"\"\n\n        messages = [\n            {\"role\": \"system\", \"content\": system_content},\n            {\"role\": \"user\", \"content\": f\"Create a policy brief for this research paper:\\n\\nTitle: {paper_title}\\n\\n{text[:12000]}\"}\n        ]\n        \n        return _make_completion(messages, max_tokens=2000)\n    except Exception as e:\n        return f\"Error generating policy brief: {str(e)}\"\n\n\ndef generate_analogy(concept: str, context: str = \"\") -> str:\n    \"\"\"\n    Generate a cross-domain analogy for a research concept\n    \"\"\"\n    if not is_available():\n        return \"AI API not configured.\"\n    \n    try:\n        system_content = \"\"\"You are an expert at creating intuitive analogies for complex technical concepts.\n\nRules:\n1. Create an analogy using everyday experiences\n2. The analogy should be accurate and capture the key essence\n3. Explain how the analogy maps to the technical concept\n4. Keep it concise but clear\n\nExample: \"Attention in NLP is like triaging patients in an ER - the most critical cases get immediate focus while others wait.\"\n\"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": system_content},\n            {\"role\": \"user\", \"content\": f\"Create an intuitive analogy for this concept:\\n\\nConcept: {concept}\\n\\nContext: {context[:500] if context else 'No additional context'}\"}\n        ]\n        \n        return _make_completion(messages, max_tokens=300)\n    except Exception as e:\n        return f\"Error generating analogy: {str(e)}\"\n\n\ndef extract_key_insights(text: str) -> List[str]:\n    \"\"\"\n    Extract key insights from paper text\n    \"\"\"\n    if not is_available():\n        return [\"AI API not configured.\"]\n    \n    try:\n        system_content = \"\"\"Extract 5-7 key insights from this research paper.\n                \nFormat as JSON:\n{\"insights\": [\"insight 1\", \"insight 2\", ...]}\n\nEach insight should be:\n- A complete, standalone statement\n- Focused on findings, methods, or implications\n- Clear and concise\n\nIMPORTANT: Return ONLY the JSON object, no other text.\"\"\"\n\n        messages = [\n            {\"role\": \"system\", \"content\": system_content},\n            {\"role\": \"user\", \"content\": text[:10000]}\n        ]\n        \n        response_text = _make_completion(messages, max_tokens=800, json_mode=True)\n        json_text = _extract_json(response_text)\n        \n        result = json.loads(json_text)\n        return result.get('insights', [])\n    except Exception as e:\n        return [f\"Error extracting insights: {str(e)}\"]\n","path":null,"size_bytes":13151,"size_tokens":null},"utils/arxiv_pubmed.py":{"content":"\"\"\"\narXiv and PubMed API integration for ScholarLens\nFetches research papers from open-access sources\n\"\"\"\n\nimport requests\nimport xml.etree.ElementTree as ET\nfrom typing import List, Dict, Optional\nfrom datetime import datetime\nimport re\nimport time\n\n\nclass ArxivAPI:\n    \"\"\"\n    arXiv API client for fetching research papers\n    \"\"\"\n    \n    BASE_URL = \"http://export.arxiv.org/api/query\"\n    \n    def __init__(self):\n        self.session = requests.Session()\n    \n    def search(self, query: str, max_results: int = 10, \n               category: str = None, sort_by: str = \"relevance\") -> List[Dict]:\n        \"\"\"\n        Search arXiv for papers matching the query\n        \n        Args:\n            query: Search query string\n            max_results: Maximum number of results (default 10, max 100)\n            category: arXiv category filter (e.g., 'cs.AI', 'cs.LG')\n            sort_by: Sort order ('relevance', 'lastUpdatedDate', 'submittedDate')\n        \"\"\"\n        search_query = f\"all:{query}\"\n        if category:\n            search_query = f\"cat:{category} AND {search_query}\"\n        \n        sort_mapping = {\n            'relevance': 'relevance',\n            'lastUpdatedDate': 'lastUpdatedDate',\n            'submittedDate': 'submittedDate'\n        }\n        \n        params = {\n            'search_query': search_query,\n            'start': 0,\n            'max_results': min(max_results, 100),\n            'sortBy': sort_mapping.get(sort_by, 'relevance'),\n            'sortOrder': 'descending'\n        }\n        \n        try:\n            response = self.session.get(self.BASE_URL, params=params, timeout=30)\n            response.raise_for_status()\n            \n            return self._parse_response(response.text)\n        except Exception as e:\n            print(f\"arXiv API error: {e}\")\n            return []\n    \n    def get_paper_by_id(self, arxiv_id: str) -> Optional[Dict]:\n        \"\"\"\n        Get a specific paper by its arXiv ID\n        \"\"\"\n        arxiv_id = arxiv_id.replace('arXiv:', '').strip()\n        \n        params = {\n            'id_list': arxiv_id,\n            'max_results': 1\n        }\n        \n        try:\n            response = self.session.get(self.BASE_URL, params=params, timeout=30)\n            response.raise_for_status()\n            \n            papers = self._parse_response(response.text)\n            return papers[0] if papers else None\n        except Exception as e:\n            print(f\"arXiv API error: {e}\")\n            return None\n    \n    def _parse_response(self, xml_content: str) -> List[Dict]:\n        \"\"\"\n        Parse arXiv API XML response\n        \"\"\"\n        papers = []\n        \n        try:\n            root = ET.fromstring(xml_content)\n            ns = {\n                'atom': 'http://www.w3.org/2005/Atom',\n                'arxiv': 'http://arxiv.org/schemas/atom'\n            }\n            \n            for entry in root.findall('atom:entry', ns):\n                paper = {}\n                \n                title_elem = entry.find('atom:title', ns)\n                paper['title'] = title_elem.text.strip().replace('\\n', ' ') if title_elem is not None else ''\n                \n                abstract_elem = entry.find('atom:summary', ns)\n                paper['abstract'] = abstract_elem.text.strip().replace('\\n', ' ') if abstract_elem is not None else ''\n                \n                paper['authors'] = []\n                for author in entry.findall('atom:author', ns):\n                    name_elem = author.find('atom:name', ns)\n                    if name_elem is not None:\n                        paper['authors'].append({'name': name_elem.text})\n                \n                id_elem = entry.find('atom:id', ns)\n                if id_elem is not None:\n                    paper['arxiv_id'] = id_elem.text.split('/abs/')[-1]\n                    paper['url'] = id_elem.text\n                \n                published_elem = entry.find('atom:published', ns)\n                if published_elem is not None:\n                    try:\n                        pub_date = datetime.fromisoformat(published_elem.text.replace('Z', '+00:00'))\n                        paper['publication_date'] = pub_date\n                        paper['year'] = pub_date.year\n                    except:\n                        paper['year'] = None\n                \n                paper['categories'] = []\n                for cat in entry.findall('atom:category', ns):\n                    term = cat.get('term')\n                    if term:\n                        paper['categories'].append(term)\n                \n                for link in entry.findall('atom:link', ns):\n                    if link.get('title') == 'pdf':\n                        paper['pdf_url'] = link.get('href')\n                        break\n                \n                doi_elem = entry.find('arxiv:doi', ns)\n                if doi_elem is not None:\n                    paper['doi'] = doi_elem.text\n                \n                paper['source'] = 'arxiv'\n                papers.append(paper)\n            \n        except ET.ParseError as e:\n            print(f\"XML parse error: {e}\")\n        \n        return papers\n    \n    def get_categories(self) -> Dict[str, str]:\n        \"\"\"\n        Return common arXiv categories\n        \"\"\"\n        return {\n            'cs.AI': 'Artificial Intelligence',\n            'cs.LG': 'Machine Learning',\n            'cs.CL': 'Computation and Language (NLP)',\n            'cs.CV': 'Computer Vision',\n            'cs.NE': 'Neural and Evolutionary Computing',\n            'cs.IR': 'Information Retrieval',\n            'cs.RO': 'Robotics',\n            'stat.ML': 'Machine Learning (Statistics)',\n            'math.OC': 'Optimization and Control',\n            'eess.SP': 'Signal Processing',\n        }\n\n\nclass PubMedAPI:\n    \"\"\"\n    PubMed API client for fetching biomedical research papers\n    \"\"\"\n    \n    SEARCH_URL = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n    FETCH_URL = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n    \n    def __init__(self, api_key: str = None):\n        self.api_key = api_key\n        self.session = requests.Session()\n    \n    def search(self, query: str, max_results: int = 10,\n               min_date: str = None, max_date: str = None) -> List[Dict]:\n        \"\"\"\n        Search PubMed for papers matching the query\n        \n        Args:\n            query: Search query string\n            max_results: Maximum number of results\n            min_date: Minimum publication date (YYYY/MM/DD)\n            max_date: Maximum publication date (YYYY/MM/DD)\n        \"\"\"\n        search_params = {\n            'db': 'pubmed',\n            'term': query,\n            'retmax': min(max_results, 100),\n            'retmode': 'json',\n            'sort': 'relevance'\n        }\n        \n        if self.api_key:\n            search_params['api_key'] = self.api_key\n        \n        if min_date:\n            search_params['mindate'] = min_date\n        if max_date:\n            search_params['maxdate'] = max_date\n        \n        try:\n            response = self.session.get(self.SEARCH_URL, params=search_params, timeout=30)\n            response.raise_for_status()\n            \n            data = response.json()\n            id_list = data.get('esearchresult', {}).get('idlist', [])\n            \n            if not id_list:\n                return []\n            \n            time.sleep(0.34)\n            \n            return self._fetch_papers(id_list)\n        except Exception as e:\n            print(f\"PubMed search error: {e}\")\n            return []\n    \n    def _fetch_papers(self, pmids: List[str]) -> List[Dict]:\n        \"\"\"\n        Fetch full paper details for given PubMed IDs\n        \"\"\"\n        fetch_params = {\n            'db': 'pubmed',\n            'id': ','.join(pmids),\n            'retmode': 'xml',\n            'rettype': 'abstract'\n        }\n        \n        if self.api_key:\n            fetch_params['api_key'] = self.api_key\n        \n        try:\n            response = self.session.get(self.FETCH_URL, params=fetch_params, timeout=30)\n            response.raise_for_status()\n            \n            return self._parse_pubmed_xml(response.text)\n        except Exception as e:\n            print(f\"PubMed fetch error: {e}\")\n            return []\n    \n    def _parse_pubmed_xml(self, xml_content: str) -> List[Dict]:\n        \"\"\"\n        Parse PubMed XML response\n        \"\"\"\n        papers = []\n        \n        try:\n            root = ET.fromstring(xml_content)\n            \n            for article in root.findall('.//PubmedArticle'):\n                paper = {}\n                \n                medline = article.find('.//MedlineCitation')\n                if medline is None:\n                    continue\n                \n                pmid_elem = medline.find('.//PMID')\n                paper['pmid'] = pmid_elem.text if pmid_elem is not None else ''\n                \n                article_elem = medline.find('.//Article')\n                if article_elem is None:\n                    continue\n                \n                title_elem = article_elem.find('.//ArticleTitle')\n                paper['title'] = title_elem.text if title_elem is not None else ''\n                \n                abstract_elem = article_elem.find('.//Abstract/AbstractText')\n                if abstract_elem is not None:\n                    paper['abstract'] = abstract_elem.text or ''\n                else:\n                    paper['abstract'] = ''\n                \n                paper['authors'] = []\n                for author in article_elem.findall('.//Author'):\n                    last_name = author.find('LastName')\n                    first_name = author.find('ForeName')\n                    if last_name is not None:\n                        name = last_name.text\n                        if first_name is not None:\n                            name = f\"{first_name.text} {name}\"\n                        paper['authors'].append({'name': name})\n                \n                pub_date = article_elem.find('.//PubDate')\n                if pub_date is not None:\n                    year_elem = pub_date.find('Year')\n                    if year_elem is not None:\n                        try:\n                            paper['year'] = int(year_elem.text)\n                        except:\n                            paper['year'] = None\n                \n                journal_elem = article_elem.find('.//Journal/Title')\n                paper['journal'] = journal_elem.text if journal_elem is not None else ''\n                \n                doi_elem = article.find('.//ArticleId[@IdType=\"doi\"]')\n                if doi_elem is not None:\n                    paper['doi'] = doi_elem.text\n                \n                paper['url'] = f\"https://pubmed.ncbi.nlm.nih.gov/{paper['pmid']}/\"\n                paper['source'] = 'pubmed'\n                \n                mesh_terms = []\n                for mesh in medline.findall('.//MeshHeading/DescriptorName'):\n                    if mesh.text:\n                        mesh_terms.append(mesh.text)\n                paper['mesh_terms'] = mesh_terms\n                \n                papers.append(paper)\n            \n        except ET.ParseError as e:\n            print(f\"XML parse error: {e}\")\n        \n        return papers\n    \n    def get_paper_by_pmid(self, pmid: str) -> Optional[Dict]:\n        \"\"\"\n        Get a specific paper by its PubMed ID\n        \"\"\"\n        papers = self._fetch_papers([pmid])\n        return papers[0] if papers else None\n\n\ndef download_arxiv_pdf(arxiv_id: str, save_path: str) -> bool:\n    \"\"\"\n    Download PDF from arXiv\n    \"\"\"\n    arxiv_id = arxiv_id.replace('arXiv:', '').strip()\n    pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n    \n    try:\n        response = requests.get(pdf_url, timeout=60)\n        response.raise_for_status()\n        \n        with open(save_path, 'wb') as f:\n            f.write(response.content)\n        \n        return True\n    except Exception as e:\n        print(f\"PDF download error: {e}\")\n        return False\n\n\ndef search_papers(query: str, sources: List[str] = None, \n                  max_results: int = 10) -> List[Dict]:\n    \"\"\"\n    Search across multiple paper sources with deduplication\n    \"\"\"\n    if sources is None:\n        sources = ['arxiv', 'pubmed']\n    \n    all_papers = []\n    seen_titles = set()\n    \n    if 'arxiv' in sources:\n        try:\n            arxiv = ArxivAPI()\n            arxiv_papers = arxiv.search(query, max_results=max_results)\n            for paper in arxiv_papers:\n                title_key = paper.get('title', '').lower().strip()[:100]\n                if title_key and title_key not in seen_titles:\n                    seen_titles.add(title_key)\n                    all_papers.append(paper)\n        except Exception as e:\n            print(f\"arXiv search error: {e}\")\n    \n    if 'pubmed' in sources:\n        try:\n            pubmed = PubMedAPI()\n            pubmed_papers = pubmed.search(query, max_results=max_results)\n            for paper in pubmed_papers:\n                title_key = paper.get('title', '').lower().strip()[:100]\n                if title_key and title_key not in seen_titles:\n                    seen_titles.add(title_key)\n                    all_papers.append(paper)\n        except Exception as e:\n            print(f\"PubMed search error: {e}\")\n    \n    return all_papers\n","path":null,"size_bytes":13363,"size_tokens":null},"TECHNICAL_REPORT.md":{"content":"# ScholarLens - Technical Report\n\n## AI-Powered Research Intelligence Platform\n\n---\n\n## 1. Executive Summary\n\nScholarLens is a comprehensive research intelligence platform designed to transform how researchers, students, and policymakers interact with academic literature. By leveraging artificial intelligence, natural language processing, and knowledge graph technologies, ScholarLens ingests research papers, builds dynamic knowledge graphs, and provides multi-audience explanations, trend forecasts, and interactive exploration tools.\n\n**Tagline:** *Making Research Intuitive, Interactive, and Insightful*\n\n---\n\n## 2. Purpose and Vision\n\n### 2.1 Problem Statement\n\nResearchers face significant challenges in managing and synthesizing the ever-growing volume of academic literature:\n- **Information Overload**: Thousands of papers published daily across disciplines\n- **Disconnected Knowledge**: Difficulty identifying relationships between methods, datasets, and research groups\n- **Accessibility Barriers**: Technical jargon limits understanding across expertise levels\n- **Manual Effort**: Time-consuming literature reviews and knowledge organization\n- **Trend Blindness**: Difficulty identifying emerging research directions\n\n### 2.2 Solution\n\nScholarLens addresses these challenges by providing:\n- **Automated Paper Ingestion**: PDF upload, arXiv/PubMed API integration\n- **Intelligent Entity Extraction**: Automatic identification of methods, datasets, authors\n- **Knowledge Graph Visualization**: Interactive exploration of research connections\n- **AI-Powered Summaries**: Multi-audience explanations (expert, student, policymaker)\n- **Predictive Analytics**: Trend forecasting and emerging method detection\n- **Learning Tools**: Flashcards, quizzes, and study roadmaps\n\n---\n\n## 3. Target Audience\n\n| Audience | Primary Use Cases |\n|----------|-------------------|\n| **Researchers** | Literature review, trend analysis, collaboration discovery |\n| **Graduate Students** | Learning new fields, understanding complex papers, exam preparation |\n| **Policymakers** | Understanding research implications, risk assessment |\n| **Research Librarians** | Corpus management, research support |\n| **Industry R&D Teams** | Technology scouting, competitive analysis |\n\n---\n\n## 4. Core Features\n\n### 4.1 Smart Corpus Management\n- **PDF Upload**: Batch upload of research papers with automatic text extraction\n- **API Integration**: Search and import papers from arXiv and PubMed\n- **Entity Recognition**: Automatic extraction of methods, datasets, and author information\n- **Semantic Search**: TF-IDF based concept search across all papers\n- **Topic Clustering**: Automatic grouping of papers using K-means clustering\n\n### 4.2 Knowledge Graph Explorer\n- **Interactive Visualization**: Plotly-powered graph exploration\n- **Multiple View Types**:\n  - Paper-Method-Dataset relationships\n  - Co-authorship networks\n  - Method prerequisite chains (DAGs)\n- **Collaboration Detection**: Identify potential research collaborators\n- **Entity Statistics**: Usage counts, category distributions\n\n### 4.3 Evidence-Backed Q&A (RAG System)\n- **Retrieval-Augmented Generation**: Answers grounded in uploaded papers\n- **Source Citations**: Every answer includes paper references\n- **Query History**: Saved questions and answers for future reference\n- **Context-Aware**: Uses semantic chunking for relevant passage retrieval\n\n### 4.4 Multi-Audience Summaries\n- **Expert Summaries**: Technical depth with methodology focus\n- **Student Summaries**: Simplified explanations with analogies\n- **Policymaker Summaries**: Applications, risks, and societal implications\n- **Policy Brief Generation**: Structured reports for decision-makers\n- **Cross-Domain Analogies**: Explain concepts using familiar domains\n\n### 4.5 Analytics Dashboard (8+ SQL Reports)\n\n| Report | Description |\n|--------|-------------|\n| Top Co-Authorship Pairs | Most frequent collaborating author pairs |\n| Trending Topics Over Time | Publication trends by year and topic |\n| Papers Per Institution | Research output by organization |\n| Research Growth by Field | Year-over-year growth analysis |\n| Top Authors by Frequency | Most prolific researchers |\n| Most Used Datasets | Popular datasets across papers |\n| Collaboration Network Density | Network connectivity metrics |\n| Emerging Methods | New techniques appearing across domains |\n\n**Trend Forecasting Features:**\n- Time-series analysis of method/dataset popularity\n- Linear regression for trajectory prediction\n- Emerging vs. declining method classification\n\n### 4.6 Learning Mode\n- **Key Insights**: Automated extraction of main findings\n- **Flashcard Generation**: AI-generated study cards from paper content\n- **Quiz Generation**: Multiple-choice questions for self-assessment\n- **Study Roadmaps**: Personalized learning paths based on prerequisites\n\n### 4.7 Research Workspace\n- **Reading Lists**: Organize papers with priority and status tracking\n- **Note-Taking**: Attach notes to individual papers\n- **Multi-Format Export**:\n  - Markdown (for documentation)\n  - LaTeX (for academic writing)\n  - BibTeX (for citation managers)\n  - CSV (for data analysis)\n  - Plain Text\n\n---\n\n## 5. Detailed Implementation Guide\n\nThis section provides a comprehensive breakdown of how each feature was built, including the specific libraries, algorithms, and code patterns used.\n\n### 5.1 PDF Processing and Text Extraction\n\n**Library Used:** `pdfplumber`\n\n**Implementation Details:**\n```python\n# utils/pdf_processor.py\nimport pdfplumber\n\ndef extract_text_from_pdf(pdf_file):\n    text = \"\"\n    with pdfplumber.open(pdf_file) as pdf:\n        for page in pdf.pages:\n            text += page.extract_text() or \"\"\n    \n    # Section detection using regex patterns\n    sections = {\n        'abstract': extract_section(text, r'abstract', r'introduction|keywords'),\n        'introduction': extract_section(text, r'introduction', r'method|related'),\n        'methodology': extract_section(text, r'method', r'result|experiment'),\n        'results': extract_section(text, r'result', r'discussion|conclusion'),\n        'conclusion': extract_section(text, r'conclusion', r'reference|acknowledgment')\n    }\n    return {'text': text, 'sections': sections, 'metadata': extract_metadata(pdf)}\n```\n\n**Why pdfplumber?**\n- Better text extraction than PyPDF2 for academic papers\n- Handles multi-column layouts common in research papers\n- Preserves reading order and formatting\n\n---\n\n### 5.2 Named Entity Recognition (NER)\n\n**Library Used:** Custom regex-based extractor (no external NLP libraries)\n\n**Implementation Details:**\n```python\n# utils/ner_extractor.py\nimport re\n\n# Predefined vocabulary of known methods and datasets\nKNOWN_METHODS = [\n    'neural network', 'deep learning', 'transformer', 'attention mechanism',\n    'BERT', 'GPT', 'CNN', 'RNN', 'LSTM', 'GAN', 'VAE', 'reinforcement learning',\n    'random forest', 'SVM', 'gradient descent', 'backpropagation', 'dropout',\n    'batch normalization', 'adam optimizer', 'cross-entropy', 'softmax'\n]\n\nKNOWN_DATASETS = [\n    'ImageNet', 'COCO', 'MNIST', 'CIFAR-10', 'CIFAR-100', 'WikiText',\n    'SQuAD', 'GLUE', 'SuperGLUE', 'Penn Treebank', 'WMT', 'LibriSpeech'\n]\n\ndef extract_entities(text):\n    methods = []\n    datasets = []\n    \n    # Pattern matching for known entities\n    for method in KNOWN_METHODS:\n        if re.search(rf'\\b{re.escape(method)}\\b', text, re.IGNORECASE):\n            methods.append({'name': method, 'category': categorize_method(method)})\n    \n    # Regex patterns for unknown methods (e.g., \"X algorithm\", \"Y model\")\n    method_patterns = [\n        r'(\\w+)\\s+algorithm',\n        r'(\\w+)\\s+model',\n        r'(\\w+)\\s+network',\n        r'(\\w+)\\s+method'\n    ]\n    \n    for pattern in method_patterns:\n        matches = re.findall(pattern, text, re.IGNORECASE)\n        for match in matches:\n            if match.lower() not in ['the', 'a', 'an', 'our', 'this']:\n                methods.append({'name': match, 'category': 'extracted'})\n    \n    return {'methods': methods, 'datasets': datasets}\n```\n\n**Why Custom Regex Instead of spaCy/NLTK?**\n- Lighter weight (no large model downloads)\n- Domain-specific vocabulary for academic papers\n- Faster processing for batch uploads\n- More control over entity categories\n\n---\n\n### 5.3 Semantic Search with TF-IDF\n\n**Library Used:** `scikit-learn` (TfidfVectorizer, cosine_similarity)\n\n**Implementation Details:**\n```python\n# utils/semantic_search.py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nclass SemanticSearchIndex:\n    def __init__(self):\n        self.vectorizer = None\n        self.tfidf_matrix = None\n        self.documents = []\n    \n    def build_index(self, documents):\n        self.documents = documents\n        texts = [doc.get('content', '') + ' ' + doc.get('title', '') for doc in documents]\n        \n        # Adaptive max_df based on corpus size\n        n_docs = len(texts)\n        max_df = 0.95 if n_docs > 10 else 1.0\n        min_df = 2 if n_docs > 5 else 1\n        \n        self.vectorizer = TfidfVectorizer(\n            max_features=5000,\n            stop_words='english',\n            ngram_range=(1, 2),  # Unigrams and bigrams\n            max_df=max_df,\n            min_df=min_df\n        )\n        self.tfidf_matrix = self.vectorizer.fit_transform(texts)\n    \n    def search(self, query, top_k=5):\n        query_vector = self.vectorizer.transform([query])\n        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n        top_indices = np.argsort(similarities)[-top_k:][::-1]\n        \n        results = []\n        for idx in top_indices:\n            if similarities[idx] > 0.1:  # Minimum threshold\n                results.append({\n                    'document': self.documents[idx],\n                    'score': float(similarities[idx])\n                })\n        return results\n```\n\n**Why TF-IDF Instead of Embeddings?**\n- No API costs (unlike OpenAI embeddings)\n- Works offline\n- Fast indexing and search\n- Sufficient accuracy for academic text\n- Adaptive parameters handle small corpora\n\n---\n\n### 5.4 Topic Clustering\n\n**Library Used:** `scikit-learn` (KMeans, TfidfVectorizer)\n\n**Implementation Details:**\n```python\n# utils/topic_modeling.py\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\ndef cluster_papers(papers, max_clusters=10):\n    texts = [p['title'] + ' ' + p.get('abstract', '') for p in papers]\n    \n    vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform(texts)\n    \n    # Find optimal number of clusters using silhouette score\n    best_k = 2\n    best_score = -1\n    \n    for k in range(2, min(max_clusters, len(papers))):\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        labels = kmeans.fit_predict(tfidf_matrix)\n        score = silhouette_score(tfidf_matrix, labels)\n        \n        if score > best_score:\n            best_score = score\n            best_k = k\n    \n    # Final clustering with optimal k\n    kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n    assignments = kmeans.fit_predict(tfidf_matrix)\n    \n    # Extract top keywords per cluster\n    feature_names = vectorizer.get_feature_names_out()\n    keywords = {}\n    for i in range(best_k):\n        center = kmeans.cluster_centers_[i]\n        top_indices = center.argsort()[-5:][::-1]\n        keywords[i] = [feature_names[idx] for idx in top_indices]\n    \n    return {\n        'assignments': assignments.tolist(),\n        'n_clusters': best_k,\n        'keywords': keywords,\n        'quality_score': best_score\n    }\n```\n\n**Algorithm Choice:**\n- K-Means for simplicity and speed\n- Silhouette score for automatic cluster count selection\n- TF-IDF features capture topic-specific vocabulary\n\n---\n\n### 5.5 Knowledge Graph Construction\n\n**Library Used:** `NetworkX` (graph operations), `Plotly` (visualization)\n\n**Implementation Details:**\n```python\n# utils/graph_builder.py\nimport networkx as nx\nimport plotly.graph_objects as go\n\ndef build_knowledge_graph(papers, methods, datasets, authors):\n    G = nx.Graph()\n    \n    # Add nodes with types\n    for paper in papers:\n        G.add_node(f\"paper_{paper.id}\", \n                   type='paper', \n                   label=paper.title[:30],\n                   size=20)\n    \n    for method in methods:\n        G.add_node(f\"method_{method.id}\",\n                   type='method',\n                   label=method.name,\n                   size=15)\n    \n    # Add edges based on relationships\n    for paper in papers:\n        for method in paper.methods:\n            G.add_edge(f\"paper_{paper.id}\", f\"method_{method.id}\")\n        for author in paper.authors:\n            G.add_edge(f\"paper_{paper.id}\", f\"author_{author.id}\")\n    \n    return G\n\ndef visualize_graph(G):\n    # Spring layout for node positioning\n    pos = nx.spring_layout(G, k=2, iterations=50)\n    \n    # Create Plotly traces\n    edge_trace = create_edge_trace(G, pos)\n    node_trace = create_node_trace(G, pos)\n    \n    fig = go.Figure(data=[edge_trace, node_trace],\n                    layout=go.Layout(\n                        showlegend=False,\n                        hovermode='closest',\n                        xaxis=dict(showgrid=False, zeroline=False),\n                        yaxis=dict(showgrid=False, zeroline=False)\n                    ))\n    return fig\n```\n\n**Graph Types Implemented:**\n1. **Paper-Method-Dataset Graph**: Shows which papers use which methods/datasets\n2. **Co-Authorship Network**: Connects authors who published together\n3. **Method Prerequisites (DAG)**: Directed graph showing method dependencies\n\n---\n\n### 5.6 AI Integration (OpenAI / Google Gemini)\n\n**Libraries Used:** `openai` (OpenAI API), `google-generativeai` (Gemini API)\n\n**Implementation Details:**\n```python\n# utils/openai_helper.py\nimport os\nfrom openai import OpenAI\n\ndef get_ai_client():\n    # Check for available API keys\n    if os.environ.get('OPENAI_API_KEY'):\n        return 'openai', OpenAI()\n    elif os.environ.get('GOOGLE_API_KEY'):\n        import google.generativeai as genai\n        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n        return 'gemini', genai.GenerativeModel('gemini-2.0-flash')\n    return None, None\n\ndef generate_summary(text, audience='expert'):\n    provider, client = get_ai_client()\n    \n    prompts = {\n        'expert': \"Provide a technical summary focusing on methodology...\",\n        'student': \"Explain this paper simply with analogies...\",\n        'policymaker': \"Summarize applications, risks, and implications...\"\n    }\n    \n    if provider == 'openai':\n        response = client.chat.completions.create(\n            model=\"gpt-4o-mini\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a research paper analyst.\"},\n                {\"role\": \"user\", \"content\": f\"{prompts[audience]}\\n\\n{text}\"}\n            ],\n            max_tokens=1000\n        )\n        return response.choices[0].message.content\n    \n    elif provider == 'gemini':\n        response = client.generate_content(f\"{prompts[audience]}\\n\\n{text}\")\n        return response.text\n```\n\n**Dual Provider Support:**\n- OpenAI GPT-4o-mini (paid, higher quality)\n- Google Gemini 2.0 Flash (free tier available)\n- Automatic fallback between providers\n\n---\n\n### 5.7 RAG (Retrieval-Augmented Generation)\n\n**Implementation Details:**\n```python\n# Chunking strategy for RAG\ndef chunk_paper(paper_content, chunk_size=500, overlap=100):\n    sentences = paper_content.split('. ')\n    chunks = []\n    current_chunk = []\n    current_length = 0\n    \n    for sentence in sentences:\n        if current_length + len(sentence) > chunk_size:\n            chunks.append('. '.join(current_chunk))\n            # Keep overlap\n            current_chunk = current_chunk[-2:] if len(current_chunk) > 2 else current_chunk\n            current_length = sum(len(s) for s in current_chunk)\n        \n        current_chunk.append(sentence)\n        current_length += len(sentence)\n    \n    return chunks\n\n# RAG Query Processing\ndef answer_question(question, paper_chunks):\n    # Step 1: Retrieve relevant chunks using TF-IDF\n    relevant_chunks = search_index.search(question, top_k=5)\n    \n    # Step 2: Build context from retrieved chunks\n    context = \"\\n\\n\".join([chunk['content'] for chunk in relevant_chunks])\n    \n    # Step 3: Generate answer with citations\n    prompt = f\"\"\"Based on the following research paper excerpts, answer the question.\n    Include citations to specific papers.\n    \n    Context:\n    {context}\n    \n    Question: {question}\n    \"\"\"\n    \n    answer = generate_with_ai(prompt)\n    \n    # Step 4: Add source references\n    sources = [chunk['paper_title'] for chunk in relevant_chunks]\n    \n    return {'answer': answer, 'sources': sources}\n```\n\n---\n\n### 5.8 arXiv and PubMed API Integration\n\n**Libraries Used:** `requests`, `xml.etree.ElementTree`\n\n**Implementation Details:**\n```python\n# utils/arxiv_pubmed.py\nimport requests\nimport xml.etree.ElementTree as ET\nimport time\n\ndef search_arxiv(query, max_results=10):\n    base_url = \"http://export.arxiv.org/api/query\"\n    params = {\n        'search_query': f'all:{query}',\n        'start': 0,\n        'max_results': max_results,\n        'sortBy': 'relevance'\n    }\n    \n    response = requests.get(base_url, params=params)\n    root = ET.fromstring(response.content)\n    \n    papers = []\n    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n        paper = {\n            'title': entry.find('{http://www.w3.org/2005/Atom}title').text,\n            'abstract': entry.find('{http://www.w3.org/2005/Atom}summary').text,\n            'authors': [author.find('{http://www.w3.org/2005/Atom}name').text \n                       for author in entry.findall('{http://www.w3.org/2005/Atom}author')],\n            'arxiv_id': entry.find('{http://www.w3.org/2005/Atom}id').text.split('/')[-1],\n            'year': entry.find('{http://www.w3.org/2005/Atom}published').text[:4],\n            'source': 'arxiv'\n        }\n        papers.append(paper)\n    \n    time.sleep(0.5)  # Rate limiting\n    return papers\n\ndef search_pubmed(query, max_results=10):\n    # Step 1: Search for IDs\n    search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n    search_params = {\n        'db': 'pubmed',\n        'term': query,\n        'retmax': max_results,\n        'retmode': 'json'\n    }\n    \n    search_response = requests.get(search_url, params=search_params).json()\n    pmids = search_response['esearchresult']['idlist']\n    \n    # Step 2: Fetch details for each ID\n    fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n    fetch_params = {\n        'db': 'pubmed',\n        'id': ','.join(pmids),\n        'retmode': 'xml'\n    }\n    \n    fetch_response = requests.get(fetch_url, params=fetch_params)\n    # Parse XML and extract paper details...\n    \n    return papers\n```\n\n**Rate Limiting:**\n- arXiv: 3 requests/second (0.5s delay implemented)\n- PubMed: 3 requests/second (10 with API key)\n\n---\n\n### 5.9 Trend Forecasting\n\n**Library Used:** `numpy`, `scipy` (linear regression)\n\n**Implementation Details:**\n```python\n# utils/trend_forecasting.py\nimport numpy as np\nfrom scipy import stats\n\ndef analyze_method_trends(method_usage_by_year):\n    \"\"\"\n    method_usage_by_year: dict {year: count}\n    Returns: trend classification and forecast\n    \"\"\"\n    years = sorted(method_usage_by_year.keys())\n    counts = [method_usage_by_year[y] for y in years]\n    \n    if len(years) < 3:\n        return {'trend': 'insufficient_data', 'slope': 0}\n    \n    # Linear regression\n    x = np.array(range(len(years)))\n    y = np.array(counts)\n    \n    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n    \n    # Classify trend\n    if slope > 0.5 and p_value < 0.05:\n        trend = 'emerging'\n    elif slope < -0.5 and p_value < 0.05:\n        trend = 'declining'\n    else:\n        trend = 'stable'\n    \n    # Forecast next year\n    next_year_prediction = intercept + slope * len(years)\n    \n    return {\n        'trend': trend,\n        'slope': slope,\n        'r_squared': r_value ** 2,\n        'prediction': max(0, next_year_prediction),\n        'confidence': 1 - p_value\n    }\n```\n\n---\n\n### 5.10 Multi-Format Export\n\n**Implementation Details:**\n```python\n# utils/export_utils.py\n\ndef export_to_markdown(papers):\n    md = \"# Literature Review\\n\\n\"\n    for paper in papers:\n        md += f\"## {paper.title}\\n\\n\"\n        md += f\"**Authors:** {', '.join([a.name for a in paper.authors])}\\n\\n\"\n        md += f\"**Year:** {paper.year}\\n\\n\"\n        md += f\"**Abstract:** {paper.abstract}\\n\\n\"\n        md += \"---\\n\\n\"\n    return md\n\ndef export_to_latex(papers):\n    latex = \"\\\\documentclass{article}\\n\\\\begin{document}\\n\\n\"\n    latex += \"\\\\section{Literature Review}\\n\\n\"\n    for paper in papers:\n        latex += f\"\\\\subsection{{{escape_latex(paper.title)}}}\\n\\n\"\n        latex += f\"\\\\textbf{{Authors:}} {', '.join([a.name for a in paper.authors])}\\n\\n\"\n        latex += f\"\\\\textbf{{Year:}} {paper.year}\\n\\n\"\n        latex += f\"{escape_latex(paper.abstract)}\\n\\n\"\n    latex += \"\\\\end{document}\"\n    return latex\n\ndef export_to_bibtex(papers):\n    bibtex = \"\"\n    for paper in papers:\n        key = generate_citation_key(paper)\n        bibtex += f\"@article{{{key},\\n\"\n        bibtex += f\"  title = {{{paper.title}}},\\n\"\n        bibtex += f\"  author = {{{' and '.join([a.name for a in paper.authors])}}},\\n\"\n        bibtex += f\"  year = {{{paper.year}}},\\n\"\n        if paper.doi:\n            bibtex += f\"  doi = {{{paper.doi}}},\\n\"\n        bibtex += \"}\\n\\n\"\n    return bibtex\n\ndef export_to_csv(papers):\n    import csv\n    import io\n    \n    output = io.StringIO()\n    writer = csv.writer(output)\n    writer.writerow(['Title', 'Authors', 'Year', 'Abstract', 'DOI', 'Methods'])\n    \n    for paper in papers:\n        writer.writerow([\n            paper.title,\n            '; '.join([a.name for a in paper.authors]),\n            paper.year,\n            paper.abstract,\n            paper.doi or '',\n            '; '.join([m.name for m in paper.methods])\n        ])\n    \n    return output.getvalue()\n```\n\n---\n\n### 5.11 Database Design with SQLAlchemy\n\n**Library Used:** `SQLAlchemy` ORM, `PostgreSQL` (production), `SQLite` (fallback)\n\n**Implementation Details:**\n```python\n# models.py\nfrom sqlalchemy import Column, Integer, String, Text, DateTime, ForeignKey, Table\nfrom sqlalchemy.orm import relationship, declarative_base\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.pool import QueuePool\n\nBase = declarative_base()\n\n# Many-to-Many association tables\npaper_authors = Table('paper_authors', Base.metadata,\n    Column('paper_id', Integer, ForeignKey('papers.id')),\n    Column('author_id', Integer, ForeignKey('authors.id'))\n)\n\npaper_methods = Table('paper_methods', Base.metadata,\n    Column('paper_id', Integer, ForeignKey('papers.id')),\n    Column('method_id', Integer, ForeignKey('methods.id'))\n)\n\nclass Paper(Base):\n    __tablename__ = 'papers'\n    \n    id = Column(Integer, primary_key=True)\n    title = Column(String(500), nullable=False)\n    abstract = Column(Text)\n    content = Column(Text)\n    year = Column(Integer)\n    doi = Column(String(100))\n    source = Column(String(50))  # 'pdf', 'arxiv', 'pubmed'\n    source_id = Column(String(100))  # arxiv_id or pmid\n    created_at = Column(DateTime, default=datetime.utcnow)\n    \n    # Relationships\n    authors = relationship('Author', secondary=paper_authors, back_populates='papers')\n    methods = relationship('Method', secondary=paper_methods, back_populates='papers')\n    datasets = relationship('Dataset', secondary=paper_datasets, back_populates='papers')\n    chunks = relationship('PaperChunk', back_populates='paper')\n\n# Database connection with pooling\ndef get_engine():\n    database_url = os.environ.get('DATABASE_URL')\n    \n    if database_url:\n        # PostgreSQL with connection pooling\n        engine = create_engine(\n            database_url,\n            poolclass=QueuePool,\n            pool_size=5,\n            max_overflow=10,\n            pool_pre_ping=True  # Auto-reconnect on stale connections\n        )\n    else:\n        # SQLite fallback for local development\n        engine = create_engine('sqlite:///scholarlens.db')\n    \n    return engine\n```\n\n**Connection Handling:**\n- Pool pre-ping for automatic reconnection\n- Session-per-request pattern\n- Proper session cleanup to prevent leaks\n\n---\n\n### 5.12 Streamlit UI Components\n\n**Key Patterns Used:**\n```python\n# Session State Management\nif 'search_index' not in st.session_state:\n    st.session_state.search_index = SemanticSearchIndex()\n\n# Multi-column layouts\ncol1, col2 = st.columns([3, 1])\nwith col1:\n    st.text_input(\"Search...\")\nwith col2:\n    st.button(\"Submit\")\n\n# Expanders for collapsible content\nwith st.expander(\"Paper Details\"):\n    st.write(paper.abstract)\n\n# Progress indicators\nprogress_bar = st.progress(0)\nfor i, item in enumerate(items):\n    process(item)\n    progress_bar.progress((i + 1) / len(items))\n\n# File downloads\nst.download_button(\n    label=\"Download Report\",\n    data=export_data,\n    file_name=\"report.md\",\n    mime=\"text/markdown\"\n)\n```\n\n---\n\n## 6. Technical Architecture\n\n### 5.1 Technology Stack\n\n| Layer | Technology |\n|-------|------------|\n| **Frontend** | Streamlit (Python) |\n| **Backend** | Python 3.11 |\n| **Database** | PostgreSQL with SQLAlchemy ORM |\n| **AI/NLP** | OpenAI GPT-4o-mini / Google Gemini 2.0 Flash |\n| **Visualization** | Plotly, NetworkX |\n| **PDF Processing** | pdfplumber |\n| **Search** | TF-IDF with scikit-learn |\n| **APIs** | arXiv API, PubMed E-utilities |\n\n### 5.2 System Architecture Diagram\n\n```\n\n                        ScholarLens                               \n\n                \n     Streamlit       AI/NLP         Visualization          \n     Frontend        Engine           Engine               \n                \n                                                              \n            \n                Application Core (Python)                      \n                  \n     PDF       NER       Semantic  Graph              \n    Processor Extractor  Search    Builder            \n                  \n            \n                                                                \n            \n             PostgreSQL Database (SQLAlchemy)                  \n    Papers  Authors  Methods  Datasets  Notes             \n            \n                                                                \n            \n                External APIs                                  \n                     \n     arXiv      PubMed     OpenAI/Gemini                \n                     \n            \n\n```\n\n### 5.3 Database Schema\n\n**Core Tables:**\n- `papers` - Research papers (title, abstract, content, year, DOI, source)\n- `authors` - Author profiles (name, h-index, citations)\n- `institutions` - Research organizations (name, type, location)\n- `methods` - Research methods/techniques (name, category, usage_count)\n- `datasets` - Datasets (name, domain, usage_count)\n\n**Relationship Tables:**\n- `paper_authors` - Paper-Author associations\n- `paper_methods` - Paper-Method associations\n- `paper_datasets` - Paper-Dataset associations\n- `author_institutions` - Author-Institution affiliations\n- `method_prerequisites` - Method dependency chains\n\n**User Content Tables:**\n- `paper_chunks` - Text chunks for semantic search/RAG\n- `flashcards` - Generated study flashcards\n- `notes` - User notes on papers\n- `saved_queries` - Q&A history\n- `reading_list` - User reading lists\n\n### 5.4 Module Structure\n\n```\n/\n app.py                    # Main Streamlit application (1300+ lines)\n models.py                 # SQLAlchemy database models\n utils/\n    __init__.py\n    pdf_processor.py      # PDF text extraction and section parsing\n    ner_extractor.py      # Named Entity Recognition (regex-based)\n    openai_helper.py      # OpenAI/Gemini API integration\n    semantic_search.py    # TF-IDF vectorization and search\n    graph_builder.py      # NetworkX knowledge graph construction\n    analytics.py          # SQL analytics (8+ reports)\n    arxiv_pubmed.py       # External API integration\n    topic_modeling.py     # K-means clustering, LDA\n    trend_forecasting.py  # Time-series analysis\n    export_utils.py       # Multi-format export\n .streamlit/\n     config.toml           # Streamlit configuration\n```\n\n---\n\n## 6. Key Algorithms and Techniques\n\n### 6.1 Named Entity Recognition (NER)\n- **Approach**: Regex pattern matching with domain-specific vocabularies\n- **Entities Extracted**: Methods, datasets, techniques, algorithms\n- **Example Patterns**: \n  - Methods: \"neural network\", \"transformer\", \"BERT\"\n  - Datasets: \"ImageNet\", \"COCO\", \"MNIST\"\n\n### 6.2 Semantic Search\n- **Algorithm**: TF-IDF (Term Frequency-Inverse Document Frequency)\n- **Implementation**: scikit-learn TfidfVectorizer\n- **Adaptive Configuration**: max_df adjusted based on corpus size\n- **Similarity Metric**: Cosine similarity\n\n### 6.3 Topic Clustering\n- **Algorithm**: K-means clustering on TF-IDF vectors\n- **Optimal K Selection**: Silhouette score analysis\n- **Label Generation**: Top keywords per cluster\n\n### 6.4 Knowledge Graph Construction\n- **Library**: NetworkX for graph operations\n- **Visualization**: Plotly for interactive rendering\n- **Node Types**: Papers, Authors, Methods, Datasets\n- **Edge Types**: Authorship, Usage, Prerequisites\n\n### 6.5 RAG (Retrieval-Augmented Generation)\n- **Chunking Strategy**: Sentence-based with overlap\n- **Retrieval**: TF-IDF similarity matching\n- **Generation**: OpenAI GPT-4o-mini or Gemini 2.0 Flash\n- **Citation Format**: Inline references to source papers\n\n### 6.6 Trend Forecasting\n- **Method**: Linear regression on time-series data\n- **Features**: Year-over-year usage counts\n- **Output**: Trajectory classification (emerging/stable/declining)\n\n---\n\n## 7. AI Integration\n\n### 7.1 Supported Providers\n\n| Provider | Model | Cost |\n|----------|-------|------|\n| **OpenAI** | GPT-4o-mini | Paid (API usage) |\n| **Google AI Studio** | Gemini 2.0 Flash | Free tier available |\n\n### 7.2 AI-Powered Features\n\n1. **Multi-Audience Summarization**\n   - Expert: Technical methodology focus\n   - Student: Analogies and simplified explanations\n   - Policymaker: Applications and risks\n\n2. **Question Answering**\n   - Context-aware responses\n   - Source citations\n   - Follow-up suggestions\n\n3. **Content Generation**\n   - Flashcard creation\n   - Quiz question generation\n   - Policy brief drafting\n\n4. **Cross-Domain Analogies**\n   - Explain ML concepts using cooking analogies\n   - Translate technical jargon to everyday language\n\n---\n\n## 8. Security and Data Privacy\n\n### 8.1 Security Measures\n- **Environment Variables**: Sensitive keys stored in environment secrets\n- **Database**: PostgreSQL with connection pooling and SSL\n- **No Data Sharing**: User uploads remain private\n- **Session Isolation**: Per-user session state management\n\n### 8.2 API Key Management\n- Keys stored as environment secrets (not in code)\n- Support for multiple AI providers\n- Graceful degradation when APIs unavailable\n\n---\n\n## 9. Deployment Options\n\n### 9.1 Cloud Deployment (Replit)\n- **Database**: PostgreSQL (Neon-backed)\n- **Hosting**: Replit infrastructure\n- **URL**: Accessible via Replit domain or custom domain\n- **Scaling**: Automatic based on usage\n\n### 9.2 Local Deployment\n- **Requirements**: Python 3.11+, PostgreSQL (optional)\n- **Fallback**: SQLite for offline use\n- **Configuration**: `.env` file with API keys\n\n---\n\n## 10. Performance Considerations\n\n### 10.1 Optimizations\n- **Connection Pooling**: SQLAlchemy pool with pre-ping\n- **Lazy Loading**: On-demand data fetching\n- **Caching**: Session state for expensive computations\n- **Batch Processing**: PDF upload batching\n\n### 10.2 Scalability\n- **Database**: PostgreSQL supports large datasets\n- **Search**: TF-IDF scales to thousands of documents\n- **Graph**: NetworkX handles moderate graph sizes\n\n---\n\n## 11. Future Roadmap\n\n### Planned Enhancements\n1. **Citation Network Analysis**: Paper citation graphs\n2. **Collaborative Filtering**: Personalized paper recommendations\n3. **PDF Annotation**: In-document highlighting and notes\n4. **Multi-Language Support**: Translation of non-English papers\n5. **Mobile Responsive Design**: Improved mobile experience\n6. **API Endpoints**: RESTful API for external integration\n7. **Advanced NER**: Transformer-based entity extraction\n8. **Real-time Collaboration**: Multi-user workspaces\n\n---\n\n## 12. Conclusion\n\nScholarLens represents a significant advancement in research intelligence tools, combining modern AI capabilities with intuitive user interfaces to democratize access to academic knowledge. By automating tedious tasks like entity extraction, literature organization, and trend analysis, ScholarLens enables researchers to focus on what matters most: advancing human knowledge.\n\nThe platform's multi-audience approach ensures that complex research findings can be understood by experts, students, and decision-makers alike, bridging the gap between academic research and real-world application.\n\n---\n\n## Appendix A: Installation Guide\n\n### Cloud (Replit)\n1. Fork the project on Replit\n2. Set `OPENAI_API_KEY` or `GOOGLE_API_KEY` in Secrets\n3. Click \"Run\" to start the application\n\n### Local Setup\n```bash\n# Clone repository\ngit clone <repository-url>\ncd scholarlens\n\n# Install dependencies\npip install -r requirements.txt\n\n# Configure environment\ncp .env.example .env\n# Edit .env with your API keys\n\n# Run application\nstreamlit run app.py --server.port 5000\n```\n\n---\n\n## Appendix B: API Reference\n\n### arXiv API\n- **Endpoint**: `http://export.arxiv.org/api/query`\n- **Parameters**: `search_query`, `max_results`, `start`\n- **Rate Limit**: 3 requests per second\n\n### PubMed API\n- **Search**: `https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi`\n- **Fetch**: `https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi`\n- **Rate Limit**: 3 requests per second (10 with API key)\n\n---\n\n## Appendix C: Glossary\n\n| Term | Definition |\n|------|------------|\n| **RAG** | Retrieval-Augmented Generation - AI technique combining search with LLMs |\n| **TF-IDF** | Term Frequency-Inverse Document Frequency - Text vectorization method |\n| **NER** | Named Entity Recognition - Extracting structured entities from text |\n| **Knowledge Graph** | Network representation of entities and relationships |\n| **DAG** | Directed Acyclic Graph - Used for prerequisite chains |\n\n---\n\n*Document Version: 1.0*  \n*Last Updated: December 2024*  \n*Platform: ScholarLens v1.0*\n","path":null,"size_bytes":37359,"size_tokens":null},"utils/trend_forecasting.py":{"content":"\"\"\"\nTrend forecasting for ScholarLens\nTime-series analysis of method/dataset popularity with predictions\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Dict, Tuple, Optional\nfrom datetime import datetime\nfrom collections import defaultdict\n\n\nclass TrendForecaster:\n    \"\"\"\n    Simple trend forecasting using linear regression and moving averages\n    \"\"\"\n    \n    def __init__(self):\n        self.data = {}\n    \n    def fit(self, years: List[int], counts: List[int]):\n        \"\"\"\n        Fit trend model to historical data\n        \"\"\"\n        if len(years) < 2:\n            self.slope = 0\n            self.intercept = counts[0] if counts else 0\n            return\n        \n        x = np.array(years)\n        y = np.array(counts)\n        \n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n        \n        numerator = np.sum((x - x_mean) * (y - y_mean))\n        denominator = np.sum((x - x_mean) ** 2)\n        \n        if denominator == 0:\n            self.slope = 0\n            self.intercept = y_mean\n        else:\n            self.slope = numerator / denominator\n            self.intercept = y_mean - self.slope * x_mean\n    \n    def predict(self, future_years: List[int]) -> List[float]:\n        \"\"\"\n        Predict values for future years\n        \"\"\"\n        predictions = []\n        for year in future_years:\n            pred = self.slope * year + self.intercept\n            predictions.append(max(0, pred))\n        return predictions\n    \n    def get_trend_direction(self) -> str:\n        \"\"\"\n        Get trend direction (rising, falling, stable)\n        \"\"\"\n        if self.slope > 0.5:\n            return \"rising\"\n        elif self.slope < -0.5:\n            return \"falling\"\n        else:\n            return \"stable\"\n    \n    def get_growth_rate(self) -> float:\n        \"\"\"\n        Get annual growth rate percentage\n        \"\"\"\n        if self.intercept == 0:\n            return 0.0\n        return (self.slope / self.intercept) * 100\n\n\ndef analyze_method_trends(method_data: List[Dict]) -> Dict:\n    \"\"\"\n    Analyze trends for methods over time\n    \n    Args:\n        method_data: List of dicts with 'method', 'year', 'count' keys\n    \n    Returns:\n        Dictionary with trend analysis results\n    \"\"\"\n    if not method_data:\n        return {}\n    \n    method_series = defaultdict(lambda: defaultdict(int))\n    for item in method_data:\n        method = item.get('method', 'Unknown')\n        year = item.get('year')\n        count = item.get('count', 0)\n        if year:\n            method_series[method][year] += count\n    \n    trends = {}\n    current_year = datetime.now().year\n    future_years = [current_year + 1, current_year + 2, current_year + 3]\n    \n    for method, year_counts in method_series.items():\n        years = sorted(year_counts.keys())\n        counts = [year_counts[y] for y in years]\n        \n        if len(years) < 2:\n            continue\n        \n        forecaster = TrendForecaster()\n        forecaster.fit(years, counts)\n        \n        predictions = forecaster.predict(future_years)\n        \n        recent_years = years[-3:] if len(years) >= 3 else years\n        recent_counts = [year_counts[y] for y in recent_years]\n        \n        if len(recent_counts) >= 2:\n            recent_change = recent_counts[-1] - recent_counts[0]\n            recent_growth = (recent_change / recent_counts[0] * 100) if recent_counts[0] > 0 else 0\n        else:\n            recent_growth = 0\n        \n        total_count = sum(counts)\n        peak_year = years[counts.index(max(counts))]\n        \n        trends[method] = {\n            'historical': {year: count for year, count in zip(years, counts)},\n            'predictions': {year: pred for year, pred in zip(future_years, predictions)},\n            'trend_direction': forecaster.get_trend_direction(),\n            'growth_rate': forecaster.get_growth_rate(),\n            'recent_growth': recent_growth,\n            'total_count': total_count,\n            'peak_year': peak_year,\n            'peak_count': max(counts),\n            'first_appeared': min(years),\n            'latest_year': max(years)\n        }\n    \n    return trends\n\n\ndef identify_emerging_methods(trends: Dict, min_recent_growth: float = 20.0) -> List[Dict]:\n    \"\"\"\n    Identify methods that are emerging (high recent growth)\n    \"\"\"\n    emerging = []\n    \n    for method, data in trends.items():\n        if data['recent_growth'] >= min_recent_growth and data['trend_direction'] == 'rising':\n            emerging.append({\n                'method': method,\n                'recent_growth': data['recent_growth'],\n                'growth_rate': data['growth_rate'],\n                'predictions': data['predictions']\n            })\n    \n    emerging.sort(key=lambda x: x['recent_growth'], reverse=True)\n    return emerging\n\n\ndef identify_declining_methods(trends: Dict, min_decline: float = -20.0) -> List[Dict]:\n    \"\"\"\n    Identify methods that are declining\n    \"\"\"\n    declining = []\n    \n    for method, data in trends.items():\n        if data['recent_growth'] <= min_decline and data['trend_direction'] == 'falling':\n            declining.append({\n                'method': method,\n                'recent_growth': data['recent_growth'],\n                'peak_year': data['peak_year'],\n                'peak_count': data['peak_count']\n            })\n    \n    declining.sort(key=lambda x: x['recent_growth'])\n    return declining\n\n\ndef calculate_moving_average(data: List[Tuple[int, int]], window: int = 3) -> List[Tuple[int, float]]:\n    \"\"\"\n    Calculate moving average for time series\n    \"\"\"\n    if len(data) < window:\n        return [(year, float(count)) for year, count in data]\n    \n    sorted_data = sorted(data, key=lambda x: x[0])\n    years = [d[0] for d in sorted_data]\n    counts = [d[1] for d in sorted_data]\n    \n    result = []\n    for i in range(len(counts)):\n        start_idx = max(0, i - window + 1)\n        window_vals = counts[start_idx:i + 1]\n        avg = sum(window_vals) / len(window_vals)\n        result.append((years[i], avg))\n    \n    return result\n\n\ndef compare_method_trajectories(method_trends: Dict, methods: List[str]) -> Dict:\n    \"\"\"\n    Compare trajectories of multiple methods\n    \"\"\"\n    comparison = {}\n    \n    for method in methods:\n        if method not in method_trends:\n            continue\n        \n        data = method_trends[method]\n        comparison[method] = {\n            'historical': data['historical'],\n            'predictions': data['predictions'],\n            'trend': data['trend_direction'],\n            'growth': data['growth_rate']\n        }\n    \n    return comparison\n\n\ndef generate_forecast_summary(trends: Dict) -> str:\n    \"\"\"\n    Generate a natural language summary of trends\n    \"\"\"\n    if not trends:\n        return \"No trend data available.\"\n    \n    rising = [m for m, d in trends.items() if d['trend_direction'] == 'rising']\n    falling = [m for m, d in trends.items() if d['trend_direction'] == 'falling']\n    stable = [m for m, d in trends.items() if d['trend_direction'] == 'stable']\n    \n    summary_parts = []\n    \n    if rising:\n        top_rising = sorted(rising, key=lambda m: trends[m]['recent_growth'], reverse=True)[:3]\n        summary_parts.append(f\"**Rising trends:** {', '.join(top_rising)}\")\n    \n    if falling:\n        top_falling = sorted(falling, key=lambda m: trends[m]['recent_growth'])[:3]\n        summary_parts.append(f\"**Declining:** {', '.join(top_falling)}\")\n    \n    if stable:\n        summary_parts.append(f\"**Stable methods:** {len(stable)} methods showing steady usage\")\n    \n    all_methods = list(trends.items())\n    if all_methods:\n        newest = max(all_methods, key=lambda x: x[1]['first_appeared'])\n        summary_parts.append(f\"**Newest:** {newest[0]} (first appeared {newest[1]['first_appeared']})\")\n    \n    return \"\\n\\n\".join(summary_parts)\n\n\ndef create_timeline_data(trends: Dict, methods: List[str] = None) -> Dict:\n    \"\"\"\n    Create data for timeline visualization\n    \"\"\"\n    if methods is None:\n        methods = list(trends.keys())[:10]\n    \n    timeline_data = {\n        'years': set(),\n        'series': {}\n    }\n    \n    for method in methods:\n        if method not in trends:\n            continue\n        \n        data = trends[method]\n        all_years = list(data['historical'].keys()) + list(data['predictions'].keys())\n        timeline_data['years'].update(all_years)\n        \n        timeline_data['series'][method] = {\n            'historical': data['historical'],\n            'predicted': data['predictions']\n        }\n    \n    timeline_data['years'] = sorted(timeline_data['years'])\n    \n    return timeline_data\n","path":null,"size_bytes":8599,"size_tokens":null},"models.py":{"content":"\"\"\"\nDatabase models for ScholarLens - AI-Powered Research Intelligence Platform\nUses SQLAlchemy ORM for PostgreSQL database management\n\"\"\"\n\nimport os\nfrom datetime import datetime\nfrom sqlalchemy import (\n    create_engine, Column, Integer, String, Text, DateTime, Float,\n    ForeignKey, Table, Boolean, JSON, UniqueConstraint\n)\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import relationship, sessionmaker\n\ntry:\n    from dotenv import load_dotenv\n    load_dotenv()\nexcept ImportError:\n    pass\n\nDATABASE_URL = os.environ.get(\"DATABASE_URL\")\n\nif not DATABASE_URL:\n    print(\"WARNING: DATABASE_URL not set. Please create a .env file with:\")\n    print(\"DATABASE_URL=postgresql://username:password@localhost:5432/scholarlens\")\n    print(\"OPENAI_API_KEY=your-openai-api-key\")\n    DATABASE_URL = \"sqlite:///scholarlens.db\"\n    print(\"Using SQLite as fallback database.\")\n\nif DATABASE_URL.startswith(\"postgresql\"):\n    engine = create_engine(\n        DATABASE_URL,\n        pool_pre_ping=True,\n        pool_recycle=300,\n        pool_size=5,\n        max_overflow=10,\n        connect_args={\"connect_timeout\": 10}\n    )\nelse:\n    engine = create_engine(DATABASE_URL)\n\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nBase = declarative_base()\n\n# Association tables for many-to-many relationships\npaper_authors = Table(\n    'paper_authors', Base.metadata,\n    Column('paper_id', Integer, ForeignKey('papers.id', ondelete='CASCADE'), primary_key=True, index=True),\n    Column('author_id', Integer, ForeignKey('authors.id', ondelete='CASCADE'), primary_key=True, index=True),\n    Column('author_position', Integer, default=0)\n)\n\npaper_methods = Table(\n    'paper_methods', Base.metadata,\n    Column('paper_id', Integer, ForeignKey('papers.id', ondelete='CASCADE'), primary_key=True, index=True),\n    Column('method_id', Integer, ForeignKey('methods.id', ondelete='CASCADE'), primary_key=True, index=True)\n)\n\npaper_datasets = Table(\n    'paper_datasets', Base.metadata,\n    Column('paper_id', Integer, ForeignKey('papers.id', ondelete='CASCADE'), primary_key=True, index=True),\n    Column('dataset_id', Integer, ForeignKey('datasets.id', ondelete='CASCADE'), primary_key=True, index=True)\n)\n\nauthor_institutions = Table(\n    'author_institutions', Base.metadata,\n    Column('author_id', Integer, ForeignKey('authors.id', ondelete='CASCADE'), primary_key=True, index=True),\n    Column('institution_id', Integer, ForeignKey('institutions.id', ondelete='CASCADE'), primary_key=True, index=True)\n)\n\nmethod_prerequisites = Table(\n    'method_prerequisites', Base.metadata,\n    Column('method_id', Integer, ForeignKey('methods.id', ondelete='CASCADE'), primary_key=True, index=True),\n    Column('prerequisite_id', Integer, ForeignKey('methods.id', ondelete='CASCADE'), primary_key=True, index=True)\n)\n\n\nclass Paper(Base):\n    \"\"\"Research paper model\"\"\"\n    __tablename__ = 'papers'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    title = Column(String(500), nullable=False)\n    abstract = Column(Text)\n    content = Column(Text)\n    source = Column(String(50))  # 'pdf', 'arxiv', 'pubmed'\n    source_id = Column(String(100))  # arxiv id, pubmed id, etc.\n    doi = Column(String(100))\n    publication_date = Column(DateTime)\n    year = Column(Integer, index=True)\n    venue = Column(String(200))\n    citation_count = Column(Integer, default=0)\n    pdf_path = Column(String(500))\n    embedding = Column(JSON)  # Store vector embedding as JSON\n    topics = Column(JSON)  # Store extracted topics\n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    \n    # Relationships\n    authors = relationship('Author', secondary=paper_authors, back_populates='papers')\n    methods = relationship('Method', secondary=paper_methods, back_populates='papers')\n    datasets = relationship('Dataset', secondary=paper_datasets, back_populates='papers')\n    chunks = relationship('PaperChunk', back_populates='paper', cascade='all, delete-orphan')\n    notes = relationship('Note', back_populates='paper', cascade='all, delete-orphan')\n    flashcards = relationship('Flashcard', back_populates='paper', cascade='all, delete-orphan')\n\n\nclass Author(Base):\n    \"\"\"Author model\"\"\"\n    __tablename__ = 'authors'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    name = Column(String(200), nullable=False, index=True)\n    email = Column(String(200))\n    orcid = Column(String(50))\n    h_index = Column(Integer, default=0)\n    total_citations = Column(Integer, default=0)\n    research_interests = Column(JSON)\n    created_at = Column(DateTime, default=datetime.utcnow)\n    \n    # Relationships\n    papers = relationship('Paper', secondary=paper_authors, back_populates='authors')\n    institutions = relationship('Institution', secondary=author_institutions, back_populates='authors')\n    \n    __table_args__ = (UniqueConstraint('name', name='uq_author_name'),)\n\n\nclass Institution(Base):\n    \"\"\"Research institution model\"\"\"\n    __tablename__ = 'institutions'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    name = Column(String(300), nullable=False, index=True)\n    country = Column(String(100))\n    city = Column(String(100))\n    type = Column(String(50))  # university, company, research_lab\n    website = Column(String(300))\n    created_at = Column(DateTime, default=datetime.utcnow)\n    \n    # Relationships\n    authors = relationship('Author', secondary=author_institutions, back_populates='institutions')\n    \n    __table_args__ = (UniqueConstraint('name', name='uq_institution_name'),)\n\n\nclass Method(Base):\n    \"\"\"Research method/technique model\"\"\"\n    __tablename__ = 'methods'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    name = Column(String(200), nullable=False, index=True)\n    description = Column(Text)\n    category = Column(String(100))  # e.g., 'deep_learning', 'nlp', 'computer_vision'\n    first_appeared_year = Column(Integer)\n    usage_count = Column(Integer, default=0)\n    created_at = Column(DateTime, default=datetime.utcnow)\n    \n    # Relationships\n    papers = relationship('Paper', secondary=paper_methods, back_populates='methods')\n    prerequisites = relationship(\n        'Method',\n        secondary=method_prerequisites,\n        primaryjoin=id == method_prerequisites.c.method_id,\n        secondaryjoin=id == method_prerequisites.c.prerequisite_id,\n        backref='dependent_methods'\n    )\n    \n    __table_args__ = (UniqueConstraint('name', name='uq_method_name'),)\n\n\nclass Dataset(Base):\n    \"\"\"Dataset model\"\"\"\n    __tablename__ = 'datasets'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    name = Column(String(200), nullable=False, index=True)\n    description = Column(Text)\n    domain = Column(String(100))\n    size = Column(String(100))\n    url = Column(String(500))\n    usage_count = Column(Integer, default=0)\n    created_at = Column(DateTime, default=datetime.utcnow)\n    \n    # Relationships\n    papers = relationship('Paper', secondary=paper_datasets, back_populates='datasets')\n    \n    __table_args__ = (UniqueConstraint('name', name='uq_dataset_name'),)\n\n\nclass PaperChunk(Base):\n    \"\"\"Paper content chunks for RAG\"\"\"\n    __tablename__ = 'paper_chunks'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    paper_id = Column(Integer, ForeignKey('papers.id', ondelete='CASCADE'), nullable=False)\n    chunk_index = Column(Integer, nullable=False)\n    content = Column(Text, nullable=False)\n    section = Column(String(100))  # abstract, introduction, methods, results, etc.\n    embedding = Column(JSON)  # Store vector embedding as JSON\n    created_at = Column(DateTime, default=datetime.utcnow)\n    \n    # Relationships\n    paper = relationship('Paper', back_populates='chunks')\n\n\nclass Note(Base):\n    \"\"\"User notes on papers\"\"\"\n    __tablename__ = 'notes'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    paper_id = Column(Integer, ForeignKey('papers.id', ondelete='CASCADE'), nullable=False)\n    content = Column(Text, nullable=False)\n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    \n    # Relationships\n    paper = relationship('Paper', back_populates='notes')\n\n\nclass Flashcard(Base):\n    \"\"\"Flashcards generated from papers\"\"\"\n    __tablename__ = 'flashcards'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    paper_id = Column(Integer, ForeignKey('papers.id', ondelete='CASCADE'), nullable=False)\n    question = Column(Text, nullable=False)\n    answer = Column(Text, nullable=False)\n    difficulty = Column(String(20), default='medium')\n    times_reviewed = Column(Integer, default=0)\n    correct_count = Column(Integer, default=0)\n    created_at = Column(DateTime, default=datetime.utcnow)\n    \n    # Relationships\n    paper = relationship('Paper', back_populates='flashcards')\n\n\nclass SavedQuery(Base):\n    \"\"\"Saved user queries\"\"\"\n    __tablename__ = 'saved_queries'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    query = Column(Text, nullable=False)\n    response = Column(Text)\n    sources = Column(JSON)\n    created_at = Column(DateTime, default=datetime.utcnow)\n\n\nclass ReadingList(Base):\n    \"\"\"Reading list items\"\"\"\n    __tablename__ = 'reading_list'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    paper_id = Column(Integer, ForeignKey('papers.id', ondelete='CASCADE'), nullable=False)\n    priority = Column(Integer, default=0)\n    status = Column(String(20), default='unread')  # unread, reading, completed\n    added_at = Column(DateTime, default=datetime.utcnow)\n\n\ndef init_db():\n    \"\"\"Initialize database tables\"\"\"\n    Base.metadata.create_all(bind=engine)\n\n\ndef get_db():\n    \"\"\"Get database session\"\"\"\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n\n\ndef get_session():\n    \"\"\"Get a new database session\"\"\"\n    return SessionLocal()\n","path":null,"size_bytes":9995,"size_tokens":null},"utils/ner_extractor.py":{"content":"\"\"\"\nNamed Entity Recognition for ScholarLens\nExtracts methods, datasets, authors, and institutions from research papers\n\"\"\"\n\nimport re\nfrom typing import List, Dict, Set, Tuple\n\n\nKNOWN_METHODS = {\n    'neural network', 'deep learning', 'machine learning', 'cnn', 'rnn', 'lstm',\n    'transformer', 'bert', 'gpt', 'attention mechanism', 'self-attention',\n    'convolutional neural network', 'recurrent neural network', 'gan',\n    'generative adversarial network', 'autoencoder', 'variational autoencoder',\n    'reinforcement learning', 'supervised learning', 'unsupervised learning',\n    'transfer learning', 'fine-tuning', 'pre-training', 'embedding',\n    'word2vec', 'glove', 'fasttext', 'elmo', 'xlnet', 'roberta', 'albert',\n    't5', 'bart', 'gpt-2', 'gpt-3', 'gpt-4', 'llama', 'palm', 'claude',\n    'diffusion model', 'stable diffusion', 'dalle', 'midjourney',\n    'random forest', 'decision tree', 'svm', 'support vector machine',\n    'logistic regression', 'linear regression', 'naive bayes',\n    'k-means', 'clustering', 'pca', 'principal component analysis',\n    'gradient descent', 'sgd', 'adam', 'optimizer', 'backpropagation',\n    'dropout', 'batch normalization', 'layer normalization',\n    'resnet', 'vgg', 'inception', 'efficientnet', 'mobilenet',\n    'yolo', 'faster rcnn', 'mask rcnn', 'unet', 'segmentation',\n    'object detection', 'image classification', 'semantic segmentation',\n    'named entity recognition', 'ner', 'pos tagging', 'parsing',\n    'sentiment analysis', 'text classification', 'question answering',\n    'machine translation', 'summarization', 'text generation',\n    'knowledge graph', 'graph neural network', 'gnn', 'gcn',\n    'contrastive learning', 'self-supervised learning', 'few-shot learning',\n    'zero-shot learning', 'meta-learning', 'multi-task learning',\n    'cross-entropy loss', 'mse loss', 'focal loss', 'triplet loss',\n    'beam search', 'greedy decoding', 'nucleus sampling', 'top-k sampling',\n    'rag', 'retrieval augmented generation', 'chain of thought', 'cot',\n    'prompt engineering', 'in-context learning', 'instruction tuning',\n    'rlhf', 'reinforcement learning from human feedback', 'dpo',\n}\n\nKNOWN_DATASETS = {\n    'imagenet', 'cifar-10', 'cifar-100', 'mnist', 'fashion-mnist',\n    'coco', 'pascal voc', 'cityscapes', 'ade20k', 'lvis',\n    'squad', 'glue', 'superglue', 'mrpc', 'sst-2', 'mnli', 'qnli',\n    'wikitext', 'penn treebank', 'ptb', 'billion word', 'bookcorpus',\n    'wikipedia', 'common crawl', 'c4', 'pile', 'redpajama',\n    'imdb', 'yelp', 'amazon reviews', 'sentiment140',\n    'conll-2003', 'ontonotes', 'ace2005',\n    'wmt', 'iwslt', 'europarl', 'opus',\n    'ms marco', 'natural questions', 'triviaqa', 'hotpotqa',\n    'pubmed', 'arxiv', 'semantic scholar',\n    'kinetics', 'ucf101', 'hmdb51', 'youtube-8m',\n    'librispeech', 'voxceleb', 'commonvoice',\n    'laion', 'conceptual captions', 'cc3m', 'cc12m',\n    'webtext', 'openwebtext', 'refinedweb',\n    'humaneval', 'mbpp', 'apps', 'code contest',\n    'mmlu', 'hellaswag', 'winogrande', 'arc',\n    'gsm8k', 'math', 'big-bench',\n}\n\nINSTITUTION_PATTERNS = [\n    r'University of [A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*',\n    r'[A-Z][a-z]+\\s+University',\n    r'[A-Z][a-z]+\\s+Institute of Technology',\n    r'MIT|Stanford|Harvard|Oxford|Cambridge|Berkeley|CMU|Caltech',\n    r'Google(?:\\s+Research)?|Microsoft(?:\\s+Research)?|Meta(?:\\s+AI)?|OpenAI|DeepMind|Anthropic',\n    r'IBM(?:\\s+Research)?|Amazon(?:\\s+AI)?|Apple(?:\\s+ML)?|NVIDIA(?:\\s+Research)?',\n    r'[A-Z][a-z]+\\s+Labs?',\n    r'[A-Z][a-z]+\\s+Research(?:\\s+Center)?',\n]\n\n\ndef extract_entities(text: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Extract all entity types from text\n    \"\"\"\n    return {\n        'methods': extract_methods(text),\n        'datasets': extract_datasets(text),\n        'authors': extract_authors(text),\n        'institutions': extract_institutions(text),\n    }\n\n\ndef extract_methods(text: str) -> List[Dict]:\n    \"\"\"\n    Extract research methods and techniques from text\n    \"\"\"\n    text_lower = text.lower()\n    found_methods = []\n    seen = set()\n    \n    for method in KNOWN_METHODS:\n        if method in text_lower and method not in seen:\n            count = text_lower.count(method)\n            context = find_context(text, method)\n            found_methods.append({\n                'name': method.title(),\n                'raw_name': method,\n                'count': count,\n                'context': context,\n                'category': categorize_method(method)\n            })\n            seen.add(method)\n    \n    abbreviation_patterns = [\n        r'\\b([A-Z]{2,6})\\s*\\(',\n        r'\\(\\s*([A-Z]{2,6})\\s*\\)',\n    ]\n    \n    for pattern in abbreviation_patterns:\n        matches = re.findall(pattern, text)\n        for match in matches:\n            if len(match) >= 2 and match.lower() not in seen:\n                if is_likely_method_abbrev(match):\n                    found_methods.append({\n                        'name': match,\n                        'raw_name': match.lower(),\n                        'count': text.count(match),\n                        'context': find_context(text, match),\n                        'category': 'unknown'\n                    })\n                    seen.add(match.lower())\n    \n    return sorted(found_methods, key=lambda x: x['count'], reverse=True)\n\n\ndef extract_datasets(text: str) -> List[Dict]:\n    \"\"\"\n    Extract datasets from text\n    \"\"\"\n    text_lower = text.lower()\n    found_datasets = []\n    seen = set()\n    \n    for dataset in KNOWN_DATASETS:\n        if dataset in text_lower and dataset not in seen:\n            count = text_lower.count(dataset)\n            context = find_context(text, dataset)\n            found_datasets.append({\n                'name': dataset.upper() if len(dataset) <= 6 else dataset.title(),\n                'raw_name': dataset,\n                'count': count,\n                'context': context,\n                'domain': categorize_dataset(dataset)\n            })\n            seen.add(dataset)\n    \n    dataset_patterns = [\n        r'([A-Z][a-z]*(?:-[A-Z]?[a-z]+)*)\\s+dataset',\n        r'dataset\\s+called\\s+([A-Z][a-z]+(?:-[a-z]+)*)',\n        r'benchmark(?:ed)?\\s+on\\s+([A-Z][a-z]+(?:-[0-9]+)?)',\n    ]\n    \n    for pattern in dataset_patterns:\n        matches = re.findall(pattern, text)\n        for match in matches:\n            if match.lower() not in seen and len(match) > 2:\n                found_datasets.append({\n                    'name': match,\n                    'raw_name': match.lower(),\n                    'count': 1,\n                    'context': find_context(text, match),\n                    'domain': 'unknown'\n                })\n                seen.add(match.lower())\n    \n    return sorted(found_datasets, key=lambda x: x['count'], reverse=True)\n\n\ndef extract_authors(text: str) -> List[Dict]:\n    \"\"\"\n    Extract author names from text\n    \"\"\"\n    authors = []\n    seen = set()\n    \n    header_text = text[:3000]\n    \n    name_patterns = [\n        r'([A-Z][a-z]+(?:\\s+[A-Z]\\.?)?\\s+[A-Z][a-z]+)',\n        r'([A-Z]\\.\\s*[A-Z][a-z]+)',\n        r'([A-Z][a-z]+,\\s*[A-Z]\\.)',\n    ]\n    \n    for pattern in name_patterns:\n        matches = re.findall(pattern, header_text)\n        for match in matches:\n            name = match.strip()\n            name_key = name.lower().replace('.', '').replace(',', '')\n            \n            if name_key not in seen and is_likely_author_name(name):\n                authors.append({\n                    'name': name,\n                    'normalized_name': normalize_author_name(name)\n                })\n                seen.add(name_key)\n    \n    return authors[:20]\n\n\ndef extract_institutions(text: str) -> List[Dict]:\n    \"\"\"\n    Extract institutions from text\n    \"\"\"\n    institutions = []\n    seen = set()\n    \n    header_text = text[:5000]\n    \n    for pattern in INSTITUTION_PATTERNS:\n        matches = re.findall(pattern, header_text)\n        for match in matches:\n            if match.lower() not in seen:\n                institutions.append({\n                    'name': match,\n                    'type': categorize_institution(match)\n                })\n                seen.add(match.lower())\n    \n    return institutions\n\n\ndef find_context(text: str, term: str, window: int = 100) -> str:\n    \"\"\"\n    Find surrounding context for a term\n    \"\"\"\n    text_lower = text.lower()\n    term_lower = term.lower()\n    \n    idx = text_lower.find(term_lower)\n    if idx == -1:\n        return \"\"\n    \n    start = max(0, idx - window)\n    end = min(len(text), idx + len(term) + window)\n    \n    context = text[start:end].strip()\n    context = re.sub(r'\\s+', ' ', context)\n    \n    return context\n\n\ndef categorize_method(method: str) -> str:\n    \"\"\"\n    Categorize a method into a domain\n    \"\"\"\n    method_lower = method.lower()\n    \n    nlp_keywords = ['bert', 'gpt', 'transformer', 'attention', 'language', 'text', 'nlp', 'word', 'embedding', 'translation', 'summarization', 'ner', 'pos', 'parsing']\n    cv_keywords = ['cnn', 'image', 'vision', 'detection', 'segmentation', 'resnet', 'vgg', 'yolo', 'rcnn', 'unet']\n    dl_keywords = ['neural', 'deep', 'learning', 'network', 'layer', 'backpropagation', 'gradient', 'optimizer']\n    gen_keywords = ['generative', 'gan', 'diffusion', 'autoencoder', 'vae', 'dalle', 'stable']\n    \n    if any(kw in method_lower for kw in nlp_keywords):\n        return 'nlp'\n    elif any(kw in method_lower for kw in cv_keywords):\n        return 'computer_vision'\n    elif any(kw in method_lower for kw in gen_keywords):\n        return 'generative_ai'\n    elif any(kw in method_lower for kw in dl_keywords):\n        return 'deep_learning'\n    else:\n        return 'machine_learning'\n\n\ndef categorize_dataset(dataset: str) -> str:\n    \"\"\"\n    Categorize a dataset into a domain\n    \"\"\"\n    dataset_lower = dataset.lower()\n    \n    if any(kw in dataset_lower for kw in ['imagenet', 'cifar', 'mnist', 'coco', 'voc', 'cityscapes', 'ade', 'lvis', 'laion']):\n        return 'computer_vision'\n    elif any(kw in dataset_lower for kw in ['squad', 'glue', 'wiki', 'book', 'web', 'crawl', 'pile', 'imdb', 'conll', 'wmt']):\n        return 'nlp'\n    elif any(kw in dataset_lower for kw in ['kinetics', 'ucf', 'hmdb', 'youtube']):\n        return 'video'\n    elif any(kw in dataset_lower for kw in ['libri', 'vox', 'voice']):\n        return 'audio'\n    elif any(kw in dataset_lower for kw in ['humaneval', 'mbpp', 'code']):\n        return 'code'\n    else:\n        return 'general'\n\n\ndef categorize_institution(name: str) -> str:\n    \"\"\"\n    Categorize institution type\n    \"\"\"\n    name_lower = name.lower()\n    \n    if 'university' in name_lower or 'institute' in name_lower or 'college' in name_lower:\n        return 'academic'\n    elif any(company in name_lower for company in ['google', 'microsoft', 'meta', 'amazon', 'apple', 'nvidia', 'ibm']):\n        return 'industry'\n    elif 'research' in name_lower or 'lab' in name_lower:\n        return 'research_lab'\n    else:\n        return 'other'\n\n\ndef is_likely_method_abbrev(abbrev: str) -> bool:\n    \"\"\"\n    Check if an abbreviation is likely a method name\n    \"\"\"\n    excluded = {'THE', 'AND', 'FOR', 'WITH', 'FROM', 'THIS', 'THAT', 'HAVE', 'BEEN', 'ALSO', 'MORE', 'WERE', 'OUR', 'ARE', 'BUT', 'NOT', 'CAN', 'ALL', 'WAS', 'HAS'}\n    return abbrev not in excluded and len(abbrev) >= 2\n\n\ndef is_likely_author_name(name: str) -> bool:\n    \"\"\"\n    Check if a string is likely an author name\n    \"\"\"\n    excluded_words = {\n        'the', 'and', 'for', 'with', 'from', 'this', 'that', 'which', 'where', 'when',\n        'abstract', 'introduction', 'method', 'result', 'conclusion', 'figure', 'table', \n        'section', 'chapter', 'appendix', 'reference', 'acknowledgment',\n        'model', 'network', 'learning', 'neural', 'deep', 'machine', 'algorithm',\n        'encoding', 'decoding', 'training', 'testing', 'validation', 'evaluation',\n        'function', 'loss', 'optimization', 'gradient', 'layer', 'weight', 'bias',\n        'attention', 'transformer', 'embedding', 'vector', 'matrix', 'tensor',\n        'linear', 'nonlinear', 'convolutional', 'recurrent', 'generative', 'discriminative',\n        'classification', 'regression', 'segmentation', 'detection', 'recognition',\n        'prediction', 'generation', 'translation', 'summarization', 'extraction',\n        'distance', 'signed', 'field', 'control', 'avoidance', 'collision', 'trajectory',\n        'policy', 'reward', 'state', 'action', 'agent', 'environment', 'simulation',\n        'robot', 'aerial', 'robots', 'autonomous', 'navigation', 'planning', 'path',\n        'sensor', 'camera', 'lidar', 'imu', 'encoder', 'decoder', 'feature', 'features',\n        'input', 'output', 'hidden', 'latent', 'representation', 'space', 'dimension',\n        'batch', 'epoch', 'iteration', 'step', 'rate', 'schedule', 'warmup', 'decay',\n        'accuracy', 'precision', 'recall', 'score', 'metric', 'error', 'loss',\n        'positional', 'relative', 'absolute', 'global', 'local', 'spatial', 'temporal',\n        'multi', 'single', 'dual', 'triple', 'cross', 'self', 'auto', 'semi', 'fully',\n        'based', 'driven', 'aware', 'guided', 'constrained', 'regularized', 'normalized',\n        'reinforcement', 'supervised', 'unsupervised', 'transfer', 'contrastive', 'federated',\n        'title', 'journal', 'proceedings', 'conference', 'workshop', 'arxiv', 'preprint',\n        'digital', 'substation', 'substations', 'virtualized', 'protection', 'virtual',\n        'predictive', 'adaptive', 'dynamic', 'static', 'hybrid', 'integrated', 'distributed',\n        'analysis', 'system', 'systems', 'framework', 'architecture', 'design', 'implementation',\n        'approach', 'technique', 'techniques', 'strategies', 'strategy', 'solution', 'solutions',\n        'data', 'dataset', 'datasets', 'benchmark', 'benchmarks', 'evaluation', 'evaluations',\n        'power', 'energy', 'electric', 'electrical', 'voltage', 'current', 'frequency',\n        'grid', 'smart', 'cyber', 'security', 'attack', 'defense', 'threat', 'vulnerability',\n        'communication', 'protocol', 'protocols', 'standard', 'standards', 'specification',\n        'real', 'time', 'online', 'offline', 'continuous', 'discrete', 'optimal', 'optimized',\n    }\n    \n    technical_patterns = [\n        r'.*\\s+(model|network|learning|algorithm|method|system|framework|architecture)$',\n        r'.*\\s+(encoding|decoding|processing|training|inference)$',\n        r'.*\\s+(function|layer|module|block|unit|cell)$',\n        r'.*\\s+(control|planning|navigation|avoidance|detection)$',\n        r'.*\\s+(distance|field|space|representation|embedding)$',\n        r'^(neural|deep|machine|reinforcement|supervised|unsupervised)\\s+.*',\n        r'^(linear|nonlinear|convolutional|recurrent|attention)\\s+.*',\n        r'^(signed|positional|relative|absolute|global|local)\\s+.*',\n        r'^(transfer|contrastive|federated|meta|few-shot|zero-shot)\\s+.*',\n        r'^title\\s+.*',\n    ]\n    \n    name_lower = name.lower()\n    name_normalized = re.sub(r'[^a-z\\s]', '', name_lower).strip()\n    \n    if name_normalized in KNOWN_METHODS:\n        return False\n    \n    for known_method in KNOWN_METHODS:\n        if name_normalized == known_method or known_method == name_normalized:\n            return False\n        if ' ' in known_method:\n            method_words = set(known_method.split())\n            name_words = set(name_normalized.split())\n            if method_words == name_words:\n                return False\n    \n    for pattern in technical_patterns:\n        if re.match(pattern, name_lower):\n            return False\n    \n    words = name_lower.split()\n    if any(word in excluded_words for word in words):\n        return False\n    \n    if len(name) < 3 or len(name) > 50:\n        return False\n    \n    if len(words) < 2:\n        return False\n    \n    if all(len(word) <= 2 for word in words):\n        return False\n    \n    return True\n\n\ndef normalize_author_name(name: str) -> str:\n    \"\"\"\n    Normalize author name format\n    \"\"\"\n    name = name.strip()\n    name = re.sub(r'\\s+', ' ', name)\n    \n    if ',' in name:\n        parts = name.split(',')\n        if len(parts) == 2:\n            name = f\"{parts[1].strip()} {parts[0].strip()}\"\n    \n    return name\n\n\ndef build_method_prerequisites() -> Dict[str, List[str]]:\n    \"\"\"\n    Build a mapping of methods to their prerequisites\n    \"\"\"\n    return {\n        'Transformer': ['Attention Mechanism', 'Neural Network'],\n        'Bert': ['Transformer', 'Pre-Training'],\n        'Gpt': ['Transformer', 'Language Model'],\n        'Gpt-2': ['Gpt', 'Transformer'],\n        'Gpt-3': ['Gpt-2', 'Transformer'],\n        'Gpt-4': ['Gpt-3', 'Transformer'],\n        'Attention Mechanism': ['Neural Network', 'Rnn'],\n        'Self-Attention': ['Attention Mechanism'],\n        'Lstm': ['Rnn', 'Neural Network'],\n        'Rnn': ['Neural Network', 'Backpropagation'],\n        'Cnn': ['Neural Network', 'Convolution'],\n        'Resnet': ['Cnn', 'Batch Normalization'],\n        'Gan': ['Neural Network', 'Generative Model'],\n        'Diffusion Model': ['Generative Model', 'Neural Network'],\n        'Stable Diffusion': ['Diffusion Model', 'Autoencoder'],\n        'Variational Autoencoder': ['Autoencoder', 'Bayesian'],\n        'Reinforcement Learning': ['Machine Learning', 'Markov Decision Process'],\n        'Transfer Learning': ['Deep Learning', 'Pre-Training'],\n        'Fine-Tuning': ['Transfer Learning', 'Pre-Training'],\n        'Rag': ['Transformer', 'Information Retrieval'],\n        'Chain Of Thought': ['Prompt Engineering', 'Language Model'],\n        'Rlhf': ['Reinforcement Learning', 'Language Model'],\n    }\n","path":null,"size_bytes":17537,"size_tokens":null},"utils/__init__.py":{"content":"\"\"\"\nScholarLens utility modules\n\"\"\"\n","path":null,"size_bytes":36,"size_tokens":null},"app.py":{"content":"\"\"\"\nScholarLens - AI-Powered Research Intelligence Platform\nMain Streamlit application\n\"\"\"\n\nimport streamlit as st\nimport os\nfrom datetime import datetime\n\nst.set_page_config(\n    page_title=\"ScholarLens\",\n    page_icon=\"\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\"\n)\n\nfrom models import init_db, get_session, Paper, Author, Method, Dataset, Institution, PaperChunk, Note, Flashcard, SavedQuery, ReadingList\nfrom utils.pdf_processor import extract_text_from_pdf, chunk_text, get_section_for_chunk\nfrom utils.ner_extractor import extract_entities, build_method_prerequisites\nfrom utils.openai_helper import (\n    is_available as openai_available,\n    generate_summary,\n    answer_question,\n    generate_flashcards,\n    generate_quiz,\n    generate_policy_brief,\n    generate_analogy,\n    extract_key_insights\n)\nfrom utils.semantic_search import PaperSearchIndex, find_similar_papers\nfrom utils.graph_builder import (\n    build_knowledge_graph,\n    build_method_dag,\n    create_plotly_graph,\n    create_method_dag_visualization,\n    build_coauthorship_network,\n    calculate_graph_metrics,\n    find_collaboration_opportunities\n)\nfrom utils.analytics import (\n    get_top_coauthorship_pairs,\n    get_trending_topics_over_time,\n    get_papers_per_institution,\n    get_research_growth_by_field,\n    get_top_authors_by_publication,\n    get_most_used_datasets,\n    get_collaboration_network_density,\n    get_emerging_methods,\n    get_dataset_method_cooccurrence,\n    get_yearly_publication_stats,\n    get_method_category_distribution,\n    get_summary_statistics\n)\nfrom utils.arxiv_pubmed import ArxivAPI, PubMedAPI, search_papers\nfrom utils.topic_modeling import cluster_papers, extract_topics, PaperClusterer\nfrom utils.trend_forecasting import (\n    analyze_method_trends,\n    identify_emerging_methods,\n    identify_declining_methods,\n    generate_forecast_summary,\n    create_timeline_data\n)\nfrom utils.export_utils import (\n    generate_markdown_review,\n    generate_latex_review,\n    generate_bibtex,\n    generate_plain_text_review,\n    generate_csv_export,\n    create_summary_statistics as create_export_stats\n)\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport pandas as pd\nimport networkx as nx\n\ninit_db()\n\nif 'search_index' not in st.session_state:\n    st.session_state.search_index = PaperSearchIndex()\nif 'current_paper_id' not in st.session_state:\n    st.session_state.current_paper_id = None\nif 'chat_history' not in st.session_state:\n    st.session_state.chat_history = []\n\n\ndef main():\n    st.sidebar.title(\"ScholarLens\")\n    st.sidebar.markdown(\"*AI-Powered Research Intelligence*\")\n    st.sidebar.markdown(\"---\")\n    \n    pages = {\n        \"Upload Papers\": page_upload,\n        \"Knowledge Graph\": page_knowledge_graph,\n        \"Research Q&A\": page_qa,\n        \"Multi-Audience Summaries\": page_summaries,\n        \"Analytics Dashboard\": page_analytics,\n        \"Learning Mode\": page_learning,\n        \"Research Workspace\": page_workspace\n    }\n    \n    selection = st.sidebar.radio(\"Navigate\", list(pages.keys()))\n    \n    st.sidebar.markdown(\"---\")\n    session = get_session()\n    stats = get_summary_statistics(session)\n    session.close()\n    \n    st.sidebar.markdown(\"### Quick Stats\")\n    st.sidebar.metric(\"Papers\", stats['total_papers'])\n    st.sidebar.metric(\"Authors\", stats['total_authors'])\n    st.sidebar.metric(\"Methods\", stats['total_methods'])\n    st.sidebar.metric(\"Datasets\", stats['total_datasets'])\n    \n    if not openai_available():\n        st.sidebar.warning(\"OpenAI API key not configured. Some AI features will be limited.\")\n    \n    pages[selection]()\n\n\ndef page_upload():\n    \"\"\"ScholarLens - Paper Upload\"\"\"\n    st.title(\"ScholarLens\")\n    st.markdown(\"*Making Research Intuitive, Interactive, and Insightful*\")\n    \n    col1, col2 = st.columns([2, 1])\n    \n    with col1:\n        st.subheader(\"Upload Research Papers\")\n        \n        uploaded_files = st.file_uploader(\n            \"Upload PDF files\",\n            type=['pdf'],\n            accept_multiple_files=True,\n            help=\"Upload one or more research papers in PDF format\"\n        )\n        \n        if uploaded_files:\n            if st.button(\"Process Papers\", type=\"primary\"):\n                progress_bar = st.progress(0)\n                status_text = st.empty()\n                \n                session = get_session()\n                \n                for i, uploaded_file in enumerate(uploaded_files):\n                    status_text.text(f\"Processing: {uploaded_file.name}\")\n                    \n                    result = extract_text_from_pdf(uploaded_file)\n                    \n                    if result['success']:\n                        entities = extract_entities(result['text'])\n                        \n                        extracted_year = result['metadata'].get('year')\n                        if not extracted_year:\n                            import re\n                            from datetime import datetime\n                            year_match = re.search(r'\\b(20[0-2][0-9])\\b', result['text'][:5000])\n                            if year_match:\n                                extracted_year = int(year_match.group(1))\n                            else:\n                                extracted_year = datetime.now().year\n                        \n                        paper = Paper(\n                            title=result['metadata'].get('title', uploaded_file.name.replace('.pdf', '')),\n                            abstract=result['sections'].get('abstract', ''),\n                            content=result['text'][:50000],\n                            source='pdf',\n                            year=extracted_year,\n                            doi=result['metadata'].get('doi'),\n                            pdf_path=uploaded_file.name,\n                            topics=[m['name'] for m in entities['methods'][:5]]\n                        )\n                        session.add(paper)\n                        session.flush()\n                        \n                        for author_data in entities['authors'][:10]:\n                            author = session.query(Author).filter_by(name=author_data['normalized_name']).first()\n                            if not author:\n                                author = Author(name=author_data['normalized_name'])\n                                session.add(author)\n                                session.flush()\n                            paper.authors.append(author)\n                        \n                        for method_data in entities['methods'][:20]:\n                            method = session.query(Method).filter_by(name=method_data['name']).first()\n                            if not method:\n                                method = Method(\n                                    name=method_data['name'],\n                                    category=method_data.get('category', 'unknown'),\n                                    usage_count=1\n                                )\n                                session.add(method)\n                            else:\n                                method.usage_count += 1\n                            session.flush()\n                            paper.methods.append(method)\n                        \n                        for dataset_data in entities['datasets'][:20]:\n                            dataset = session.query(Dataset).filter_by(name=dataset_data['name']).first()\n                            if not dataset:\n                                dataset = Dataset(\n                                    name=dataset_data['name'],\n                                    domain=dataset_data.get('domain', 'unknown'),\n                                    usage_count=1\n                                )\n                                session.add(dataset)\n                            else:\n                                dataset.usage_count += 1\n                            session.flush()\n                            paper.datasets.append(dataset)\n                        \n                        for inst_data in entities['institutions'][:10]:\n                            institution = session.query(Institution).filter_by(name=inst_data['name']).first()\n                            if not institution:\n                                institution = Institution(\n                                    name=inst_data['name'],\n                                    type=inst_data.get('type', 'unknown')\n                                )\n                                session.add(institution)\n                                session.flush()\n                            for author in paper.authors:\n                                if institution not in author.institutions:\n                                    author.institutions.append(institution)\n                        \n                        chunks = chunk_text(result['text'], chunk_size=1000, overlap=200)\n                        for chunk in chunks:\n                            paper_chunk = PaperChunk(\n                                paper_id=paper.id,\n                                chunk_index=chunk['index'],\n                                content=chunk['content'],\n                                section=get_section_for_chunk(chunk['content'], result['sections'])\n                            )\n                            session.add(paper_chunk)\n                        \n                        st.session_state.search_index.index_paper(\n                            paper.id,\n                            paper.title,\n                            paper.abstract,\n                            chunks,\n                            [m['name'] for m in entities['methods']]\n                        )\n                        \n                        session.commit()\n                        st.success(f\"Processed: {uploaded_file.name}\")\n                    else:\n                        st.error(f\"Failed to process {uploaded_file.name}: {result.get('error', 'Unknown error')}\")\n                    \n                    progress_bar.progress((i + 1) / len(uploaded_files))\n                \n                session.close()\n                status_text.text(\"Processing complete!\")\n                st.balloons()\n    \n    with col2:\n        st.subheader(\"Recent Papers\")\n        session = get_session()\n        recent_papers = session.query(Paper).order_by(Paper.created_at.desc()).limit(5).all()\n        \n        for paper in recent_papers:\n            with st.expander(paper.title[:50] + \"...\" if len(paper.title) > 50 else paper.title):\n                st.write(f\"**Year:** {paper.year or 'Unknown'}\")\n                st.write(f\"**Authors:** {', '.join([a.name for a in paper.authors[:3]])}\")\n                st.write(f\"**Methods:** {', '.join([m.name for m in paper.methods[:3]])}\")\n        \n        session.close()\n    \n    st.markdown(\"---\")\n    st.subheader(\"Manage Papers\")\n    st.markdown(\"*Delete papers from your library*\")\n    \n    session = get_session()\n    all_papers = session.query(Paper).order_by(Paper.created_at.desc()).all()\n    \n    if all_papers:\n        if 'papers_to_delete' not in st.session_state:\n            st.session_state.papers_to_delete = []\n        \n        paper_options = {f\"{p.title[:60]}... (ID: {p.id})\" if len(p.title) > 60 else f\"{p.title} (ID: {p.id})\": p.id for p in all_papers}\n        \n        selected_papers = st.multiselect(\n            \"Select papers to delete\",\n            options=list(paper_options.keys()),\n            key=\"delete_paper_select\"\n        )\n        \n        if selected_papers:\n            st.warning(f\"You have selected {len(selected_papers)} paper(s) for deletion.\")\n            \n            col_del1, col_del2 = st.columns(2)\n            \n            with col_del1:\n                if st.button(\"Delete Selected Papers\", type=\"primary\"):\n                    deleted_count = 0\n                    for paper_label in selected_papers:\n                        paper_id = paper_options[paper_label]\n                        paper_to_delete = session.query(Paper).filter_by(id=paper_id).first()\n                        if paper_to_delete:\n                            session.query(PaperChunk).filter_by(paper_id=paper_id).delete()\n                            session.query(Note).filter_by(paper_id=paper_id).delete()\n                            session.query(ReadingList).filter_by(paper_id=paper_id).delete()\n                            session.query(Flashcard).filter_by(paper_id=paper_id).delete()\n                            \n                            paper_to_delete.authors = []\n                            paper_to_delete.methods = []\n                            paper_to_delete.datasets = []\n                            session.flush()\n                            \n                            session.delete(paper_to_delete)\n                            deleted_count += 1\n                    \n                    session.commit()\n                    st.success(f\"Deleted {deleted_count} paper(s) successfully!\")\n                    st.rerun()\n            \n            with col_del2:\n                if st.button(\"Delete ALL Papers\", type=\"secondary\"):\n                    st.session_state.confirm_delete_all = True\n        \n        if st.session_state.get('confirm_delete_all', False):\n            st.error(\"Are you sure you want to delete ALL papers? This cannot be undone!\")\n            col_confirm1, col_confirm2 = st.columns(2)\n            with col_confirm1:\n                if st.button(\"Yes, Delete All\"):\n                    for paper in all_papers:\n                        session.query(PaperChunk).filter_by(paper_id=paper.id).delete()\n                        session.query(Note).filter_by(paper_id=paper.id).delete()\n                        session.query(ReadingList).filter_by(paper_id=paper.id).delete()\n                        session.query(Flashcard).filter_by(paper_id=paper.id).delete()\n                        paper.authors = []\n                        paper.methods = []\n                        paper.datasets = []\n                    session.flush()\n                    session.query(Paper).delete()\n                    session.commit()\n                    st.session_state.confirm_delete_all = False\n                    st.success(\"All papers deleted!\")\n                    st.rerun()\n            with col_confirm2:\n                if st.button(\"Cancel\"):\n                    st.session_state.confirm_delete_all = False\n                    st.rerun()\n    else:\n        st.info(\"No papers in your library yet.\")\n    \n    session.close()\n    \n    st.markdown(\"---\")\n    st.subheader(\"Semantic Search\")\n    \n    search_query = st.text_input(\"Search papers by concept, method, or keyword\")\n    \n    if search_query:\n        results = st.session_state.search_index.search_all(search_query, top_k=5)\n        \n        if results:\n            st.write(f\"Found {len(results)} relevant results:\")\n            for result in results:\n                with st.expander(f\"Score: {result['score']:.3f} | {result.get('source', 'content')}\"):\n                    st.write(result['content'][:500] + \"...\")\n        else:\n            session = get_session()\n            papers = session.query(Paper).all()\n            paper_dicts = [{'id': p.id, 'title': p.title, 'abstract': p.abstract, 'content': p.content} for p in papers]\n            session.close()\n            \n            if paper_dicts:\n                similar = find_similar_papers(search_query, paper_dicts, top_k=3)\n                if similar:\n                    st.write(\"Similar papers found:\")\n                    for paper in similar:\n                        st.write(f\"- {paper['title']} (Score: {paper.get('similarity_score', 0):.3f})\")\n    \n    st.markdown(\"---\")\n    st.subheader(\"Fetch from Open-Access Sources\")\n    st.markdown(\"*Search arXiv and PubMed for research papers*\")\n    \n    if 'api_search_results' not in st.session_state:\n        st.session_state.api_search_results = []\n    if 'imported_papers' not in st.session_state:\n        st.session_state.imported_papers = set()\n    \n    if st.session_state.imported_papers:\n        st.success(f\"Successfully imported {len(st.session_state.imported_papers)} paper(s) to your library!\")\n        if st.button(\"Clear Import Messages\"):\n            st.session_state.imported_papers = set()\n            st.rerun()\n    \n    col1, col2 = st.columns([3, 1])\n    \n    with col1:\n        api_query = st.text_input(\"Search arXiv/PubMed\", placeholder=\"e.g., transformer attention mechanism\")\n    \n    with col2:\n        source = st.selectbox(\"Source\", [\"arXiv\", \"PubMed\", \"Both\"])\n        max_results = st.slider(\"Max results\", 5, 20, 10)\n    \n    if api_query and st.button(\"Search External Sources\", type=\"secondary\"):\n        with st.spinner(\"Searching...\"):\n            sources = []\n            if source in [\"arXiv\", \"Both\"]:\n                sources.append(\"arxiv\")\n            if source in [\"PubMed\", \"Both\"]:\n                sources.append(\"pubmed\")\n            \n            results = search_papers(api_query, sources=sources, max_results=max_results)\n            st.session_state.api_search_results = results\n            st.session_state.imported_papers = set()\n    \n    if st.session_state.api_search_results:\n        results = st.session_state.api_search_results\n        st.info(f\"Found {len(results)} papers\")\n        \n        for i, paper in enumerate(results):\n            paper_id = paper.get('arxiv_id') or paper.get('pmid') or f\"paper_{i}\"\n            is_imported = paper_id in st.session_state.imported_papers\n            \n            title_display = paper.get('title', 'Untitled')[:80]\n            if is_imported:\n                title_display = f\" {title_display}\"\n            \n            with st.expander(f\"{title_display}...\"):\n                st.write(f\"**Source:** {paper.get('source', 'Unknown').upper()}\")\n                st.write(f\"**Year:** {paper.get('year', 'Unknown')}\")\n                \n                if paper.get('authors'):\n                    author_names = [a.get('name', '') for a in paper['authors'][:5]]\n                    st.write(f\"**Authors:** {', '.join(author_names)}\")\n                \n                if paper.get('abstract'):\n                    st.write(f\"**Abstract:** {paper['abstract'][:500]}...\")\n                \n                if paper.get('url'):\n                    st.markdown(f\"[View Paper]({paper['url']})\")\n                \n                if is_imported:\n                    st.success(\"Already imported to library\")\n                else:\n                    if st.button(\"Import to Library\", key=f\"import_{i}\"):\n                        try:\n                            session = get_session()\n                            \n                            new_paper = Paper(\n                                title=paper.get('title', 'Untitled'),\n                                abstract=paper.get('abstract', ''),\n                                source=paper.get('source', 'api'),\n                                source_id=paper.get('arxiv_id') or paper.get('pmid'),\n                                year=paper.get('year'),\n                                doi=paper.get('doi')\n                            )\n                            session.add(new_paper)\n                            session.flush()\n                            \n                            for author_data in paper.get('authors', [])[:10]:\n                                author_name = author_data.get('name', '')\n                                if author_name:\n                                    author = session.query(Author).filter_by(name=author_name).first()\n                                    if not author:\n                                        author = Author(name=author_name)\n                                        session.add(author)\n                                        session.flush()\n                                    new_paper.authors.append(author)\n                            \n                            if paper.get('abstract'):\n                                entities = extract_entities(paper['abstract'])\n                                for method_data in entities['methods'][:10]:\n                                    method = session.query(Method).filter_by(name=method_data['name']).first()\n                                    if not method:\n                                        method = Method(name=method_data['name'], category=method_data.get('category'))\n                                        session.add(method)\n                                        session.flush()\n                                    new_paper.methods.append(method)\n                            \n                            session.commit()\n                            session.close()\n                            st.session_state.imported_papers.add(paper_id)\n                            st.rerun()\n                        except Exception as e:\n                            session.rollback()\n                            session.close()\n                            st.error(f\"Failed to import: {str(e)}\")\n    \n    st.markdown(\"---\")\n    st.subheader(\"Topic Clustering\")\n    \n    session = get_session()\n    papers = session.query(Paper).all()\n    \n    if papers and len(papers) >= 3:\n        if st.button(\"Auto-Cluster Papers by Topic\"):\n            with st.spinner(\"Clustering papers...\"):\n                paper_dicts = [{'id': p.id, 'title': p.title, 'abstract': p.abstract or '', 'content': p.content or ''} for p in papers]\n                \n                clustering_result = cluster_papers(paper_dicts)\n                \n                if clustering_result['assignments']:\n                    st.success(f\"Clustered into {clustering_result['n_clusters']} topics (Quality: {clustering_result['quality_score']:.2f})\")\n                    \n                    for cluster_id, label in clustering_result['labels'].items():\n                        papers_in_cluster = [paper_dicts[i] for i, c in enumerate(clustering_result['assignments']) if c == cluster_id]\n                        \n                        with st.expander(f\"Topic: {label} ({len(papers_in_cluster)} papers)\"):\n                            keywords = clustering_result['keywords'].get(cluster_id, [])\n                            st.write(f\"**Keywords:** {', '.join(keywords)}\")\n                            for p in papers_in_cluster[:5]:\n                                st.write(f\"- {p['title'][:70]}...\")\n    elif papers:\n        st.info(\"Need at least 3 papers for topic clustering\")\n    \n    session.close()\n\n\ndef page_knowledge_graph():\n    \"\"\"Interactive Knowledge Graph Exploration\"\"\"\n    st.title(\"Knowledge Graph Explorer\")\n    st.markdown(\"Explore connections between papers, methods, datasets, and authors\")\n    \n    session = get_session()\n    \n    tab1, tab2, tab3, tab4 = st.tabs([\"Full Knowledge Graph\", \"Single Paper Graph\", \"Concept Dependency Map\", \"Co-authorship Network\"])\n    \n    with tab1:\n        st.subheader(\"Research Knowledge Graph\")\n        \n        with st.expander(\" How to Read This Graph\", expanded=False):\n            st.markdown(\"\"\"\n            **What does this graph show?**\n            \n            This interactive graph visualizes the relationships in your research corpus:\n            \n            - **Green nodes = Papers** - Each paper you've uploaded\n            - **Blue nodes = Methods** - Techniques, algorithms, and approaches used in papers\n            - **Orange nodes = Datasets** - Data sources referenced in papers  \n            - **Purple nodes = Authors** - Researchers who wrote the papers\n            \n            **How to interpret:**\n            - **Lines (edges)** connect related items (e.g., a paper to its methods)\n            - **Larger nodes** have more connections (more influential)\n            - **Hover** over any node to see details\n            - **Zoom & pan** to explore different areas\n            \n            **Note:** Only connected nodes are shown. Isolated items without relationships are hidden for clarity.\n            \"\"\")\n        \n        papers = session.query(Paper).all()\n        methods = session.query(Method).all()\n        datasets = session.query(Dataset).all()\n        authors = session.query(Author).all()\n        \n        if papers:\n            col_opts1, col_opts2 = st.columns([1, 2])\n            with col_opts1:\n                show_labels = st.checkbox(\"Show labels on nodes\", value=False, key=\"kg_show_labels\")\n            \n            paper_dicts = [{'id': p.id, 'title': p.title, 'year': p.year, \n                          'methods': [{'id': m.id, 'name': m.name} for m in p.methods],\n                          'datasets': [{'id': d.id, 'name': d.name} for d in p.datasets],\n                          'authors': [{'id': a.id, 'name': a.name} for a in p.authors]} \n                         for p in papers]\n            method_dicts = [{'id': m.id, 'name': m.name, 'category': m.category, 'usage_count': m.usage_count} for m in methods]\n            dataset_dicts = [{'id': d.id, 'name': d.name, 'domain': d.domain, 'usage_count': d.usage_count} for d in datasets]\n            author_dicts = [{'id': a.id, 'name': a.name, 'papers': [{'id': p.id} for p in a.papers]} for a in authors]\n            \n            G = build_knowledge_graph(paper_dicts, method_dicts, dataset_dicts, author_dicts)\n            \n            connected_count = sum(1 for n in G.nodes() if G.degree(n) > 0)\n            total_count = G.number_of_nodes()\n            \n            col1, col2 = st.columns([3, 1])\n            \n            with col1:\n                fig = create_plotly_graph(G, \"Research Knowledge Graph\", show_labels=show_labels)\n                st.plotly_chart(fig, use_container_width=True)\n            \n            with col2:\n                st.markdown(\"### Graph Statistics\")\n                metrics = calculate_graph_metrics(G)\n                st.metric(\"Connected Nodes\", connected_count, help=\"Nodes with at least one connection\")\n                st.metric(\"Edges\", metrics['num_edges'], help=\"Total connections between nodes\")\n                st.metric(\"Density\", f\"{metrics['density']:.4f}\", help=\"How interconnected the graph is (0-1)\")\n                \n                if total_count > connected_count:\n                    st.caption(f\"*{total_count - connected_count} isolated nodes hidden*\")\n                \n                st.markdown(\"---\")\n                st.markdown(\"### Quick Insights\")\n                \n                paper_count = sum(1 for n in G.nodes() if G.nodes[n].get('type') == 'paper' and G.degree(n) > 0)\n                method_count = sum(1 for n in G.nodes() if G.nodes[n].get('type') == 'method' and G.degree(n) > 0)\n                dataset_count = sum(1 for n in G.nodes() if G.nodes[n].get('type') == 'dataset' and G.degree(n) > 0)\n                author_count = sum(1 for n in G.nodes() if G.nodes[n].get('type') == 'author' and G.degree(n) > 0)\n                \n                st.write(f\" **{paper_count}** papers\")\n                st.write(f\" **{method_count}** methods\")\n                st.write(f\" **{dataset_count}** datasets\")\n                st.write(f\" **{author_count}** authors\")\n        else:\n            st.info(\"Upload papers to build the knowledge graph\")\n    \n    with tab2:\n        st.subheader(\"Single Paper Knowledge Graph\")\n        st.markdown(\"*View the knowledge graph for a specific paper*\")\n        \n        papers = session.query(Paper).all()\n        \n        if papers:\n            paper_options = {f\"{p.title[:60]}...\" if len(p.title) > 60 else p.title: p.id for p in papers}\n            selected_paper_title = st.selectbox(\n                \"Select a paper to visualize\",\n                options=list(paper_options.keys()),\n                key=\"single_paper_graph_select\"\n            )\n            \n            if selected_paper_title:\n                selected_paper_id = paper_options[selected_paper_title]\n                selected_paper = session.query(Paper).filter_by(id=selected_paper_id).first()\n                \n                if selected_paper:\n                    paper_dict = {\n                        'id': selected_paper.id,\n                        'title': selected_paper.title,\n                        'year': selected_paper.year,\n                        'methods': [{'id': m.id, 'name': m.name} for m in selected_paper.methods],\n                        'datasets': [{'id': d.id, 'name': d.name} for d in selected_paper.datasets],\n                        'authors': [{'id': a.id, 'name': a.name} for a in selected_paper.authors]\n                    }\n                    \n                    method_dicts = [{'id': m.id, 'name': m.name, 'category': m.category, 'usage_count': 1} \n                                   for m in selected_paper.methods]\n                    dataset_dicts = [{'id': d.id, 'name': d.name, 'domain': d.domain, 'usage_count': 1} \n                                    for d in selected_paper.datasets]\n                    author_dicts = [{'id': a.id, 'name': a.name, 'papers': [{'id': selected_paper.id}]} \n                                   for a in selected_paper.authors]\n                    \n                    G = build_knowledge_graph([paper_dict], method_dicts, dataset_dicts, author_dicts)\n                    \n                    show_labels_single = st.checkbox(\"Show labels on nodes\", value=True, key=\"single_paper_show_labels\")\n                    \n                    col1, col2 = st.columns([3, 1])\n                    \n                    with col1:\n                        fig = create_plotly_graph(G, f\"Knowledge Graph: {selected_paper.title[:40]}...\", show_labels=show_labels_single)\n                        st.plotly_chart(fig, use_container_width=True)\n                    \n                    with col2:\n                        st.markdown(\"### Paper Details\")\n                        st.write(f\"**Title:** {selected_paper.title}\")\n                        st.write(f\"**Year:** {selected_paper.year or 'Unknown'}\")\n                        \n                        st.markdown(\"### Connections\")\n                        st.metric(\"Methods\", len(selected_paper.methods))\n                        st.metric(\"Datasets\", len(selected_paper.datasets))\n                        st.metric(\"Authors\", len(selected_paper.authors))\n                        \n                        st.markdown(\"### Legend\")\n                        st.markdown(\"- :green[] Paper\")\n                        st.markdown(\"- :blue[] Methods\")\n                        st.markdown(\"- :orange[] Datasets\")\n                        st.markdown(\"- :violet[] Authors\")\n                    \n                    st.markdown(\"---\")\n                    \n                    col_m, col_d, col_a = st.columns(3)\n                    \n                    with col_m:\n                        st.markdown(\"**Methods Used:**\")\n                        if selected_paper.methods:\n                            for m in selected_paper.methods:\n                                st.write(f\"- `{m.name}`\")\n                        else:\n                            st.write(\"*No methods extracted*\")\n                    \n                    with col_d:\n                        st.markdown(\"**Datasets Used:**\")\n                        if selected_paper.datasets:\n                            for d in selected_paper.datasets:\n                                st.write(f\"- `{d.name}`\")\n                        else:\n                            st.write(\"*No datasets extracted*\")\n                    \n                    with col_a:\n                        st.markdown(\"**Authors:**\")\n                        if selected_paper.authors:\n                            for a in selected_paper.authors[:10]:\n                                st.write(f\"- {a.name}\")\n                            if len(selected_paper.authors) > 10:\n                                st.write(f\"*...and {len(selected_paper.authors) - 10} more*\")\n                        else:\n                            st.write(\"*No authors extracted*\")\n        else:\n            st.info(\"Upload papers to view individual knowledge graphs\")\n    \n    with tab3:\n        st.subheader(\"Concept Dependency Map\")\n        st.markdown(\"*Understanding prerequisite relationships between methods*\")\n        \n        methods = session.query(Method).all()\n        \n        if methods:\n            prerequisites = build_method_prerequisites()\n            method_dicts = [{'name': m.name, 'category': m.category, 'usage_count': m.usage_count} for m in methods]\n            \n            G = build_method_dag(method_dicts, prerequisites)\n            \n            if G.number_of_nodes() > 0:\n                fig = create_method_dag_visualization(G, \"Method Prerequisites\")\n                st.plotly_chart(fig, use_container_width=True)\n                \n                st.markdown(\"### Learning Pathway\")\n                st.info(\"\"\"\n                **Example learning sequence:**\n                1. Neural Network  RNN  LSTM  Attention Mechanism  Transformer\n                2. Transformer  BERT, GPT  GPT-2  GPT-3  GPT-4\n                3. Generative Models  GAN  Diffusion Model  Stable Diffusion\n                \"\"\")\n            else:\n                st.info(\"Add more papers to build the concept dependency map\")\n        else:\n            st.info(\"Upload papers to see concept dependencies\")\n    \n    with tab4:\n        st.subheader(\"Co-authorship Network\")\n        \n        authors = session.query(Author).all()\n        papers = session.query(Paper).all()\n        \n        if authors and len(authors) > 1:\n            paper_dicts = [{'id': p.id, 'authors': [{'id': a.id, 'name': a.name} for a in p.authors]} for p in papers]\n            author_dicts = [{'id': a.id, 'name': a.name, 'papers': [{'id': p.id} for p in a.papers]} for a in authors]\n            \n            G = build_coauthorship_network(author_dicts, paper_dicts)\n            \n            if G.number_of_edges() > 0:\n                pos = nx.spring_layout(G, k=2, iterations=50)\n                \n                edge_x, edge_y = [], []\n                for edge in G.edges():\n                    x0, y0 = pos[edge[0]]\n                    x1, y1 = pos[edge[1]]\n                    edge_x.extend([x0, x1, None])\n                    edge_y.extend([y0, y1, None])\n                \n                edge_trace = go.Scatter(x=edge_x, y=edge_y, mode='lines',\n                                       line=dict(width=0.5, color='#888'), hoverinfo='none')\n                \n                node_x, node_y, node_text = [], [], []\n                for node in G.nodes():\n                    x, y = pos[node]\n                    node_x.append(x)\n                    node_y.append(y)\n                    node_text.append(G.nodes[node].get('name', str(node)))\n                \n                node_trace = go.Scatter(\n                    x=node_x, y=node_y, mode='markers+text',\n                    marker=dict(size=15, color='#9C27B0'),\n                    text=node_text, textposition='top center',\n                    textfont=dict(size=8)\n                )\n                \n                fig = go.Figure(data=[edge_trace, node_trace])\n                fig.update_layout(\n                    title=\"Co-authorship Network\",\n                    showlegend=False, hovermode='closest',\n                    margin=dict(b=20, l=5, r=5, t=40),\n                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                    height=500\n                )\n                st.plotly_chart(fig, use_container_width=True)\n                \n                opportunities = find_collaboration_opportunities(G)\n                if opportunities:\n                    st.markdown(\"### Potential Collaborations\")\n                    for opp in opportunities[:5]:\n                        st.write(f\"- {opp['node1_name']} & {opp['node2_name']} ({opp['common_neighbors']} common collaborators)\")\n            else:\n                st.info(\"Not enough co-authored papers to build the network\")\n        else:\n            st.info(\"Upload papers with multiple authors to see the collaboration network\")\n    \n    session.close()\n\n\ndef page_qa():\n    \"\"\"Evidence-Backed Q&A with RAG\"\"\"\n    st.title(\"Research Q&A\")\n    st.markdown(\"Ask questions and get evidence-backed answers with source citations\")\n    \n    if not openai_available():\n        st.warning(\"OpenAI API key not configured. Please add your API key to use this feature.\")\n        st.info(\"Add OPENAI_API_KEY to your environment variables to enable AI-powered Q&A.\")\n        return\n    \n    session = get_session()\n    \n    paper_options = {p.title[:80]: p.id for p in session.query(Paper).all()}\n    \n    if paper_options:\n        selected_paper = st.selectbox(\"Select a paper (or ask across all papers)\", \n                                      [\"All Papers\"] + list(paper_options.keys()))\n        \n        question = st.text_area(\"Ask a research question:\", \n                               placeholder=\"What methods were used for data augmentation?\")\n        \n        if st.button(\"Get Answer\", type=\"primary\") and question:\n            with st.spinner(\"Searching for relevant context and generating answer...\"):\n                if selected_paper == \"All Papers\":\n                    chunks = session.query(PaperChunk).all()\n                else:\n                    paper_id = paper_options[selected_paper]\n                    chunks = session.query(PaperChunk).filter_by(paper_id=paper_id).all()\n                \n                if chunks:\n                    search_results = st.session_state.search_index.search_chunks(question, top_k=5)\n                    \n                    if not search_results:\n                        search_results = [{'content': c.content, 'paper_id': c.paper_id, \n                                         'section': c.section} for c in chunks[:5]]\n                    \n                    for result in search_results:\n                        if 'paper_id' in result:\n                            paper = session.query(Paper).get(result['paper_id'])\n                            if paper:\n                                result['paper_title'] = paper.title\n                    \n                    response = answer_question(question, search_results)\n                    \n                    st.markdown(\"### Answer\")\n                    st.write(response['answer'])\n                    \n                    if response['sources']:\n                        st.markdown(\"### Sources\")\n                        for source in response['sources']:\n                            with st.expander(f\"Source {source['index']}: {source['paper_title'][:50]}...\"):\n                                st.write(f\"**Section:** {source['section']}\")\n                                st.write(source['content'])\n                    \n                    saved_query = SavedQuery(\n                        query=question,\n                        response=response['answer'],\n                        sources=[{'title': s['paper_title'], 'section': s['section']} for s in response['sources']]\n                    )\n                    session.add(saved_query)\n                    session.commit()\n                else:\n                    st.warning(\"No paper content available. Please upload papers first.\")\n        \n        st.markdown(\"---\")\n        st.subheader(\"Previous Questions\")\n        \n        saved_queries = session.query(SavedQuery).order_by(SavedQuery.created_at.desc()).limit(5).all()\n        for q in saved_queries:\n            with st.expander(q.query[:100] + \"...\" if len(q.query) > 100 else q.query):\n                st.write(q.response)\n    else:\n        st.info(\"Upload papers to start asking questions\")\n    \n    session.close()\n\n\ndef page_summaries():\n    \"\"\"Multi-Audience Summaries\"\"\"\n    st.title(\"Multi-Audience Summaries\")\n    st.markdown(\"Generate tailored summaries for different audiences\")\n    \n    if not openai_available():\n        st.warning(\"OpenAI API key not configured. Please add your API key to use this feature.\")\n        return\n    \n    session = get_session()\n    papers = session.query(Paper).all()\n    \n    if papers:\n        paper_options = {p.title[:80]: p.id for p in papers}\n        selected_paper_title = st.selectbox(\"Select a paper\", list(paper_options.keys()))\n        \n        paper_id = paper_options[selected_paper_title]\n        paper = session.query(Paper).get(paper_id)\n        \n        st.markdown(\"---\")\n        \n        col1, col2, col3 = st.columns(3)\n        \n        with col1:\n            st.markdown(\"### Expert Summary\")\n            st.caption(\"Technical details for researchers\")\n            if st.button(\"Generate Expert Summary\", key=\"expert\"):\n                with st.spinner(\"Generating expert summary...\"):\n                    summary = generate_summary(paper.content or paper.abstract, \"expert\")\n                    st.write(summary)\n        \n        with col2:\n            st.markdown(\"### Student Summary\")\n            st.caption(\"Simple explanations with analogies\")\n            if st.button(\"Generate Student Summary\", key=\"student\"):\n                with st.spinner(\"Generating student summary...\"):\n                    summary = generate_summary(paper.content or paper.abstract, \"student\")\n                    st.write(summary)\n        \n        with col3:\n            st.markdown(\"### Policymaker Summary\")\n            st.caption(\"Applications, risks, and implications\")\n            if st.button(\"Generate Policymaker Summary\", key=\"policymaker\"):\n                with st.spinner(\"Generating policymaker summary...\"):\n                    summary = generate_summary(paper.content or paper.abstract, \"policymaker\")\n                    st.write(summary)\n        \n        st.markdown(\"---\")\n        \n        st.subheader(\"Policy Brief Generator\")\n        \n        if st.button(\"Generate Full Policy Brief\", type=\"primary\"):\n            with st.spinner(\"Generating comprehensive policy brief...\"):\n                brief = generate_policy_brief(paper.content or paper.abstract, paper.title)\n                st.markdown(brief)\n                \n                st.download_button(\n                    \"Download Policy Brief\",\n                    brief,\n                    file_name=f\"policy_brief_{paper.title[:30].replace(' ', '_')}.md\",\n                    mime=\"text/markdown\"\n                )\n        \n        st.markdown(\"---\")\n        \n        st.subheader(\"Cross-Domain Analogies\")\n        \n        if paper.methods:\n            method_names = [m.name for m in paper.methods]\n            selected_method = st.selectbox(\"Select a concept to explain\", method_names)\n            \n            if st.button(\"Generate Analogy\"):\n                with st.spinner(\"Creating intuitive analogy...\"):\n                    analogy = generate_analogy(selected_method, paper.abstract or \"\")\n                    st.info(analogy)\n    else:\n        st.info(\"Upload papers to generate summaries\")\n    \n    session.close()\n\n\ndef page_analytics():\n    \"\"\"Analytics Dashboard with 8+ SQL Reports\"\"\"\n    st.title(\"Analytics Dashboard\")\n    st.markdown(\"Research intelligence with advanced SQL analytics\")\n    \n    session = get_session()\n    \n    stats = get_summary_statistics(session)\n    \n    col1, col2, col3, col4, col5 = st.columns(5)\n    col1.metric(\"Total Papers\", stats['total_papers'])\n    col2.metric(\"Authors\", stats['total_authors'])\n    col3.metric(\"Methods\", stats['total_methods'])\n    col4.metric(\"Datasets\", stats['total_datasets'])\n    col5.metric(\"Institutions\", stats['total_institutions'])\n    \n    st.markdown(\"---\")\n    \n    tab1, tab2, tab3, tab4 = st.tabs([\"Trends & Growth\", \"Authors & Collaboration\", \"Methods & Datasets\", \"Advanced Reports\"])\n    \n    with tab1:\n        st.subheader(\"Research Trends Over Time\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            yearly_stats = get_yearly_publication_stats(session)\n            if yearly_stats:\n                df = pd.DataFrame(yearly_stats)\n                df['year'] = df['year'].astype(int)\n                fig = px.bar(df, x='year', y='count', title=\"Papers by Year\")\n                fig.update_xaxes(tickmode='linear', tick0=df['year'].min(), dtick=1, tickformat='d')\n                fig.update_yaxes(tickmode='linear', tick0=0, dtick=1)\n                st.plotly_chart(fig, use_container_width=True)\n            else:\n                st.info(\"Upload papers to see publication trends\")\n        \n        with col2:\n            growth_data = get_research_growth_by_field(session)\n            if growth_data:\n                df = pd.DataFrame(growth_data)\n                df['year'] = df['year'].astype(int)\n                fig = px.line(df, x='year', y='count', color='category', \n                            title=\"Research Growth by Field\", markers=True)\n                fig.update_xaxes(tickmode='linear', tick0=df['year'].min(), dtick=1, tickformat='d')\n                fig.update_yaxes(tickmode='linear', tick0=0, dtick=max(1, df['count'].max() // 5))\n                st.plotly_chart(fig, use_container_width=True)\n            else:\n                st.info(\"Upload papers to see growth trends\")\n        \n        trending = get_trending_topics_over_time(session)\n        if trending:\n            st.subheader(\"Trending Topics Over Time\")\n            df = pd.DataFrame(trending[:20])\n            st.dataframe(df, use_container_width=True)\n        \n        st.markdown(\"---\")\n        st.subheader(\"Trend Forecasting\")\n        \n        if trending:\n            trends = analyze_method_trends(trending)\n            \n            if trends:\n                summary = generate_forecast_summary(trends)\n                st.markdown(summary)\n                \n                col1, col2 = st.columns(2)\n                \n                with col1:\n                    st.markdown(\"### Emerging Methods\")\n                    emerging_methods = identify_emerging_methods(trends)\n                    if emerging_methods:\n                        for m in emerging_methods[:5]:\n                            st.success(f\"**{m['method']}** - {m['recent_growth']:.1f}% growth\")\n                    else:\n                        st.info(\"No strongly emerging methods detected\")\n                \n                with col2:\n                    st.markdown(\"### Declining Methods\")\n                    declining_methods = identify_declining_methods(trends)\n                    if declining_methods:\n                        for m in declining_methods[:5]:\n                            st.warning(f\"**{m['method']}** - peaked in {m['peak_year']}\")\n                    else:\n                        st.info(\"No strongly declining methods detected\")\n                \n                st.markdown(\"### Method Trajectory Predictions\")\n                method_names = list(trends.keys())[:5]\n                if method_names:\n                    timeline_data = create_timeline_data(trends, method_names)\n                    \n                    chart_data = []\n                    for method, series in timeline_data['series'].items():\n                        for year, count in series.get('historical', {}).items():\n                            chart_data.append({'Method': method, 'Year': year, 'Count': count, 'Type': 'Historical'})\n                        for year, count in series.get('predicted', {}).items():\n                            chart_data.append({'Method': method, 'Year': year, 'Count': count, 'Type': 'Predicted'})\n                    \n                    if chart_data:\n                        chart_df = pd.DataFrame(chart_data)\n                        fig = px.line(chart_df, x='Year', y='Count', color='Method', \n                                    line_dash='Type', title=\"Method Popularity Forecast\")\n                        st.plotly_chart(fig, use_container_width=True)\n        else:\n            st.info(\"Upload papers with year data to see trend forecasting\")\n    \n    with tab2:\n        st.subheader(\"Author Analytics\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            top_authors = get_top_authors_by_publication(session)\n            if top_authors:\n                st.markdown(\"### Top Authors by Publication Count\")\n                df = pd.DataFrame(top_authors)\n                fig = px.bar(df.head(10), x='name', y='paper_count', \n                           title=\"Top 10 Authors\")\n                st.plotly_chart(fig, use_container_width=True)\n        \n        with col2:\n            coauthor_pairs = get_top_coauthorship_pairs(session)\n            if coauthor_pairs:\n                st.markdown(\"### Top Co-authorship Pairs\")\n                df = pd.DataFrame(coauthor_pairs)\n                st.dataframe(df, use_container_width=True)\n        \n        network_stats = get_collaboration_network_density(session)\n        st.markdown(\"### Collaboration Network Statistics\")\n        col1, col2, col3, col4 = st.columns(4)\n        col1.metric(\"Total Authors\", network_stats['total_authors'])\n        col2.metric(\"Unique Collaborations\", network_stats['unique_collaborations'])\n        col3.metric(\"Network Density\", f\"{network_stats['network_density']:.4f}\")\n        col4.metric(\"Avg Collaborators\", network_stats['avg_collaborators_per_author'])\n    \n    with tab3:\n        st.subheader(\"Methods & Datasets Analysis\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            method_dist = get_method_category_distribution(session)\n            if method_dist:\n                st.markdown(\"### Method Distribution by Category\")\n                df = pd.DataFrame(method_dist)\n                fig = px.pie(df, values='method_count', names='category',\n                           title=\"Methods by Category\")\n                st.plotly_chart(fig, use_container_width=True)\n        \n        with col2:\n            top_datasets = get_most_used_datasets(session)\n            if top_datasets:\n                st.markdown(\"### Most Used Datasets\")\n                df = pd.DataFrame(top_datasets)\n                fig = px.bar(df.head(10), x='name', y='usage_count',\n                           title=\"Top 10 Datasets\")\n                st.plotly_chart(fig, use_container_width=True)\n        \n        cooccurrence = get_dataset_method_cooccurrence(session)\n        if cooccurrence:\n            st.markdown(\"### Dataset-Method Co-occurrence\")\n            df = pd.DataFrame(cooccurrence)\n            \n            pivot = df.pivot_table(index='dataset', columns='method', values='count', fill_value=0)\n            if not pivot.empty and pivot.shape[0] > 0 and pivot.shape[1] > 0:\n                fig = px.imshow(pivot.head(10).T.head(10), \n                              title=\"Dataset-Method Heatmap\",\n                              labels=dict(x=\"Dataset\", y=\"Method\", color=\"Co-occurrence\"))\n                st.plotly_chart(fig, use_container_width=True)\n    \n    with tab4:\n        st.subheader(\"Advanced SQL Reports\")\n        \n        st.markdown(\"### Report 1: Papers per Institution\")\n        inst_data = get_papers_per_institution(session)\n        if inst_data:\n            df = pd.DataFrame(inst_data)\n            st.dataframe(df, use_container_width=True)\n        else:\n            st.info(\"No institution data available\")\n        \n        st.markdown(\"### Report 2: Emerging Methods\")\n        emerging = get_emerging_methods(session)\n        if emerging:\n            df = pd.DataFrame(emerging)\n            st.dataframe(df, use_container_width=True)\n        else:\n            st.info(\"No emerging methods detected yet\")\n        \n    \n    session.close()\n\n\ndef page_learning():\n    \"\"\"Learning Mode with Flashcards and Quizzes\"\"\"\n    st.title(\"Learning Mode\")\n    st.markdown(\"Interactive learning tools: concept maps, flashcards, and quizzes\")\n    \n    if not openai_available():\n        st.warning(\"OpenAI API key not configured. Please add your API key to use this feature.\")\n        return\n    \n    session = get_session()\n    papers = session.query(Paper).all()\n    \n    if papers:\n        paper_options = {p.title[:80]: p.id for p in papers}\n        selected_paper_title = st.selectbox(\"Select a paper to study\", list(paper_options.keys()))\n        \n        paper_id = paper_options[selected_paper_title]\n        paper = session.query(Paper).get(paper_id)\n        \n        tab1, tab2, tab3, tab4 = st.tabs([\"Key Insights\", \"Flashcards\", \"Quiz\", \"Study Roadmap\"])\n        \n        with tab1:\n            st.subheader(\"Key Insights\")\n            \n            if st.button(\"Extract Key Insights\", type=\"primary\"):\n                with st.spinner(\"Analyzing paper for key insights...\"):\n                    insights = extract_key_insights(paper.content or paper.abstract)\n                    \n                    for i, insight in enumerate(insights, 1):\n                        st.success(f\"**{i}.** {insight}\")\n        \n        with tab2:\n            st.subheader(\"Flashcards\")\n            \n            existing_flashcards = session.query(Flashcard).filter_by(paper_id=paper_id).all()\n            \n            if existing_flashcards:\n                st.write(f\"You have {len(existing_flashcards)} flashcards for this paper.\")\n                \n                if 'flashcard_index' not in st.session_state:\n                    st.session_state.flashcard_index = 0\n                \n                if st.session_state.flashcard_index < len(existing_flashcards):\n                    card = existing_flashcards[st.session_state.flashcard_index]\n                    \n                    st.markdown(f\"**Card {st.session_state.flashcard_index + 1} of {len(existing_flashcards)}**\")\n                    \n                    with st.container():\n                        st.markdown(f\"### Question\")\n                        st.write(card.question)\n                        \n                        if st.button(\"Show Answer\"):\n                            st.markdown(\"### Answer\")\n                            st.write(card.answer)\n                    \n                    col1, col2, col3 = st.columns(3)\n                    with col1:\n                        if st.button(\"Previous\") and st.session_state.flashcard_index > 0:\n                            st.session_state.flashcard_index -= 1\n                            st.rerun()\n                    with col2:\n                        st.write(f\"{st.session_state.flashcard_index + 1} / {len(existing_flashcards)}\")\n                    with col3:\n                        if st.button(\"Next\") and st.session_state.flashcard_index < len(existing_flashcards) - 1:\n                            st.session_state.flashcard_index += 1\n                            st.rerun()\n            \n            num_cards = st.slider(\"Number of new flashcards to generate\", 3, 10, 5)\n            \n            if st.button(\"Generate New Flashcards\"):\n                with st.spinner(\"Generating flashcards...\"):\n                    flashcards = generate_flashcards(paper.content or paper.abstract, num_cards)\n                    \n                    for card_data in flashcards:\n                        flashcard = Flashcard(\n                            paper_id=paper_id,\n                            question=card_data.get('question', ''),\n                            answer=card_data.get('answer', ''),\n                            difficulty=card_data.get('difficulty', 'medium')\n                        )\n                        session.add(flashcard)\n                    \n                    session.commit()\n                    st.success(f\"Generated {len(flashcards)} flashcards!\")\n                    st.rerun()\n        \n        with tab3:\n            st.subheader(\"Quiz\")\n            \n            num_questions = st.slider(\"Number of quiz questions\", 3, 10, 5)\n            \n            if st.button(\"Start Quiz\"):\n                with st.spinner(\"Generating quiz questions...\"):\n                    questions = generate_quiz(paper.content or paper.abstract, num_questions)\n                    \n                    if questions:\n                        st.session_state.quiz_questions = questions\n                        st.session_state.quiz_answers = {}\n                        st.session_state.quiz_submitted = False\n            \n            if 'quiz_questions' in st.session_state and st.session_state.quiz_questions:\n                questions = st.session_state.quiz_questions\n                \n                with st.form(\"quiz_form\"):\n                    for i, q in enumerate(questions):\n                        st.markdown(f\"**Q{i+1}: {q['question']}**\")\n                        \n                        answer = st.radio(\n                            f\"Select answer for Q{i+1}:\",\n                            q['options'],\n                            key=f\"q_{i}\"\n                        )\n                        st.session_state.quiz_answers[i] = answer\n                        st.markdown(\"---\")\n                    \n                    submitted = st.form_submit_button(\"Submit Quiz\")\n                    \n                    if submitted:\n                        correct = 0\n                        for i, q in enumerate(questions):\n                            user_answer = st.session_state.quiz_answers.get(i, \"\")\n                            if user_answer and user_answer.startswith(q['correct_answer']):\n                                correct += 1\n                        \n                        score = (correct / len(questions)) * 100\n                        st.session_state.quiz_score = score\n                        st.session_state.quiz_submitted = True\n                \n                if st.session_state.get('quiz_submitted'):\n                    score = st.session_state.quiz_score\n                    if score >= 80:\n                        st.success(f\"Great job! You scored {score:.0f}%\")\n                    elif score >= 60:\n                        st.warning(f\"Good effort! You scored {score:.0f}%\")\n                    else:\n                        st.error(f\"Keep studying! You scored {score:.0f}%\")\n        \n        with tab4:\n            st.subheader(\"Personalized Study Roadmap\")\n            \n            st.markdown(\"\"\"\n            Based on the methods used in this paper, here's a suggested learning path:\n            \"\"\")\n            \n            if paper.methods:\n                prerequisites = build_method_prerequisites()\n                \n                for method in paper.methods[:5]:\n                    prereqs = prerequisites.get(method.name, [])\n                    if prereqs:\n                        st.markdown(f\"**To understand {method.name}:**\")\n                        for i, prereq in enumerate(prereqs, 1):\n                            st.markdown(f\"  {i}. First learn: {prereq}\")\n                        st.markdown(\"\")\n                    else:\n                        st.markdown(f\"**{method.name}** - Foundational concept\")\n            else:\n                st.info(\"Methods will appear here after paper processing\")\n    else:\n        st.info(\"Upload papers to start learning\")\n    \n    session.close()\n\n\ndef page_workspace():\n    \"\"\"Personal Research Workspace\"\"\"\n    st.title(\"Research Workspace\")\n    st.markdown(\"Organize your research: saved papers, notes, and reading lists\")\n    \n    session = get_session()\n    \n    tab1, tab2, tab3 = st.tabs([\"Reading List\", \"Notes\", \"Export\"])\n    \n    with tab1:\n        st.subheader(\"Reading List\")\n        \n        papers = session.query(Paper).all()\n        reading_list = session.query(ReadingList).all()\n        reading_paper_ids = {r.paper_id for r in reading_list}\n        \n        col1, col2 = st.columns([2, 1])\n        \n        with col1:\n            st.markdown(\"### Add Papers to Reading List\")\n            \n            available_papers = [p for p in papers if p.id not in reading_paper_ids]\n            \n            if available_papers:\n                paper_to_add = st.selectbox(\n                    \"Select a paper to add\",\n                    options=[(p.id, p.title[:70]) for p in available_papers],\n                    format_func=lambda x: x[1]\n                )\n                \n                priority = st.slider(\"Priority (1=highest)\", 1, 5, 3)\n                \n                if st.button(\"Add to Reading List\"):\n                    new_item = ReadingList(paper_id=paper_to_add[0], priority=priority)\n                    session.add(new_item)\n                    session.commit()\n                    st.success(\"Added to reading list!\")\n                    st.rerun()\n        \n        with col2:\n            st.markdown(\"### Your Reading List\")\n            \n            reading_items = session.query(ReadingList).order_by(ReadingList.priority).all()\n            \n            for item in reading_items:\n                paper = session.query(Paper).get(item.paper_id)\n                if paper:\n                    status_emoji = {\"unread\": \"\", \"reading\": \"\", \"completed\": \"\"}\n                    \n                    with st.expander(f\"{status_emoji.get(item.status, '')} {paper.title[:40]}...\"):\n                        st.write(f\"**Priority:** {item.priority}\")\n                        st.write(f\"**Status:** {item.status}\")\n                        \n                        new_status = st.selectbox(\n                            \"Update status\",\n                            [\"unread\", \"reading\", \"completed\"],\n                            index=[\"unread\", \"reading\", \"completed\"].index(item.status),\n                            key=f\"status_{item.id}\"\n                        )\n                        \n                        if new_status != item.status:\n                            item.status = new_status\n                            session.commit()\n                            st.rerun()\n                        \n                        if st.button(\"Remove\", key=f\"remove_{item.id}\"):\n                            session.delete(item)\n                            session.commit()\n                            st.rerun()\n    \n    with tab2:\n        st.subheader(\"Research Notes\")\n        \n        if papers:\n            paper_options = {p.title[:70]: p.id for p in papers}\n            selected_paper = st.selectbox(\"Select paper for notes\", list(paper_options.keys()))\n            paper_id = paper_options[selected_paper]\n            \n            existing_notes = session.query(Note).filter_by(paper_id=paper_id).all()\n            \n            st.markdown(\"### Your Notes\")\n            for note in existing_notes:\n                with st.expander(f\"Note from {note.created_at.strftime('%Y-%m-%d %H:%M')}\"):\n                    st.write(note.content)\n                    if st.button(\"Delete\", key=f\"del_note_{note.id}\"):\n                        session.delete(note)\n                        session.commit()\n                        st.rerun()\n            \n            st.markdown(\"### Add New Note\")\n            new_note = st.text_area(\"Write your note\")\n            \n            if st.button(\"Save Note\") and new_note:\n                note = Note(paper_id=paper_id, content=new_note)\n                session.add(note)\n                session.commit()\n                st.success(\"Note saved!\")\n                st.rerun()\n        else:\n            st.info(\"Upload papers to add notes\")\n    \n    with tab3:\n        st.subheader(\"Export Literature Review\")\n        \n        papers = session.query(Paper).all()\n        \n        if papers:\n            st.markdown(\"### Select Papers for Export\")\n            \n            selected_papers = st.multiselect(\n                \"Select papers to include\",\n                options=[(p.id, p.title[:70]) for p in papers],\n                format_func=lambda x: x[1]\n            )\n            \n            export_format = st.selectbox(\n                \"Export Format\",\n                [\"Markdown\", \"LaTeX\", \"BibTeX\", \"Plain Text\", \"CSV\"]\n            )\n            \n            include_notes = st.checkbox(\"Include notes\", value=True)\n            \n            if st.button(\"Generate Export\", type=\"primary\") and selected_papers:\n                paper_dicts = []\n                notes_dict = {}\n                \n                for paper_id, _ in selected_papers:\n                    paper = session.query(Paper).get(paper_id)\n                    if paper:\n                        paper_dict = {\n                            'id': paper.id,\n                            'title': paper.title,\n                            'abstract': paper.abstract,\n                            'year': paper.year,\n                            'doi': paper.doi,\n                            'venue': paper.venue,\n                            'source': paper.source,\n                            'authors': [{'name': a.name} for a in paper.authors],\n                            'methods': [{'name': m.name} for m in paper.methods],\n                            'datasets': [{'name': d.name} for d in paper.datasets]\n                        }\n                        paper_dicts.append(paper_dict)\n                        \n                        if include_notes:\n                            paper_notes = session.query(Note).filter_by(paper_id=paper_id).all()\n                            notes_dict[paper_id] = [n.content for n in paper_notes]\n                \n                if export_format == \"Markdown\":\n                    content = generate_markdown_review(paper_dicts, notes_dict)\n                    st.markdown(content)\n                    st.download_button(\n                        \"Download Markdown\",\n                        content,\n                        file_name=\"literature_review.md\",\n                        mime=\"text/markdown\"\n                    )\n                \n                elif export_format == \"LaTeX\":\n                    content = generate_latex_review(paper_dicts, notes_dict)\n                    st.code(content, language=\"latex\")\n                    st.download_button(\n                        \"Download LaTeX\",\n                        content,\n                        file_name=\"literature_review.tex\",\n                        mime=\"text/x-tex\"\n                    )\n                \n                elif export_format == \"BibTeX\":\n                    content = generate_bibtex(paper_dicts)\n                    st.code(content, language=\"bibtex\")\n                    st.download_button(\n                        \"Download BibTeX\",\n                        content,\n                        file_name=\"references.bib\",\n                        mime=\"text/x-bibtex\"\n                    )\n                \n                elif export_format == \"Plain Text\":\n                    content = generate_plain_text_review(paper_dicts, notes_dict)\n                    st.text(content)\n                    st.download_button(\n                        \"Download Text\",\n                        content,\n                        file_name=\"literature_review.txt\",\n                        mime=\"text/plain\"\n                    )\n                \n                elif export_format == \"CSV\":\n                    content = generate_csv_export(paper_dicts)\n                    st.dataframe(pd.read_csv(pd.io.common.StringIO(content)))\n                    st.download_button(\n                        \"Download CSV\",\n                        content,\n                        file_name=\"papers_export.csv\",\n                        mime=\"text/csv\"\n                    )\n            \n            st.markdown(\"---\")\n            st.subheader(\"Export Statistics\")\n            \n            if selected_papers:\n                paper_dicts = []\n                for paper_id, _ in selected_papers:\n                    paper = session.query(Paper).get(paper_id)\n                    if paper:\n                        paper_dicts.append({\n                            'id': paper.id,\n                            'title': paper.title,\n                            'year': paper.year,\n                            'authors': [{'name': a.name} for a in paper.authors],\n                            'methods': [{'name': m.name} for m in paper.methods],\n                            'datasets': [{'name': d.name} for d in paper.datasets],\n                            'source': paper.source\n                        })\n                \n                if paper_dicts:\n                    stats = create_export_stats(paper_dicts)\n                    \n                    col1, col2 = st.columns(2)\n                    with col1:\n                        st.metric(\"Selected Papers\", stats['total_papers'])\n                        if stats['year_range']:\n                            st.write(f\"**Year Range:** {stats['year_range']}\")\n                        if stats['top_authors']:\n                            st.write(f\"**Top Authors:** {', '.join(stats['top_authors'][:5])}\")\n                    \n                    with col2:\n                        if stats['top_methods']:\n                            st.write(f\"**Top Methods:** {', '.join(stats['top_methods'][:5])}\")\n                        if stats['top_datasets']:\n                            st.write(f\"**Top Datasets:** {', '.join(stats['top_datasets'][:5])}\")\n        else:\n            st.info(\"Upload papers to export literature reviews\")\n    \n    session.close()\n\n\nif __name__ == \"__main__\":\n    main()\n","path":null,"size_bytes":69248,"size_tokens":null},"utils/export_utils.py":{"content":"\"\"\"\nExport utilities for ScholarLens\nGenerates literature reviews in multiple formats: Markdown, PDF, Word, LaTeX\n\"\"\"\n\nfrom typing import List, Dict, Optional\nfrom datetime import datetime\nimport io\n\n\ndef generate_markdown_review(papers: List[Dict], notes: Dict[int, List[str]] = None,\n                            title: str = \"Literature Review\") -> str:\n    \"\"\"\n    Generate a literature review in Markdown format\n    \"\"\"\n    md = f\"# {title}\\n\\n\"\n    md += f\"*Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M')}*\\n\\n\"\n    md += f\"*Total papers reviewed: {len(papers)}*\\n\\n\"\n    md += \"---\\n\\n\"\n    \n    if notes is None:\n        notes = {}\n    \n    for paper in papers:\n        md += f\"## {paper.get('title', 'Untitled')}\\n\\n\"\n        \n        if paper.get('year'):\n            md += f\"**Year:** {paper['year']}\\n\\n\"\n        \n        if paper.get('authors'):\n            if isinstance(paper['authors'], list):\n                if isinstance(paper['authors'][0], dict):\n                    author_names = [a.get('name', '') for a in paper['authors']]\n                else:\n                    author_names = paper['authors']\n                md += f\"**Authors:** {', '.join(author_names)}\\n\\n\"\n        \n        if paper.get('doi'):\n            md += f\"**DOI:** {paper['doi']}\\n\\n\"\n        \n        if paper.get('venue') or paper.get('journal'):\n            venue = paper.get('venue') or paper.get('journal')\n            md += f\"**Published in:** {venue}\\n\\n\"\n        \n        if paper.get('abstract'):\n            md += \"### Abstract\\n\\n\"\n            md += f\"{paper['abstract']}\\n\\n\"\n        \n        if paper.get('methods'):\n            if isinstance(paper['methods'], list):\n                if isinstance(paper['methods'][0], dict):\n                    method_names = [m.get('name', '') for m in paper['methods']]\n                else:\n                    method_names = paper['methods']\n                md += f\"**Methods:** {', '.join(method_names)}\\n\\n\"\n        \n        if paper.get('datasets'):\n            if isinstance(paper['datasets'], list):\n                if isinstance(paper['datasets'][0], dict):\n                    dataset_names = [d.get('name', '') for d in paper['datasets']]\n                else:\n                    dataset_names = paper['datasets']\n                md += f\"**Datasets:** {', '.join(dataset_names)}\\n\\n\"\n        \n        paper_id = paper.get('id')\n        if paper_id and paper_id in notes and notes[paper_id]:\n            md += \"### Notes\\n\\n\"\n            for note in notes[paper_id]:\n                md += f\"- {note}\\n\"\n            md += \"\\n\"\n        \n        md += \"---\\n\\n\"\n    \n    return md\n\n\ndef generate_latex_review(papers: List[Dict], notes: Dict[int, List[str]] = None,\n                         title: str = \"Literature Review\") -> str:\n    \"\"\"\n    Generate a literature review in LaTeX format\n    \"\"\"\n    latex = r\"\"\"\\documentclass[11pt,a4paper]{article}\n\\usepackage[utf8]{inputenc}\n\\usepackage{hyperref}\n\\usepackage{natbib}\n\\usepackage{geometry}\n\\geometry{margin=1in}\n\n\\title{\"\"\" + _escape_latex(title) + r\"\"\"}\n\\author{ScholarLens}\n\\date{\"\"\" + datetime.now().strftime('%B %d, %Y') + r\"\"\"}\n\n\\begin{document}\n\\maketitle\n\n\\section*{Overview}\nThis literature review covers \"\"\" + str(len(papers)) + r\"\"\" papers.\n\n\"\"\"\n    \n    if notes is None:\n        notes = {}\n    \n    for i, paper in enumerate(papers, 1):\n        title_text = _escape_latex(paper.get('title', 'Untitled'))\n        latex += f\"\\\\subsection*{{{i}. {title_text}}}\\n\\n\"\n        \n        if paper.get('year'):\n            latex += f\"\\\\textbf{{Year:}} {paper['year']}\\n\\n\"\n        \n        if paper.get('authors'):\n            if isinstance(paper['authors'], list):\n                if isinstance(paper['authors'][0], dict):\n                    author_names = [a.get('name', '') for a in paper['authors']]\n                else:\n                    author_names = paper['authors']\n                authors_text = _escape_latex(', '.join(author_names))\n                latex += f\"\\\\textbf{{Authors:}} {authors_text}\\n\\n\"\n        \n        if paper.get('doi'):\n            doi = _escape_latex(paper['doi'])\n            latex += f\"\\\\textbf{{DOI:}} \\\\href{{https://doi.org/{doi}}}{{{doi}}}\\n\\n\"\n        \n        if paper.get('abstract'):\n            abstract = _escape_latex(paper['abstract'])\n            latex += f\"\\\\textbf{{Abstract:}} {abstract}\\n\\n\"\n        \n        if paper.get('methods'):\n            if isinstance(paper['methods'], list):\n                if isinstance(paper['methods'][0], dict):\n                    method_names = [m.get('name', '') for m in paper['methods']]\n                else:\n                    method_names = paper['methods']\n                methods_text = _escape_latex(', '.join(method_names))\n                latex += f\"\\\\textbf{{Methods:}} {methods_text}\\n\\n\"\n        \n        paper_id = paper.get('id')\n        if paper_id and paper_id in notes and notes[paper_id]:\n            latex += \"\\\\textbf{Notes:}\\n\\\\begin{itemize}\\n\"\n            for note in notes[paper_id]:\n                note_text = _escape_latex(note)\n                latex += f\"  \\\\item {note_text}\\n\"\n            latex += \"\\\\end{itemize}\\n\\n\"\n        \n        latex += \"\\\\vspace{1em}\\n\\\\hrule\\n\\\\vspace{1em}\\n\\n\"\n    \n    latex += r\"\"\"\n\\end{document}\n\"\"\"\n    \n    return latex\n\n\ndef _escape_latex(text: str) -> str:\n    \"\"\"\n    Escape special LaTeX characters\n    \"\"\"\n    if not text:\n        return \"\"\n    \n    replacements = [\n        ('\\\\', r'\\textbackslash{}'),\n        ('&', r'\\&'),\n        ('%', r'\\%'),\n        ('$', r'\\$'),\n        ('#', r'\\#'),\n        ('_', r'\\_'),\n        ('{', r'\\{'),\n        ('}', r'\\}'),\n        ('~', r'\\textasciitilde{}'),\n        ('^', r'\\textasciicircum{}'),\n    ]\n    \n    for old, new in replacements:\n        text = text.replace(old, new)\n    \n    return text\n\n\ndef generate_bibtex(papers: List[Dict]) -> str:\n    \"\"\"\n    Generate BibTeX entries for papers\n    \"\"\"\n    bibtex = \"\"\n    \n    for i, paper in enumerate(papers):\n        title = paper.get('title', 'Untitled')\n        year = paper.get('year', 'n.d.')\n        \n        if paper.get('authors'):\n            if isinstance(paper['authors'], list):\n                if isinstance(paper['authors'][0], dict):\n                    first_author = paper['authors'][0].get('name', 'Unknown')\n                else:\n                    first_author = paper['authors'][0]\n            else:\n                first_author = 'Unknown'\n        else:\n            first_author = 'Unknown'\n        \n        key = f\"{first_author.split()[-1].lower()}{year}_{i}\"\n        key = ''.join(c if c.isalnum() or c == '_' else '' for c in key)\n        \n        bibtex += f\"@article{{{key},\\n\"\n        bibtex += f\"  title = {{{title}}},\\n\"\n        \n        if paper.get('authors'):\n            if isinstance(paper['authors'], list):\n                if isinstance(paper['authors'][0], dict):\n                    author_names = [a.get('name', '') for a in paper['authors']]\n                else:\n                    author_names = paper['authors']\n                bibtex += f\"  author = {{{' and '.join(author_names)}}},\\n\"\n        \n        bibtex += f\"  year = {{{year}}},\\n\"\n        \n        if paper.get('doi'):\n            bibtex += f\"  doi = {{{paper['doi']}}},\\n\"\n        \n        if paper.get('venue') or paper.get('journal'):\n            venue = paper.get('venue') or paper.get('journal')\n            bibtex += f\"  journal = {{{venue}}},\\n\"\n        \n        if paper.get('abstract'):\n            abstract = paper['abstract'].replace('{', '').replace('}', '')\n            bibtex += f\"  abstract = {{{abstract[:500]}}},\\n\"\n        \n        bibtex += \"}\\n\\n\"\n    \n    return bibtex\n\n\ndef generate_plain_text_review(papers: List[Dict], notes: Dict[int, List[str]] = None,\n                              title: str = \"Literature Review\") -> str:\n    \"\"\"\n    Generate a simple plain text literature review\n    \"\"\"\n    text = f\"{title}\\n\"\n    text += \"=\" * len(title) + \"\\n\\n\"\n    text += f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\"\n    text += f\"Papers reviewed: {len(papers)}\\n\\n\"\n    text += \"-\" * 50 + \"\\n\\n\"\n    \n    if notes is None:\n        notes = {}\n    \n    for i, paper in enumerate(papers, 1):\n        text += f\"{i}. {paper.get('title', 'Untitled')}\\n\\n\"\n        \n        if paper.get('year'):\n            text += f\"   Year: {paper['year']}\\n\"\n        \n        if paper.get('authors'):\n            if isinstance(paper['authors'], list):\n                if isinstance(paper['authors'][0], dict):\n                    author_names = [a.get('name', '') for a in paper['authors']]\n                else:\n                    author_names = paper['authors']\n                text += f\"   Authors: {', '.join(author_names)}\\n\"\n        \n        if paper.get('abstract'):\n            text += f\"\\n   Abstract:\\n   {paper['abstract'][:500]}...\\n\"\n        \n        paper_id = paper.get('id')\n        if paper_id and paper_id in notes and notes[paper_id]:\n            text += \"\\n   Notes:\\n\"\n            for note in notes[paper_id]:\n                text += f\"   - {note}\\n\"\n        \n        text += \"\\n\" + \"-\" * 50 + \"\\n\\n\"\n    \n    return text\n\n\ndef generate_csv_export(papers: List[Dict]) -> str:\n    \"\"\"\n    Generate CSV export of papers\n    \"\"\"\n    import csv\n    import io\n    \n    output = io.StringIO()\n    writer = csv.writer(output)\n    \n    writer.writerow(['Title', 'Year', 'Authors', 'Abstract', 'DOI', 'Methods', 'Datasets', 'Source'])\n    \n    for paper in papers:\n        authors = ''\n        if paper.get('authors'):\n            if isinstance(paper['authors'], list):\n                if isinstance(paper['authors'][0], dict):\n                    authors = '; '.join([a.get('name', '') for a in paper['authors']])\n                else:\n                    authors = '; '.join(paper['authors'])\n        \n        methods = ''\n        if paper.get('methods'):\n            if isinstance(paper['methods'], list):\n                if isinstance(paper['methods'][0], dict):\n                    methods = '; '.join([m.get('name', '') for m in paper['methods']])\n                else:\n                    methods = '; '.join(paper['methods'])\n        \n        datasets = ''\n        if paper.get('datasets'):\n            if isinstance(paper['datasets'], list):\n                if isinstance(paper['datasets'][0], dict):\n                    datasets = '; '.join([d.get('name', '') for d in paper['datasets']])\n                else:\n                    datasets = '; '.join(paper['datasets'])\n        \n        writer.writerow([\n            paper.get('title', ''),\n            paper.get('year', ''),\n            authors,\n            paper.get('abstract', '')[:500],\n            paper.get('doi', ''),\n            methods,\n            datasets,\n            paper.get('source', '')\n        ])\n    \n    return output.getvalue()\n\n\ndef create_summary_statistics(papers: List[Dict]) -> Dict:\n    \"\"\"\n    Create summary statistics for export\n    \"\"\"\n    from collections import Counter\n    \n    stats = {\n        'total_papers': len(papers),\n        'year_range': None,\n        'top_authors': [],\n        'top_methods': [],\n        'top_datasets': [],\n        'sources': {}\n    }\n    \n    years = [p.get('year') for p in papers if p.get('year')]\n    if years:\n        stats['year_range'] = f\"{min(years)} - {max(years)}\"\n    \n    all_authors = []\n    for paper in papers:\n        if paper.get('authors'):\n            if isinstance(paper['authors'], list):\n                if isinstance(paper['authors'][0], dict):\n                    all_authors.extend([a.get('name', '') for a in paper['authors']])\n                else:\n                    all_authors.extend(paper['authors'])\n    stats['top_authors'] = [a[0] for a in Counter(all_authors).most_common(10)]\n    \n    all_methods = []\n    for paper in papers:\n        if paper.get('methods'):\n            if isinstance(paper['methods'], list):\n                if isinstance(paper['methods'][0], dict):\n                    all_methods.extend([m.get('name', '') for m in paper['methods']])\n                else:\n                    all_methods.extend(paper['methods'])\n    stats['top_methods'] = [m[0] for m in Counter(all_methods).most_common(10)]\n    \n    all_datasets = []\n    for paper in papers:\n        if paper.get('datasets'):\n            if isinstance(paper['datasets'], list):\n                if isinstance(paper['datasets'][0], dict):\n                    all_datasets.extend([d.get('name', '') for d in paper['datasets']])\n                else:\n                    all_datasets.extend(paper['datasets'])\n    stats['top_datasets'] = [d[0] for d in Counter(all_datasets).most_common(10)]\n    \n    sources = [p.get('source', 'unknown') for p in papers]\n    stats['sources'] = dict(Counter(sources))\n    \n    return stats\n","path":null,"size_bytes":12759,"size_tokens":null},"utils/analytics.py":{"content":"\"\"\"\nAnalytics and SQL reporting utilities for ScholarLens\nProvides 8+ analytical reports for research data analysis\n\"\"\"\n\nfrom sqlalchemy import text, func\nfrom sqlalchemy.orm import Session\nfrom typing import List, Dict, Tuple\nimport pandas as pd\nfrom datetime import datetime\n\n\ndef get_top_coauthorship_pairs(session: Session, limit: int = 10) -> List[Dict]:\n    \"\"\"\n    Report 1: Top co-authorship pairs by publication count\n    \"\"\"\n    query = text(\"\"\"\n        SELECT \n            a1.name as author1,\n            a2.name as author2,\n            COUNT(DISTINCT pa1.paper_id) as collaboration_count\n        FROM paper_authors pa1\n        JOIN paper_authors pa2 ON pa1.paper_id = pa2.paper_id AND pa1.author_id < pa2.author_id\n        JOIN authors a1 ON pa1.author_id = a1.id\n        JOIN authors a2 ON pa2.author_id = a2.id\n        GROUP BY a1.id, a2.id, a1.name, a2.name\n        ORDER BY collaboration_count DESC\n        LIMIT :limit\n    \"\"\")\n    \n    result = session.execute(query, {\"limit\": limit})\n    return [{\"author1\": row[0], \"author2\": row[1], \"count\": row[2]} for row in result]\n\n\ndef get_trending_topics_over_time(session: Session) -> List[Dict]:\n    \"\"\"\n    Report 2: Most cited or trending research topics over time\n    \"\"\"\n    try:\n        query = text(\"\"\"\n            SELECT \n                m.name as method,\n                COALESCE(m.category, 'unknown') as category,\n                p.year,\n                COUNT(*) as paper_count\n            FROM methods m\n            JOIN paper_methods pm ON m.id = pm.method_id\n            JOIN papers p ON pm.paper_id = p.id\n            WHERE p.year IS NOT NULL\n            GROUP BY m.id, m.name, m.category, p.year\n            ORDER BY p.year DESC, paper_count DESC\n        \"\"\")\n        \n        result = session.execute(query)\n        return [{\"method\": row[0], \"category\": row[1], \"year\": row[2], \"count\": row[3]} for row in result]\n    except Exception:\n        return []\n\n\ndef get_papers_per_institution(session: Session, limit: int = 20) -> List[Dict]:\n    \"\"\"\n    Report 3: Paper count per institution\n    \"\"\"\n    query = text(\"\"\"\n        SELECT \n            i.name as institution,\n            i.country,\n            i.type,\n            COUNT(DISTINCT pa.paper_id) as paper_count\n        FROM institutions i\n        JOIN author_institutions ai ON i.id = ai.institution_id\n        JOIN authors a ON ai.author_id = a.id\n        JOIN paper_authors pa ON a.id = pa.author_id\n        GROUP BY i.id, i.name, i.country, i.type\n        ORDER BY paper_count DESC\n        LIMIT :limit\n    \"\"\")\n    \n    result = session.execute(query, {\"limit\": limit})\n    return [{\"institution\": row[0], \"country\": row[1], \"type\": row[2], \"count\": row[3]} for row in result]\n\n\ndef get_research_growth_by_field(session: Session) -> List[Dict]:\n    \"\"\"\n    Report 4: Growth rate of research in specific AI subfields\n    \"\"\"\n    try:\n        query = text(\"\"\"\n            SELECT \n                COALESCE(m.category, 'unknown') as category,\n                p.year,\n                COUNT(*) as paper_count\n            FROM methods m\n            JOIN paper_methods pm ON m.id = pm.method_id\n            JOIN papers p ON pm.paper_id = p.id\n            WHERE p.year IS NOT NULL\n            GROUP BY m.category, p.year\n            ORDER BY m.category, p.year\n        \"\"\")\n        \n        result = session.execute(query)\n        rows = [{\"category\": row[0], \"year\": row[1], \"count\": row[2]} for row in result]\n        \n        for i, row in enumerate(rows):\n            if i > 0 and rows[i-1]['category'] == row['category']:\n                row['growth'] = row['count'] - rows[i-1]['count']\n            else:\n                row['growth'] = 0\n        \n        return rows\n    except Exception:\n        return []\n\n\ndef get_top_authors_by_publication(session: Session, limit: int = 20) -> List[Dict]:\n    \"\"\"\n    Report 5: Top authors by publication frequency\n    \"\"\"\n    query = text(\"\"\"\n        SELECT \n            a.name,\n            COUNT(DISTINCT pa.paper_id) as paper_count,\n            a.h_index,\n            a.total_citations\n        FROM authors a\n        JOIN paper_authors pa ON a.id = pa.author_id\n        GROUP BY a.id, a.name, a.h_index, a.total_citations\n        ORDER BY paper_count DESC\n        LIMIT :limit\n    \"\"\")\n    \n    result = session.execute(query, {\"limit\": limit})\n    return [{\"name\": row[0], \"paper_count\": row[1], \"h_index\": row[2], \"citations\": row[3]} for row in result]\n\n\ndef get_most_used_datasets(session: Session, limit: int = 20) -> List[Dict]:\n    \"\"\"\n    Report 6: Datasets most frequently reused across papers\n    \"\"\"\n    query = text(\"\"\"\n        SELECT \n            d.name,\n            d.domain,\n            COUNT(DISTINCT pd.paper_id) as usage_count,\n            d.description\n        FROM datasets d\n        JOIN paper_datasets pd ON d.id = pd.dataset_id\n        GROUP BY d.id, d.name, d.domain, d.description\n        ORDER BY usage_count DESC\n        LIMIT :limit\n    \"\"\")\n    \n    result = session.execute(query, {\"limit\": limit})\n    return [{\"name\": row[0], \"domain\": row[1], \"usage_count\": row[2], \"description\": row[3]} for row in result]\n\n\ndef get_collaboration_network_density(session: Session) -> Dict:\n    \"\"\"\n    Report 7: Collaboration network density metrics\n    \"\"\"\n    author_count_query = text(\"SELECT COUNT(*) FROM authors\")\n    author_count = session.execute(author_count_query).scalar() or 0\n    \n    collab_query = text(\"\"\"\n        SELECT COUNT(DISTINCT \n            CASE WHEN pa1.author_id < pa2.author_id \n            THEN CONCAT(pa1.author_id, '-', pa2.author_id)\n            ELSE CONCAT(pa2.author_id, '-', pa1.author_id)\n            END\n        ) as collaboration_pairs\n        FROM paper_authors pa1\n        JOIN paper_authors pa2 ON pa1.paper_id = pa2.paper_id AND pa1.author_id != pa2.author_id\n    \"\"\")\n    collab_count = session.execute(collab_query).scalar() or 0\n    \n    max_possible = (author_count * (author_count - 1)) / 2 if author_count > 1 else 1\n    density = collab_count / max_possible if max_possible > 0 else 0\n    \n    avg_collabs_query = text(\"\"\"\n        SELECT AVG(collab_count) FROM (\n            SELECT a.id, COUNT(DISTINCT pa2.author_id) as collab_count\n            FROM authors a\n            JOIN paper_authors pa1 ON a.id = pa1.author_id\n            JOIN paper_authors pa2 ON pa1.paper_id = pa2.paper_id AND pa1.author_id != pa2.author_id\n            GROUP BY a.id\n        ) subq\n    \"\"\")\n    avg_collaborators = session.execute(avg_collabs_query).scalar() or 0\n    \n    return {\n        \"total_authors\": author_count,\n        \"unique_collaborations\": collab_count,\n        \"network_density\": round(density, 4),\n        \"avg_collaborators_per_author\": round(float(avg_collaborators), 2) if avg_collaborators else 0\n    }\n\n\ndef get_emerging_methods(session: Session, recent_years: int = 2) -> List[Dict]:\n    \"\"\"\n    Report 8: Recent methods emerging in multiple domains\n    \"\"\"\n    current_year = datetime.now().year\n    \n    query = text(\"\"\"\n        SELECT \n            m.name,\n            m.category,\n            COUNT(DISTINCT pm.paper_id) as paper_count,\n            COUNT(DISTINCT m.category) as domain_count,\n            MIN(p.year) as first_appeared\n        FROM methods m\n        JOIN paper_methods pm ON m.id = pm.method_id\n        JOIN papers p ON pm.paper_id = p.id\n        WHERE p.year >= :min_year\n        GROUP BY m.id, m.name, m.category\n        HAVING COUNT(DISTINCT pm.paper_id) >= 2\n        ORDER BY paper_count DESC, first_appeared DESC\n        LIMIT 20\n    \"\"\")\n    \n    result = session.execute(query, {\"min_year\": current_year - recent_years})\n    return [{\"name\": row[0], \"category\": row[1], \"paper_count\": row[2], \n             \"domain_count\": row[3], \"first_appeared\": row[4]} for row in result]\n\n\ndef get_dataset_method_cooccurrence(session: Session, limit: int = 20) -> List[Dict]:\n    \"\"\"\n    Additional Report: Dataset-Method co-occurrence patterns\n    \"\"\"\n    query = text(\"\"\"\n        SELECT \n            d.name as dataset,\n            m.name as method,\n            COUNT(DISTINCT p.id) as co_occurrence_count\n        FROM datasets d\n        JOIN paper_datasets pd ON d.id = pd.dataset_id\n        JOIN papers p ON pd.paper_id = p.id\n        JOIN paper_methods pm ON p.id = pm.paper_id\n        JOIN methods m ON pm.method_id = m.id\n        GROUP BY d.id, d.name, m.id, m.name\n        ORDER BY co_occurrence_count DESC\n        LIMIT :limit\n    \"\"\")\n    \n    result = session.execute(query, {\"limit\": limit})\n    return [{\"dataset\": row[0], \"method\": row[1], \"count\": row[2]} for row in result]\n\n\ndef get_yearly_publication_stats(session: Session) -> List[Dict]:\n    \"\"\"\n    Get yearly publication statistics\n    \"\"\"\n    query = text(\"\"\"\n        SELECT \n            year,\n            COUNT(*) as paper_count\n        FROM papers\n        WHERE year IS NOT NULL\n        GROUP BY year\n        ORDER BY year DESC\n    \"\"\")\n    \n    result = session.execute(query)\n    return [{\"year\": row[0], \"count\": row[1]} for row in result]\n\n\ndef get_method_category_distribution(session: Session) -> List[Dict]:\n    \"\"\"\n    Get distribution of methods by category\n    \"\"\"\n    query = text(\"\"\"\n        SELECT \n            COALESCE(category, 'Other') as category,\n            COUNT(*) as method_count,\n            SUM(usage_count) as total_usage\n        FROM methods\n        GROUP BY category\n        ORDER BY method_count DESC\n    \"\"\")\n    \n    result = session.execute(query)\n    return [{\"category\": row[0], \"method_count\": row[1], \"total_usage\": row[2] or 0} for row in result]\n\n\ndef get_summary_statistics(session: Session) -> Dict:\n    \"\"\"\n    Get overall summary statistics\n    \"\"\"\n    paper_count = session.execute(text(\"SELECT COUNT(*) FROM papers\")).scalar() or 0\n    author_count = session.execute(text(\"SELECT COUNT(*) FROM authors\")).scalar() or 0\n    method_count = session.execute(text(\"SELECT COUNT(*) FROM methods\")).scalar() or 0\n    dataset_count = session.execute(text(\"SELECT COUNT(*) FROM datasets\")).scalar() or 0\n    institution_count = session.execute(text(\"SELECT COUNT(*) FROM institutions\")).scalar() or 0\n    \n    return {\n        \"total_papers\": paper_count,\n        \"total_authors\": author_count,\n        \"total_methods\": method_count,\n        \"total_datasets\": dataset_count,\n        \"total_institutions\": institution_count\n    }\n","path":null,"size_bytes":10296,"size_tokens":null},"utils/topic_modeling.py":{"content":"\"\"\"\nTopic modeling and clustering for ScholarLens\nAuto-clusters papers by topic using TF-IDF and clustering algorithms\n\"\"\"\n\nimport numpy as np\nfrom typing import List, Dict, Tuple, Optional\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nfrom sklearn.metrics import silhouette_score\nfrom collections import Counter\nimport re\n\n\nclass TopicModeler:\n    \"\"\"\n    Topic modeling using Latent Dirichlet Allocation (LDA)\n    \"\"\"\n    \n    def __init__(self, n_topics: int = 10, max_features: int = 5000):\n        self.n_topics = n_topics\n        self.vectorizer = TfidfVectorizer(\n            max_features=max_features,\n            stop_words='english',\n            max_df=0.95,\n            min_df=2,\n            ngram_range=(1, 2)\n        )\n        self.lda = LatentDirichletAllocation(\n            n_components=n_topics,\n            random_state=42,\n            learning_method='online',\n            max_iter=20\n        )\n        self.is_fitted = False\n        self.feature_names = None\n    \n    def fit(self, documents: List[str]):\n        \"\"\"\n        Fit the topic model on documents\n        \"\"\"\n        if not documents:\n            return\n        \n        cleaned_docs = [self._preprocess(doc) for doc in documents]\n        cleaned_docs = [doc for doc in cleaned_docs if len(doc) > 50]\n        \n        if len(cleaned_docs) < 2:\n            return\n        \n        tfidf_matrix = self.vectorizer.fit_transform(cleaned_docs)\n        self.feature_names = self.vectorizer.get_feature_names_out()\n        \n        self.lda.fit(tfidf_matrix)\n        self.is_fitted = True\n    \n    def get_document_topics(self, documents: List[str]) -> List[List[Tuple[int, float]]]:\n        \"\"\"\n        Get topic distribution for each document\n        \"\"\"\n        if not self.is_fitted:\n            return []\n        \n        cleaned_docs = [self._preprocess(doc) for doc in documents]\n        tfidf_matrix = self.vectorizer.transform(cleaned_docs)\n        \n        topic_distributions = self.lda.transform(tfidf_matrix)\n        \n        results = []\n        for dist in topic_distributions:\n            topics = [(i, float(prob)) for i, prob in enumerate(dist)]\n            topics.sort(key=lambda x: x[1], reverse=True)\n            results.append(topics[:3])\n        \n        return results\n    \n    def get_topic_words(self, n_words: int = 10) -> Dict[int, List[str]]:\n        \"\"\"\n        Get top words for each topic\n        \"\"\"\n        if not self.is_fitted or self.feature_names is None:\n            return {}\n        \n        topic_words = {}\n        for topic_idx, topic in enumerate(self.lda.components_):\n            top_word_indices = topic.argsort()[:-n_words-1:-1]\n            topic_words[topic_idx] = [self.feature_names[i] for i in top_word_indices]\n        \n        return topic_words\n    \n    def get_topic_labels(self) -> Dict[int, str]:\n        \"\"\"\n        Generate human-readable labels for topics\n        \"\"\"\n        topic_words = self.get_topic_words(n_words=3)\n        labels = {}\n        \n        for topic_id, words in topic_words.items():\n            labels[topic_id] = \" / \".join(words[:3]).title()\n        \n        return labels\n    \n    def _preprocess(self, text: str) -> str:\n        \"\"\"\n        Preprocess text for topic modeling\n        \"\"\"\n        text = text.lower()\n        text = re.sub(r'[^\\w\\s]', ' ', text)\n        text = re.sub(r'\\d+', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text.strip()\n\n\nclass PaperClusterer:\n    \"\"\"\n    Clusters papers based on content similarity\n    \"\"\"\n    \n    def __init__(self, n_clusters: int = 5, method: str = 'kmeans'):\n        self.n_clusters = n_clusters\n        self.method = method\n        self.vectorizer = TfidfVectorizer(\n            max_features=3000,\n            stop_words='english',\n            max_df=0.9,\n            min_df=1\n        )\n        self.clusterer = None\n        self.is_fitted = False\n        self.tfidf_matrix = None\n    \n    def fit_predict(self, documents: List[str]) -> List[int]:\n        \"\"\"\n        Cluster documents and return cluster labels\n        \"\"\"\n        if not documents:\n            return []\n        \n        cleaned_docs = [self._preprocess(doc) for doc in documents]\n        \n        self.tfidf_matrix = self.vectorizer.fit_transform(cleaned_docs)\n        \n        n_samples = len(documents)\n        actual_clusters = min(self.n_clusters, n_samples)\n        \n        if actual_clusters < 2:\n            return [0] * n_samples\n        \n        if self.method == 'kmeans':\n            self.clusterer = KMeans(\n                n_clusters=actual_clusters,\n                random_state=42,\n                n_init=10\n            )\n        else:\n            self.clusterer = AgglomerativeClustering(\n                n_clusters=actual_clusters,\n                linkage='ward'\n            )\n        \n        if self.method == 'kmeans':\n            labels = self.clusterer.fit_predict(self.tfidf_matrix)\n        else:\n            labels = self.clusterer.fit_predict(self.tfidf_matrix.toarray())\n        \n        self.is_fitted = True\n        return labels.tolist()\n    \n    def get_cluster_keywords(self, documents: List[str], labels: List[int], \n                            n_keywords: int = 5) -> Dict[int, List[str]]:\n        \"\"\"\n        Get representative keywords for each cluster\n        \"\"\"\n        if not self.is_fitted:\n            return {}\n        \n        feature_names = self.vectorizer.get_feature_names_out()\n        cluster_keywords = {}\n        \n        for cluster_id in set(labels):\n            cluster_indices = [i for i, l in enumerate(labels) if l == cluster_id]\n            \n            if not cluster_indices:\n                continue\n            \n            cluster_tfidf = self.tfidf_matrix[cluster_indices].mean(axis=0)\n            cluster_tfidf = np.asarray(cluster_tfidf).flatten()\n            \n            top_indices = cluster_tfidf.argsort()[:-n_keywords-1:-1]\n            cluster_keywords[cluster_id] = [feature_names[i] for i in top_indices]\n        \n        return cluster_keywords\n    \n    def get_cluster_sizes(self, labels: List[int]) -> Dict[int, int]:\n        \"\"\"\n        Get size of each cluster\n        \"\"\"\n        return dict(Counter(labels))\n    \n    def evaluate_clustering(self, labels: List[int]) -> float:\n        \"\"\"\n        Evaluate clustering quality using silhouette score\n        \"\"\"\n        if not self.is_fitted or self.tfidf_matrix is None:\n            return 0.0\n        \n        n_labels = len(set(labels))\n        if n_labels < 2 or n_labels >= len(labels):\n            return 0.0\n        \n        try:\n            score = silhouette_score(self.tfidf_matrix, labels)\n            return float(score)\n        except:\n            return 0.0\n    \n    def find_optimal_clusters(self, documents: List[str], \n                             max_clusters: int = 10) -> int:\n        \"\"\"\n        Find optimal number of clusters using elbow method\n        \"\"\"\n        if len(documents) < 3:\n            return 1\n        \n        cleaned_docs = [self._preprocess(doc) for doc in documents]\n        tfidf_matrix = self.vectorizer.fit_transform(cleaned_docs)\n        \n        max_k = min(max_clusters, len(documents) - 1)\n        if max_k < 2:\n            return 1\n        \n        scores = []\n        for k in range(2, max_k + 1):\n            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n            labels = kmeans.fit_predict(tfidf_matrix)\n            \n            try:\n                score = silhouette_score(tfidf_matrix, labels)\n                scores.append((k, score))\n            except:\n                continue\n        \n        if not scores:\n            return min(3, len(documents))\n        \n        best_k = max(scores, key=lambda x: x[1])[0]\n        return best_k\n    \n    def _preprocess(self, text: str) -> str:\n        \"\"\"\n        Preprocess text for clustering\n        \"\"\"\n        text = text.lower()\n        text = re.sub(r'[^\\w\\s]', ' ', text)\n        text = re.sub(r'\\d+', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text.strip()\n\n\ndef cluster_papers(papers: List[Dict], n_clusters: int = None) -> Dict:\n    \"\"\"\n    Cluster papers and return cluster assignments with metadata\n    \"\"\"\n    if not papers:\n        return {'assignments': [], 'labels': {}, 'keywords': {}, 'sizes': {}, 'quality_score': 0, 'n_clusters': 0}\n    \n    texts = []\n    for paper in papers:\n        text = paper.get('title', '') + ' ' + paper.get('abstract', '')\n        if not text.strip():\n            text = paper.get('content', '')[:2000] if paper.get('content') else ''\n        texts.append(text if text.strip() else 'empty document')\n    \n    valid_texts = [t for t in texts if t.strip() and len(t) > 10]\n    if len(valid_texts) < 2:\n        return {'assignments': [0] * len(papers), 'labels': {0: 'All Papers'}, 'keywords': {0: []}, 'sizes': {0: len(papers)}, 'quality_score': 0, 'n_clusters': 1}\n    \n    try:\n        clusterer = PaperClusterer()\n        \n        if n_clusters is None:\n            n_clusters = min(clusterer.find_optimal_clusters(texts), len(valid_texts))\n        \n        clusterer.n_clusters = max(1, n_clusters)\n        labels = clusterer.fit_predict(texts)\n        \n        keywords = clusterer.get_cluster_keywords(texts, labels)\n        sizes = clusterer.get_cluster_sizes(labels)\n        quality = clusterer.evaluate_clustering(labels)\n        \n        cluster_labels = {}\n        for cluster_id, kws in keywords.items():\n            if kws:\n                cluster_labels[cluster_id] = \" / \".join(kws[:3]).title()\n            else:\n                cluster_labels[cluster_id] = f\"Cluster {cluster_id + 1}\"\n        \n        return {\n            'assignments': labels,\n            'keywords': keywords,\n            'labels': cluster_labels,\n            'sizes': sizes,\n            'quality_score': quality,\n            'n_clusters': len(set(labels))\n        }\n    except Exception as e:\n        print(f\"Clustering error: {e}\")\n        return {'assignments': [0] * len(papers), 'labels': {0: 'All Papers'}, 'keywords': {0: []}, 'sizes': {0: len(papers)}, 'quality_score': 0, 'n_clusters': 1}\n\n\ndef extract_topics(papers: List[Dict], n_topics: int = 5) -> Dict:\n    \"\"\"\n    Extract topics from papers using LDA\n    \"\"\"\n    if not papers:\n        return {'topics': {}, 'document_topics': []}\n    \n    texts = []\n    for paper in papers:\n        text = paper.get('abstract', '') or paper.get('content', '')[:3000]\n        texts.append(text)\n    \n    modeler = TopicModeler(n_topics=n_topics)\n    modeler.fit(texts)\n    \n    if not modeler.is_fitted:\n        return {'topics': {}, 'document_topics': []}\n    \n    topic_words = modeler.get_topic_words()\n    topic_labels = modeler.get_topic_labels()\n    doc_topics = modeler.get_document_topics(texts)\n    \n    return {\n        'topics': topic_words,\n        'labels': topic_labels,\n        'document_topics': doc_topics\n    }\n","path":null,"size_bytes":11020,"size_tokens":null},"utils/pdf_processor.py":{"content":"\"\"\"\nPDF Processing utilities for ScholarLens\nHandles PDF text extraction, cleaning, and chunking\n\"\"\"\n\nimport re\nimport pdfplumber\nfrom typing import List, Dict, Tuple, Optional\nimport io\nimport logging\nimport warnings\n\nlogging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\nlogging.getLogger(\"pdfplumber\").setLevel(logging.ERROR)\nwarnings.filterwarnings(\"ignore\", message=\".*gray non-stroke color.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*FontBBox.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*cannot be parsed.*\")\n\n\ndef extract_text_from_pdf(pdf_file) -> Dict[str, any]:\n    \"\"\"\n    Extract text content from a PDF file\n    Returns structured content with sections identified\n    \"\"\"\n    full_text = \"\"\n    pages = []\n    \n    try:\n        if hasattr(pdf_file, 'read'):\n            pdf_bytes = pdf_file.read()\n            pdf_file.seek(0)\n            pdf = pdfplumber.open(io.BytesIO(pdf_bytes))\n        else:\n            pdf = pdfplumber.open(pdf_file)\n        \n        for i, page in enumerate(pdf.pages):\n            page_text = page.extract_text() or \"\"\n            pages.append({\n                'page_num': i + 1,\n                'text': page_text\n            })\n            full_text += page_text + \"\\n\"\n        \n        pdf.close()\n        \n    except Exception as e:\n        return {\n            'success': False,\n            'error': str(e),\n            'text': '',\n            'pages': []\n        }\n    \n    cleaned_text = clean_text(full_text)\n    \n    sections = identify_sections(cleaned_text)\n    \n    metadata = extract_metadata(cleaned_text)\n    \n    return {\n        'success': True,\n        'text': cleaned_text,\n        'raw_text': full_text,\n        'pages': pages,\n        'sections': sections,\n        'metadata': metadata\n    }\n\n\ndef clean_text(text: str) -> str:\n    \"\"\"\n    Clean extracted text by removing noise\n    \"\"\"\n    text = re.sub(r'\\[\\d+\\]', '', text)\n    text = re.sub(r'\\(\\d{4}\\)', lambda m: m.group(0), text)\n    \n    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n    \n    text = re.sub(r'\\s+', ' ', text)\n    \n    text = re.sub(r'(\\n\\s*){3,}', '\\n\\n', text)\n    \n    lines = text.split('\\n')\n    cleaned_lines = []\n    for line in lines:\n        line = line.strip()\n        if line and len(line) > 2:\n            cleaned_lines.append(line)\n    \n    return '\\n'.join(cleaned_lines)\n\n\ndef identify_sections(text: str) -> Dict[str, str]:\n    \"\"\"\n    Identify common paper sections from text\n    \"\"\"\n    sections = {\n        'title': '',\n        'abstract': '',\n        'introduction': '',\n        'methods': '',\n        'results': '',\n        'discussion': '',\n        'conclusion': '',\n        'references': ''\n    }\n    \n    section_patterns = {\n        'abstract': r'(?:^|\\n)\\s*(?:abstract|summary)\\s*[:\\n]?\\s*(.*?)(?=\\n\\s*(?:1\\.|I\\.|introduction|keywords|1\\s+introduction))',\n        'introduction': r'(?:^|\\n)\\s*(?:1\\.?\\s*)?introduction\\s*[:\\n]?\\s*(.*?)(?=\\n\\s*(?:2\\.|II\\.|related|background|method|approach))',\n        'methods': r'(?:^|\\n)\\s*(?:\\d\\.?\\s*)?(?:method|methodology|approach|model)\\s*[:\\n]?\\s*(.*?)(?=\\n\\s*(?:\\d\\.|experiment|result|evaluation))',\n        'results': r'(?:^|\\n)\\s*(?:\\d\\.?\\s*)?(?:result|experiment|evaluation)\\s*[:\\n]?\\s*(.*?)(?=\\n\\s*(?:\\d\\.|discussion|conclusion|related))',\n        'conclusion': r'(?:^|\\n)\\s*(?:\\d\\.?\\s*)?conclusion\\s*[:\\n]?\\s*(.*?)(?=\\n\\s*(?:reference|acknowledge|appendix|$))',\n    }\n    \n    text_lower = text.lower()\n    \n    for section_name, pattern in section_patterns.items():\n        match = re.search(pattern, text_lower, re.DOTALL | re.IGNORECASE)\n        if match:\n            start = match.start(1) if match.lastindex else match.start()\n            end = match.end(1) if match.lastindex else match.end()\n            sections[section_name] = text[start:end].strip()[:5000]\n    \n    lines = text.split('\\n')\n    if lines:\n        potential_title = lines[0].strip()\n        if len(potential_title) > 10 and len(potential_title) < 300:\n            sections['title'] = potential_title\n    \n    return sections\n\n\ndef extract_metadata(text: str) -> Dict[str, any]:\n    \"\"\"\n    Extract metadata from paper text\n    \"\"\"\n    metadata = {\n        'title': '',\n        'authors': [],\n        'year': None,\n        'doi': None,\n        'emails': [],\n        'institutions': []\n    }\n    \n    doi_pattern = r'10\\.\\d{4,}/[^\\s]+'\n    doi_match = re.search(doi_pattern, text)\n    if doi_match:\n        metadata['doi'] = doi_match.group(0)\n    \n    year_pattern = r'\\b(19|20)\\d{2}\\b'\n    years = re.findall(year_pattern, text[:2000])\n    if years:\n        valid_years = [int(y) for y in years if 1990 <= int(y) <= 2025]\n        if valid_years:\n            metadata['year'] = max(valid_years)\n    \n    email_pattern = r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+'\n    emails = re.findall(email_pattern, text[:3000])\n    metadata['emails'] = list(set(emails))[:10]\n    \n    lines = text.split('\\n')\n    if lines:\n        metadata['title'] = lines[0].strip()[:500]\n    \n    return metadata\n\n\ndef chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[Dict]:\n    \"\"\"\n    Split text into overlapping chunks for RAG\n    \"\"\"\n    chunks = []\n    \n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    \n    current_chunk = \"\"\n    current_start = 0\n    \n    for i, sentence in enumerate(sentences):\n        if len(current_chunk) + len(sentence) <= chunk_size:\n            current_chunk += sentence + \" \"\n        else:\n            if current_chunk.strip():\n                chunks.append({\n                    'index': len(chunks),\n                    'content': current_chunk.strip(),\n                    'start_sentence': current_start,\n                    'end_sentence': i - 1\n                })\n            \n            overlap_text = \"\"\n            j = i - 1\n            while j >= 0 and len(overlap_text) < overlap:\n                overlap_text = sentences[j] + \" \" + overlap_text\n                j -= 1\n            \n            current_chunk = overlap_text + sentence + \" \"\n            current_start = j + 1\n    \n    if current_chunk.strip():\n        chunks.append({\n            'index': len(chunks),\n            'content': current_chunk.strip(),\n            'start_sentence': current_start,\n            'end_sentence': len(sentences) - 1\n        })\n    \n    return chunks\n\n\ndef extract_references(text: str) -> List[str]:\n    \"\"\"\n    Extract references from paper text\n    \"\"\"\n    references = []\n    \n    ref_section_match = re.search(\n        r'(?:references|bibliography)\\s*\\n(.*?)(?:\\n\\s*(?:appendix|supplementary)|$)',\n        text.lower(),\n        re.DOTALL\n    )\n    \n    if ref_section_match:\n        ref_text = text[ref_section_match.start(1):ref_section_match.end(1)]\n        \n        ref_patterns = [\n            r'\\[\\d+\\]\\s*[^[\\]]+',\n            r'\\d+\\.\\s*[A-Z][^.]+\\.\\s*\\d{4}\\.',\n        ]\n        \n        for pattern in ref_patterns:\n            matches = re.findall(pattern, ref_text)\n            if matches:\n                references.extend([m.strip() for m in matches])\n                break\n    \n    return references[:100]\n\n\ndef get_section_for_chunk(chunk_text: str, sections: Dict[str, str]) -> str:\n    \"\"\"\n    Determine which section a chunk belongs to\n    \"\"\"\n    chunk_lower = chunk_text.lower()[:200]\n    \n    for section_name, section_text in sections.items():\n        if section_text and chunk_lower in section_text.lower():\n            return section_name\n    \n    if 'abstract' in chunk_lower or 'summary' in chunk_lower:\n        return 'abstract'\n    elif 'introduction' in chunk_lower:\n        return 'introduction'\n    elif 'method' in chunk_lower or 'approach' in chunk_lower:\n        return 'methods'\n    elif 'result' in chunk_lower or 'experiment' in chunk_lower:\n        return 'results'\n    elif 'conclusion' in chunk_lower:\n        return 'conclusion'\n    elif 'discussion' in chunk_lower:\n        return 'discussion'\n    \n    return 'body'\n","path":null,"size_bytes":7868,"size_tokens":null},"replit.md":{"content":"# ScholarLens - AI-Powered Research Intelligence Platform\n\n## Overview\nScholarLens is a comprehensive research intelligence platform that ingests research papers, builds dynamic knowledge graphs, and provides multi-audience explanations, trend forecasts, and interactive exploration tools. It serves as a full research intelligence system that organizes, explains, predicts, and teaches.\n\n## Tech Stack\n- **Frontend/Backend**: Streamlit (Python)\n- **Database**: PostgreSQL with SQLAlchemy ORM\n- **AI/NLP**: Google AI Studio (Gemini) or OpenAI for summaries, Q&A, and content generation\n- **Visualization**: Plotly, NetworkX\n- **PDF Processing**: pdfplumber\n- **Search**: TF-IDF based semantic search with scikit-learn\n- **APIs**: arXiv API, PubMed API for paper fetching\n\n## Project Structure\n```\n/\n app.py                    # Main Streamlit application\n models.py                 # SQLAlchemy database models\n utils/\n    __init__.py\n    pdf_processor.py      # PDF text extraction and processing\n    ner_extractor.py      # Named Entity Recognition for methods/datasets\n    openai_helper.py      # OpenAI API integration\n    semantic_search.py    # TF-IDF based semantic search\n    graph_builder.py      # Knowledge graph construction\n    analytics.py          # SQL analytics and reporting (8+ reports)\n    arxiv_pubmed.py       # arXiv and PubMed API integration\n    topic_modeling.py     # Topic clustering and LDA\n    trend_forecasting.py  # Time-series trend analysis\n    export_utils.py       # Export to Markdown, LaTeX, BibTeX, CSV\n .streamlit/\n    config.toml           # Streamlit configuration\n attached_assets/          # Uploaded files\n```\n\n## Key Features\n\n### 1. Smart Corpus Management\n- Upload multiple PDF research papers\n- Automatic text extraction and entity recognition\n- Semantic and entity-based search\n- **arXiv and PubMed API integration** for fetching open-access papers\n- **Auto-clustering of papers by topic** using TF-IDF and K-means\n\n### 2. Knowledge Graph Explorer\n- Interactive visualization of papers, methods, datasets, and authors\n- Concept dependency maps (DAGs) showing method prerequisites\n- Co-authorship network analysis\n- Collaboration opportunity detection\n\n### 3. Evidence-Backed Q&A\n- RAG-powered question answering with OpenAI\n- Source citations for every answer\n- Saved query history\n\n### 4. Multi-Audience Summaries\n- Expert (technical) summaries\n- Student (with analogies) summaries\n- Policymaker (applications & risks) summaries\n- **Policy brief generation** with structured reports\n- **Cross-domain analogy generation**\n\n### 5. Analytics Dashboard\n8+ SQL analytical reports:\n1. Top co-authorship pairs by publication count\n2. Trending topics over time\n3. Papers per institution\n4. Research growth by field\n5. Top authors by publication frequency\n6. Most used datasets\n7. Collaboration network density\n8. Emerging methods in multiple domains\n\n**Trend Forecasting Features:**\n- Time-series analysis of method/dataset popularity\n- Identification of emerging and declining methods\n- Predictive trajectory visualization\n\n### 6. Learning Mode\n- Key insights extraction\n- **Flashcard generation** from paper content\n- **Quiz generation** with multiple choice questions\n- Personalized study roadmaps based on method prerequisites\n\n### 7. Research Workspace\n- Reading lists with priority and status tracking\n- Note-taking on papers\n- **Literature review export** in multiple formats:\n  - Markdown\n  - LaTeX\n  - BibTeX\n  - Plain Text\n  - CSV\n\n## Database Schema\n\n### Core Tables\n- **papers**: Research papers with title, abstract, content, year, DOI, source\n- **authors**: Author names with h-index and citations\n- **institutions**: Research institutions with type and location\n- **methods**: Research methods/techniques with category and usage count\n- **datasets**: Datasets with domain and usage count\n\n### Relationship Tables\n- **paper_authors**: Many-to-many between papers and authors\n- **paper_methods**: Many-to-many between papers and methods\n- **paper_datasets**: Many-to-many between papers and datasets\n- **author_institutions**: Many-to-many between authors and institutions\n- **method_prerequisites**: Self-referential for method dependencies\n\n### User Content Tables\n- **paper_chunks**: Text chunks for RAG (semantic search)\n- **flashcards**: Generated flashcards for learning\n- **notes**: User notes on papers\n- **saved_queries**: Saved Q&A history\n- **reading_list**: User's reading list with priorities\n\n## Environment Variables Required\n- `DATABASE_URL`: PostgreSQL connection string (auto-configured)\n- `GOOGLE_API_KEY`: Google AI Studio API key (FREE - recommended)\n- `OPENAI_API_KEY`: OpenAI API key (alternative, paid)\n\nNote: If both keys are set, Google AI Studio is used by default.\n\n## Running the Application\n```bash\nstreamlit run app.py --server.port 5000\n```\n\n## API Integrations\n\n### arXiv API\n- Search papers by query and category\n- Fetch paper metadata (title, abstract, authors, year)\n- Support for all arXiv categories (cs.AI, cs.LG, cs.CL, etc.)\n\n### PubMed API\n- Search biomedical research papers\n- Fetch abstracts and metadata\n- MeSH term extraction\n\n## Recent Changes\n- November 2025: Initial full implementation\n- All 12 core MVP features implemented\n- 8+ SQL analytical reports added\n- arXiv/PubMed API integration\n- Topic clustering with TF-IDF + K-means\n- Trend forecasting with linear regression\n- Multi-format export (Markdown, LaTeX, BibTeX, CSV)\n- Interactive knowledge graph visualization\n- RAG-powered Q&A system with OpenAI\n\n## Development Notes\n- Server binds to 0.0.0.0:5000\n- Uses st.rerun() instead of deprecated experimental_rerun\n- PostgreSQL database with SQLAlchemy ORM\n- No Docker/containerization (Nix environment)\n- AI features support Google AI Studio (free) or OpenAI (paid)\n- Google AI Studio uses Gemini 2.0 Flash model\n- OpenAI uses GPT-4o-mini model\n","path":null,"size_bytes":6046,"size_tokens":null},"main.py":{"content":"def main():\n    print(\"Hello from repl-nix-workspace!\")\n\n\nif __name__ == \"__main__\":\n    main()\n","path":null,"size_bytes":96,"size_tokens":null}},"version":2}