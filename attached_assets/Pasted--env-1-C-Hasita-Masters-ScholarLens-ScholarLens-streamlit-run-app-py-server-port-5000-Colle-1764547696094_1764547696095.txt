
(env-1) C:\Hasita\Masters\ScholarLens\ScholarLens>streamlit run app.py --server.port 5000

Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.


  You can now view your Streamlit app in your browser.

  URL: http://0.0.0.0:5000

2025-11-30 19:07:25.998 Uncaught app execution
Traceback (most recent call last):
  File "C:\Hasita\env-1\Lib\site-packages\streamlit\runtime\scriptrunner\exec_code.py", line 129, in exec_func_with_error_handling
    result = func()
  File "C:\Hasita\env-1\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 669, in code_to_exec
    exec(code, module.__dict__)  # noqa: S102
    ~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Hasita\Masters\ScholarLens\ScholarLens\app.py", line 1296, in <module>
    main()
    ~~~~^^
  File "C:\Hasita\Masters\ScholarLens\ScholarLens\app.py", line 117, in main
    pages[selection]()
    ~~~~~~~~~~~~~~~~^^
  File "C:\Hasita\Masters\ScholarLens\ScholarLens\app.py", line 224, in page_upload
    st.session_state.search_index.index_paper(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        paper.id,
        ^^^^^^^^^
    ...<3 lines>...
        [m['name'] for m in entities['methods']]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Hasita\Masters\ScholarLens\ScholarLens\utils\semantic_search.py", line 106, in index_paper
    self.title_search.add_documents(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        [title],
        ^^^^^^^^
        [{'paper_id': paper_id, 'type': 'title'}]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Hasita\Masters\ScholarLens\ScholarLens\utils\semantic_search.py", line 43, in add_documents
    self.document_vectors = self.vectorizer.fit_transform(self.documents)
                            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Hasita\env-1\Lib\site-packages\sklearn\feature_extraction\text.py", line 2105, in fit_transform
    X = super().fit_transform(raw_documents)
  File "C:\Hasita\env-1\Lib\site-packages\sklearn\base.py", line 1365, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "C:\Hasita\env-1\Lib\site-packages\sklearn\feature_extraction\text.py", line 1387, in fit_transform
    raise ValueError("max_df corresponds to < documents than min_df")
ValueError: max_df corresponds to < documents than min_df
2025-11-30 19:07:58.057 Uncaught app execution
Traceback (most recent call last):
  File "C:\Hasita\env-1\Lib\site-packages\streamlit\runtime\scriptrunner\exec_code.py", line 129, in exec_func_with_error_handling
    result = func()
  File "C:\Hasita\env-1\Lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py", line 669, in code_to_exec
    exec(code, module.__dict__)  # noqa: S102
    ~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Hasita\Masters\ScholarLens\ScholarLens\app.py", line 1296, in <module>
    main()
    ~~~~^^
  File "C:\Hasita\Masters\ScholarLens\ScholarLens\app.py", line 117, in main
    pages[selection]()
    ~~~~~~~~~~~~~~~~^^
  File "C:\Hasita\Masters\ScholarLens\ScholarLens\app.py", line 224, in page_upload
    st.session_state.search_index.index_paper(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        paper.id,
        ^^^^^^^^^
    ...<3 lines>...
        [m['name'] for m in entities['methods']]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Hasita\Masters\ScholarLens\ScholarLens\utils\semantic_search.py", line 106, in index_paper
    self.title_search.add_documents(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        [title],
        ^^^^^^^^
        [{'paper_id': paper_id, 'type': 'title'}]
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Hasita\Masters\ScholarLens\ScholarLens\utils\semantic_search.py", line 43, in add_documents
    self.document_vectors = self.vectorizer.fit_transform(self.documents)
                            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Hasita\env-1\Lib\site-packages\sklearn\feature_extraction\text.py", line 2105, in fit_transform
    X = super().fit_transform(raw_documents)
  File "C:\Hasita\env-1\Lib\site-packages\sklearn\base.py", line 1365, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "C:\Hasita\env-1\Lib\site-packages\sklearn\feature_extraction\text.py", line 1390, in fit_transform
    X = self._limit_features(
        X, vocabulary, max_doc_count, min_doc_count, max_features
    )
  File "C:\Hasita\env-1\Lib\site-packages\sklearn\feature_extraction\text.py", line 1242, in _limit_features
    raise ValueError(
        "After pruning, no terms remain. Try a lower min_df or a higher max_df."
    )
ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.
